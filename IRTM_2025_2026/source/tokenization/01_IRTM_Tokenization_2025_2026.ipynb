{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VgDWmVan1SO"
      },
      "source": [
        "Start by copying this into your Google Drive!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8S4ipRELHrMV"
      },
      "source": [
        "![maastricht-university-logo.jpeg](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAkGBhQQEBUUERQVEhIVGBwUFBgXGBgfFhwVFxsaGRscGB0YGyceHxkjHhgYHy8gJScpLSwtGCAxNTAqNSYsLCn/2wBDAQkKCg4MDhoPDxosHR8kKSwpKiwsLCwsKSwsLCwsLCwsKiwpLCwpLCwsLCwsKSwsLCwsLCwpLCwsLCwpLCwsKSz/wgARCACgATsDASIAAhEBAxEB/8QAGwABAAIDAQEAAAAAAAAAAAAAAAUGAwQHAgH/xAAXAQEBAQEAAAAAAAAAAAAAAAAAAQID/9oADAMBAAIQAxAAAAG8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA+febzes214950AAPJ6U642Ff3CUEoAABG+kkApWJSyTEpR7rZ7IyWTaG+AAcu1Nqa78oW0adPl7J65Le+ep8Z0jZKu2c96nS/vTEfYo/wCFngbTVs6suhE2MrttpEunv3UbwVZM7dZ4f17l2ZWs7oxxF/OeyUbcU1av8mqi+hQE/nQZoHLpOMy9uXvTvE5nVZsulTM24yPHOxH2nXHTljud9d09TDRunx8UvdtuGqpGdC8lPkp3NEHVbxvlejrtGENKSWVahITWVOZ3KR81Sega+7m83np3aspd4iZaUJQNPcRaZ6bAa/bnaNWay51VOp0W9S/Y+Qo2bdMerpknuVSVJZX5iIKy06e0k2GPylmGDqxNbTllUVkSRhvGKpXFhodnU0V5zZdg1VkQAAAYqvbVnLtbrNc3mk9Z5h0+PtGvNLlsOng06z1S5Vq5l/UvDLD2as9HiqQGla9TS+YpuWB+W6nliqPUYeXzF1uyWSVNvNML1QejViXzJ0bq6fRjYAAAAADBnFSy2hZ8gLAjU1JYV+wBX5jYGpWrgqExz4+Va1CDzSw1I2dRBZt6jamzedTblCUAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/8QALhAAAQQBAwIFBAEFAQAAAAAAAgEDBAUABhITETMQICIjMiEwMTQVFiRAUHBC/9oACAEBAAEFAv8AkXX/AFE6QQPxL5FwDRfORdEa1B1XLC14lr5vKP2J05GkgyuQPC4sCbyukK434XE0wcH8ZOnI0kGVyB5bXvOxSDGeRtIl6i4JdfLYn0a2/Svf3tzHORzTp5ZWXDn8+WQJ6OpY23EX9QFkWUjgzLzatfaE4drYb8gW/GllYcWPag6JazNiQnt7cy8QVgXKuHfd16/6ZW2vLlpYci19tsTy23evPhK/UyNLJvK605fJen7TcbrGgTdrVVF6hQue5LeAMmXQmOnvlLlttrYWwujp74L1ZdhWouZqBPRSJ7OovxSwhUNRZHc2xKOMhHxpl93YsNAbo+7qFPTSp7Plte8NiBhYWCGkavNzItEKYI9Mkzwb8dRHlS17Dre1a2Ptagel+968smzBW9PfKUX9xZWKGOnvi9bNLleG57UA+3V2iAGofxS9nUWRGd8WBM4Tj3Amd93f/NH3dQp6Ku0EB8tr3o1SZ5FpwDwkzhbyXdkWJ+U8HogngB0RyvAlz+PDq/GE8ZgAGMwxDJEIDwYAIjMcQwqltcZYEMMEVEqW8ejCeNNIKPxRPG20FHoIHjNeAK9CA16Y1BAVcbQkGqbRfKkQd2SZgt5LvCLK+rR0a+u5CmRuNxPCTPFvGnUJJEhASNLFxPF67ET+1YWSNZElI4LtgAr9ucLnR9skXDc4hmpxBeB6k8NQ/Ks7N52dPfAbQFI7xtFYfQ0fFjmk2Atqy6hJIswbUnkRFvW8jyRNH7IAL+TDfKng3ke3AytkbyO6AMy30J5izA8j2oGrzyAkWeLn2XGkJJNAmTxcx+WZ4coixPDUPyrOzednT3wcb3Oz6oAa06v0mfs3sfq3QyfTHTmfv3/U0cfZRudHbzvV1Sg5IX3obbKrqLK5lCjzGUF6PAAFkjwvXsn00sfa39pUyTRgWP1pgqeGoW8qrAeO6nio0TXRuP8As2/Z07kz9kw6p1VstPx/pft+5EBgxjts7r3uh+JUdpzA9LuosqOzZL0kNSRLNQR/o2Kukif4DzKGh6eTGaAExEwKdEclR94wK9Gsdp0JzJdOLhR2dgyYouIunUyDVI0syoRwkTJFEhFDqBbWfXo7kWPsGdUo6sCr4ltSTh0/H+v+lkouxa99zIsdGx/5L//EACARAAICAQQDAQAAAAAAAAAAAAABESFBAhAwMSBQURL/2gAIAQMBAT8B9TPjJPNBMC1TuqF2MwJ2SZMmRWZMC3uT8/SkJyMakgggggjaBKNooXg9RAh8U8D0mk1dE0fDIjsyPoaJofQxXfDHGq9X/8QAIREAAwACAgIDAQEAAAAAAAAAAAERAiEQMRIgQUJQMHH/2gAIAQIBAT8B/J8fVqE9ZxPe9E8h4zhdmWx9QX+E2NaGtH1IpSaHFo1DrIy751DynRHkZKCcFlBZFR5bLqFhdQuoXQ2mUeW6P0WKKzLpGKrJsaIPicQm9iVJ7JwWRmY9k2fLPqNyD1s+BbyFkSMXbMXvZlpT+Pk+LxROFE4Uot7Y3X+X/8QANxAAAQMBBgIIBQMEAwAAAAAAAQACESESIjFBUXEQgQMgYZGhscHwMDJygtEjUuETQmJwQFDx/9oACAEBAAY/Av8AZTy0x/4F+pTtGCpX4AuxPbwAic1MRWPgyRM0VqI4tDc80HHHjDXQI4yRMq1EdZ/vIIWhE4K22Q093cofdOuSp1X7LwTT37hOI9gJw5oUmV8g8VoRiFZDZzVWDxVoKGi15Ky5sUlWYiy7+EGWZrrqhSZQhtYk6JssD51QMR2KGi1rog2yF9o9VDBPaVBEOVmIsn+EGWc9dT1n+8l0XvIJnLhdP4UWSDr/AG9Tcj8onR1r0XSDu50XSH/GyPfctwpfHZryRbZx1hO2Uu+bxUWeadv6KoqDnmowdom/V6IblN3VogEnVM5+in/EozWyFgvtHqojKqGxTd/RDn59Z/vIIN6YGmYQa0QwKgpqcFfvHwVFU10z4sG5QBznxRGlEBzPND6iPMKuEUVjohU0wTtkbdRarsrPRimJTt/REOE5YIWcJnYIfUrJnGnNM3Q5+ZTOforOoKqOwhBoBqvtHquSGxTd0GEGZ8+s/wB5BYWRqfwv3Ht4XjyzV26PFc+N4SoGCktBPCbInHmrwlXWq62FebKiyIOKuiF8vmrohQahfL5q8JUNoFeEwoFArzQVLWgFS5oJ4S1oBUGoU2fPrF0XjnwvH8q5dHirTiZJ94pwcYsqzlSON44qRgVLsFLepZg0oT8MUklWgoJrp8T9Mj15K/M9vDoB2yffNdIc3Op4fymO14s2KZsjuE76vRWZrtoszsFLahVm1OGUqHeSBGBUONdlJoO1Z9ylplWTjsrGJV410zVkTJQ/qT2QrTfkVoYSD3QjZmgnBQDXZScAruXwYIkK4Y7Dgv1Z7NPBQ4ygHGQMOLNimbI7hO+r0RAzcR4okYjNP5L7x6Kf2+Sc0/215Lc2jsEG5ATzUGpzMGZUZELkE1xm1j2J1v8AdXZT0cSO9M5poOH8qyMJHopaIWxtDZNAzvclObq/j4l24fBVEjUcWnkgCQCKVVlpmtVP7jK+8+qcn8l949FCcN2Iu1oOSnUeSFGTmrlm0NFyHC9EjtqhYM3oB1TOab7zR3B8ldcCg/kU1v28h/wYdUKjirxLvLhbk4yi3VGCTKtycZ7uFqSEGjJQ5fMe5SCScFaJI4F1rEzgp+Y9qEmIQbjCmYKmZyTp9nJF+lB/0zrOMU3V/wAT+EGj/U3/xAArEAEAAQMCBQMEAwEBAAAAAAABEQAhMUFRYXGBobGRwfAQINHhMFDxcED/2gAIAQEAAT8h/wCRH9QVmpOTuFWQT+BMlCygOEx98w7E+lQpiQTonp9I1nSV4jbTnU9i2JnQff8AhJokgEZida34KRM4fqnBLlROIsUIMKTgwxJ9ZwILBGb7lOzl9DaJIIjnrUFkLaZwx93eFBSUbhfXRqH3EklO8vaocfg8ygEoR1Ptm3Getvep3aT6omt5Yj0DS3sEp6f7612f1W9q3znF4LR+ajzB8bU6Q8g1OH4pboQSXedjhQL5ge5RLDhHI7UTzxZVy2N6gAvSvpG/OnazIvOYnZSCDc4m0UhYZozHtVhS8izhxaMAU2NCRwd6tmmYYNKdThZTH7UlAmWRbRQqJkEFpM8jagTEibNkprDLvMzE7KWLOE+FHH7u8PCuwaH9KtMW5q5lZQDIv6tOX2RR2Xp+lb/+iAXaaaauA8/+TXM5zolpFHf9SH80EXxrJoGzsgcDoxwa7d5atImQQTH8VEKGREkkcq+HwVdZFIYBm560nlvNeTXyG6jwf6V3bxUpapZMAxafpywMkHOUpsIAQcSue1XJhJhi9Kos2Z0cqXmvmuFep/Kjw6+T7u8KGunHgW0uVxT1yxjpXtGP26Vd1wsenWhEAAwGKsBbBf0UfSxxPQI962EKeSfamf1K6NTdlOpdSj7URck9GsVIDDbZBrza7f5oxTBY5LdorHpvxFjhz1r5/BUo0FQzxanh0Hfc+KmXQu6iVDuWhinfPFd/WzrRAyI5yxSIonPW9yiRLrsaVmoHw5V81wq6aF/UawkgRi79/d3RRf8A4XGVXZOPj0MUFFYHQXXSrPz/AMwU5C3YefqeE2WJoyCBYOFNFjLQVxe3a6poWDjialoBSF1jm00pNsxV1Jb6+pTU6A3560YhluxSUplu3/KhIE8CnQiZHFFZDPP8qjI44moUAYDjUPHpTx/yjAwYCmpQ319StiEOvSa0QFLtWEVxkAUoETI1HBkuX/L7jxeoru1tsfQKSNjVyKsg43zBUJ9AXLxvBZzQpAtUjMpqcKt5UKTmGNuM/U2DGRZccqB3wk5VJ6MMLnlTpZBhsl+v2KZQsDPLYof4oErEOBRMEG0ORKi9vCF2XEx/Iy2Os+TFWGHrvXWmtDWHVIe9Yvw3k/xSxsEPRE8/V8huV2GvgN6+RwVHavS6iZTfpUcniWKCtJ89a4Z8gZuI9qPKikkJ8Urc3Cp7LJsnPKpDWZmyjsX8cKgSHceO1TpccJzirEqrFiw8WuyhehqDgkta9PLZvMM8Sh0gb4ZzCuuaUTTxrwJW6UyEri7BQd0sSjF9acvGSn0a6pEzz/hji2kpl5vM6NTvR0uRZQZbIihRZLhbONCpOUELW00Pq+Q3K7DXwG9fI4KbLvXFUrFtuzvOlWTSV6j+KyU7Fyp6rPs9KvRs82e/mlmceyXgp0uo80x6HmiieHwxtSxG6HpcfPrShfLNDLXGskx3q/FScdYNo6RRYuGsJtcrDm8KBOUMnVS0IjxwSXmlUokN3HWmVPdF/wAlT469yY89qif0f6ef4zSG5V86T2/ih2UNQzrqfU3Dv1MJ4qG97FFtEmgZGhhgCnnbDlAHisNDtzyVhze6slMmWEh61uXCcnXsNRP1dLLv4pQaDh4yn2pIihYwI61icSdwYrP8s1i5FKssZQA5/utuQBoU/wArDm8K7d8qmLg6YKLwjE2dKmFp6Tjv5pjbB4fqmoyDBb/wLSkp7ZmyDTM8Ax2VBixWbacbRefzQOKGorRYcxpO3OpIJhG0Yfj6OU1CYjTW9DhAiojSZNx4NT7RcRSRAMoiORU/JAQRpzqIjapWRZWa1kkcOjkFXEXMRrzoDFGpzdn3q5aOGMO1G4bTQUTdkHw3qR2PVOfb1/pmbi9C1IBKDUoOizWEg13dX/k3/9oADAMBAAIAAwAAABDzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzxvTzzjjjzzzzPz3zX7bzyiRrz0KKyQs9I7wTtbTzytQe65iMzIeasgWFKPbzw+Gfy5NLezfHbP5fPLzzzw1L/ALBPtBfqOUiiwr888888c8ef/fvdtddvj+888888888888888888s88888888888888888888888/8QAIBEAAwACAgMAAwAAAAAAAAAAAAERITEQIEFQUTBhgf/aAAgBAwEBPxD1M2dUj0RZ1vjiqzhOuGujR1jbURwbiMxof0b9ido0BU8kfmNvBCeQzyM1Bts1OdkMnQ3w5KoRQdQqs1gmtGzeRtaiZosmxmAllsWgRpZ50O0sECxsaKjeKJ1UqYn9KNpFKhvGBwiovVpPYzwyvJsGgawg1FglqE8F8FsJJMERkxEfwXGB6/Cj4RMitJCIaTItDVIpCKQeFEhInq//xAAiEQEBAQEBAAIBBAMAAAAAAAABABEhMRBBIDBQUWFxkaH/2gAIAQIBAT8Q/aV5p+K+reb+Os34UG/CYDBvCTOfJjFjWz9fB0Qzz6btD6iJ9pBnNj6zITg3+5T6QG08WPg7cDewGEjx85hskZCXHbNti2G793IE8gaZPCCOQekw8j+LqJ2dAWlMXT59jd9v+3ktSDBjwsFCUewOYWMJ8LHy1GMC3Nqx9/FPEf2WOZecL/eHEhVbaF/UOv5Tw5LpZKsYZ8v83g+1wfouGbClrMlX2UmS+LW7J4tbsIduml2P2v8A/8QAKRABAQACAQMDBAMBAQEBAAAAAREAITFBUWFxgZEQIKGxMMHwUPFgcP/aAAgBAQABPxD/APIjbEYxjw9nz/yN4BYb6Umn95C6Xkr+T8h5MI75JFHhPvJPhl6C/wBYNvV3wgsmy98uEYdtYs6ls+HnOdhn2AGw5B0/h9QZojZWiD3xyeluglbDb9dQD6gA2a619sO1YQShA9GX6vRacmm3Y9DGVdqF9YfTUIylsHZWiHnGX03oB+UOZfuWXcxZ6vA0ETSc470xKbobFslh8mb4jUbT57vmnnBG4wgieE+2YsYPUz+cPF958D4bl+bP/RbS++azsjpqo8OGjtJ+xf6ZoSQkyc10t/pi4SriovosY9pwMjIohy9goNWsEEewhEABFasNQHM0HjC1Il4A8qdf3luz1wHTAVDrpvJ2sOl2HB3wwGhKWvVBEtvLgWJW6H3Xi9+mXJFC4heixkhKrq2HQxK8BeuDqFFh0MeS/jJ2BolAUBo7dspx9MKWggqHToPOFmiloBeE30OTLIOUPziMBxIBACgXbVee2A5gSh0FKURSj3xwwDA11ARO/LhWG7qHk6nHl0/gJoIh3/sv0oyeV36zq+SPnLHhgWh3jby+fsFusn3V+HijFYP/ADPZ7ZqY2Hxuw9fkc12bR8nkt9IvdD8Y5xiQUZQS3RUx0urg06QrYPN1iy4kaABqbN9LXahvIPvdqNwp2U5649Hj+cfTuOICTYlCcPpMPkWKR4bZafTTDjDFAF2QLi5EL1w7/oVO5NALSAhUVeuu2CPoxq+XHZEflwS4CDWBDpQU8t6YQ82IIHmMpkE9v2YGUrtC3KPJuB0A+mAiAC2gYOZC86F+7FyosEhZuIdhE0nGOnlQ5Agg6B32vabTFh96HhS+w5BZ4kPlfczxhcjQAA8BihWw2msNODywxU+nq34JiCRQDv8A24yhV84JD7gPvkiSefXD6CHtlaYfOxPlMmFIj2DTu73S+mchphuzsOC7KdVx/wCnrxEoD10jj1Nk6hOuDWaNGF0EUKKp0C3PwuA2soQrVHZuc3FRVRzFQL0TzXEBNL0Rvyhh5e9EUId1Iiv4x/QdD8TFdM92B+QY2kDphVaXqXXUXxipYLWCFgCq6z/a744voYMiF2XQiL76xjGqgVvKqSMe33fMM9el5ycquginyPeHnIrgaSQnZI3utfOAENHTLY0/oAb92GXDtrgv3/RXziqFEVVVjau3OB6fQsMUrQ7hvAzFi4BwGbO1oNUA3vsGQ4xX0V3k7nN3h0kr1B8JE9nNwcbi1pKKD4wsEUrZzHeKgKQ3J7UGeMJ+loeDSlqPnHQBDgsl+MasI6IVa0I5zqnRCK+XlfXDQjAVHkw/KIlQE4gxhYCaitLpmFbWeAUr8qudQu1caWesfGC30+AVuvdzprvRJO6CmDADuopNkppTWaYwQNiw58uB4NSTxxh8qEAbHk5wi6QFHBwiFEBNiDHP3GhUHoAOThBxPp4CU36Bt/WVnd7FH59mvkx8+AERoqQo6uDGvVqS98iRvuYawAzXUQFgcdDOB6fRpSGNwgeDOTGMiPEq8OzFbjRIiqGgvOJDvTBAyAeE+xUwWsGgpyab9HKFOH+LeiaCAQFV8smIeRUKCJrTxz2wgM4RGOgMtOe/8k/LOtr1KH1PcxuNdrT5BLHkXFC5dthA9Fr3P0400d0KA35PfCStkfP4Q/jOB6fZBp/o9v0JQgh8ABUiQFvEBGMWPtVKeTBZ8B6iciOwdnHolDzEBhI2tx65q0aQQY7HT+zItlSRnkeHxjk0BgAVKiXWQvkfAHi3h3xkSS7H+yM9s6DjZ5EDteHCJqEAHTsE5xS6Z3EFSkpGhZigTRQGHeHB5Zg20DTUK2OtDlyOQs6FIOnWntkZJQqOsB2WsX9EFRAbN9WsSAsaD1pTb4xK01ADFAiWCzw5K8qgV5mg5d4iiAygJD3R+P4WSpyJPz1xNfyHMdl+yPGQqBW6MWJtDTvWKRKgIQ1o6Ke+bHFw6xyBda25wPT7MNP9Ht+hOqor7RXwFXwOEXNQldADoa6DWNsaEPIH8D4+jUsrf7/qTAEIVs878Avsw2e8HaCL2Pfx5tYOjFXoa9eAww9RDa5oPAa11zYZ0Frujv2eWXBqL8VgpH0wjcJtDZXbvXGBAWBptB4THc45yC6U3AHbbwpsz/a7Ml27RKD8jemRHIaXaCrd185sQpXtWRJ0wWUC8m8HiPt4bO7Y6gfm1i2Gy/WQft7v41QiaRBH1HLLPYNT5fHuOcRmHm4xsHY6k85wPT6WT1u9vyCvjN2igAhaNGkocNxufE7TdpqrNeuORSpval70+/13Bf6vb6WgNri8COIzq1aKwR6gPXI2j+xh6qZj3UdCwHwr3zQUBOUlCnW74cAiqJIo2qaOZPpb/ndjJQrNyOm25eAmaZjrpBTWkaro85/tdn0L75UXqbPhzoARNThUODeQJt/k0+gmE+6odwr6n4DAIIADsGj+VPqRXlj+ETYjsTjEjZagvF0/OEQJu4j5Nk8Wd7ggAAQAgBwB2wiHNM37jRZthNkBhSI6uumAi6TInag/8Y4bZpu0GyzT6EgMBlRKjmQ9srqnK8s5WdVr75yp1BncBsf31x3sZLPUn6y3XLRuHQc0Nq4yTJWZXZbvIHKA+CY6hAVlWD44w9XStK1qQfLXeCG/RKsBuHthPxQhVNprsw5qxgIHUPUrsys2YQAo0m7rJYpH62mnkh9nIw0V9JZ6APd/xiq7OUGloqA2buL8eJfpKIWL09zN5Y7XK7Tytfx/x5/9/wD/2Q==)\n",
        "#Faculty of Science and Engineering - Department of Advanced Computer Sciences\n",
        "# Course Information Retrieval and Text Mining - Tutorial Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZVnJSqfnp_v"
      },
      "source": [
        "By Jan Scholtes- Version 2025-2026\n",
        "\n",
        "\n",
        "Welcome to the tutorial on Tokenization. In this notebook you will learn how to preprocess text into tokens.\n",
        "\n",
        "This is the basis of any Information Retrieval, Text Mining or NLP process. Tokenization is closely related to sentence detection, stemming, lemmatization and is part of the large NLP research topic named morphology.\n",
        "\n",
        "Tokenization is highly language dependent. In this tutorial we focus on Western-European languages.\n",
        "\n",
        "In this notebook, we will use the Stanford NLTK library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: The Tokenization Problem in the Real World\n",
        "\n",
        "Before we dive into technical solutions, let's understand why tokenization is a critical problem in Information Retrieval and NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 1: Information Retrieval Failure\n",
        "\n",
        "Imagine you're searching for information about \"New York City\".\n",
        "\n",
        "**Without proper tokenization:**\n",
        "- System splits on spaces: \"New\", \"York\", \"City\"\n",
        "- Returns results mentioning \"New\" separately from \"York\" and \"City\"\n",
        "- You get irrelevant results about \"new products\" and \"city maps\"\n",
        "\n",
        "**With proper tokenization:**\n",
        "- System recognizes \"New York City\" as a named entity\n",
        "- Returns highly relevant articles about New York City\n",
        "- Search precision increases significantly\n",
        "\n",
        "**Real-world impact:** E-commerce searches, news aggregation, legal document retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 2: Morphological Confusion\n",
        "\n",
        "Consider these words: \"running\", \"runs\", \"runner\"\n",
        "\n",
        "**Without morphological processing:**\n",
        "- These are treated as 3 completely separate, unrelated concepts\n",
        "- Statistical analysis treats them as independent events\n",
        "- If \"running\" appears 5 times and \"runs\" appears 3 times, system sees 8 separate instances\n",
        "\n",
        "**With lemmatization/stemming:**\n",
        "- All three map to the common root: \"run\"\n",
        "- Combined frequency: 8 instances of the concept \"run\"\n",
        "- Statistical significance improves, algorithms work better\n",
        "\n",
        "**Real-world impact:** Text classification, sentiment analysis, topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 3: Out-of-Vocabulary Catastrophe\n",
        "\n",
        "Consider a rare word: \"unfathomable\"\n",
        "\n",
        "**Traditional word-level tokenizers:**\n",
        "- If the word isn't in their training vocabulary, it becomes `[UNK]` (unknown token)\n",
        "- System has NO idea what this word means\n",
        "- Complete loss of information\n",
        "\n",
        "**Modern subword tokenizers (WordPiece):**\n",
        "- Even if \"unfathomable\" isn't in vocabulary, it can be broken down\n",
        "- Becomes: [\"un\", \"fath\", \"##om\", \"##able\"]\n",
        "- System can understand morphological structure and partial meaning\n",
        "- Graceful degradation instead of complete failure\n",
        "\n",
        "**Real-world impact:** Handling rare words, slang, misspellings, technical terminology, multilingual text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4: Domain-Specific Ambiguity\n",
        "\n",
        "The same text needs different tokenization for different domains:\n",
        "\n",
        "**E-commerce Example:**\n",
        "- Product: \"iPhone 13 Pro Max\"\n",
        "- Bad tokenization: Treats \"Pro\" and \"Max\" separately\n",
        "- Good tokenization: Recognizes \"Pro Max\" as a product tier\n",
        "- Impact: Users searching for \"Pro Max phones\" find what they want\n",
        "\n",
        "**Medical Example:**\n",
        "- Diagnosis: \"Type 2 Diabetes Mellitus\"\n",
        "- Bad tokenization: [\"Type\", \"2\", \"Diabetes\", \"Mellitus\"]\n",
        "- Good tokenization: [\"Type 2 Diabetes Mellitus\"] (one medical concept)\n",
        "- Impact: Drug recommendations, diagnostic coding accuracy\n",
        "\n",
        "**Legal Example:**\n",
        "- Contract term: \"Act of God\"\n",
        "- Bad tokenization: Three separate words with no connection\n",
        "- Good tokenization: Single legal/insurance concept\n",
        "- Impact: Contract analysis, risk assessment, liability determination"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interactive Challenge: Predict Tokenization Outputs\n",
        "\n",
        "Before we show you how to tokenize text, try to predict the outcomes!\n",
        "\n",
        "**Text samples to tokenize:**\n",
        "1. \"don't\" - Does the apostrophe split it?\n",
        "2. \"john.doe@example.com\" - How does email get split?\n",
        "3. \"Dr. Smith\" - What happens to abbreviations?\n",
        "4. \"award-winning\" - Does the hyphen cause a split?\n",
        "5. \"U.S.A.\" - Multiple dots and capitals?\n",
        "\n",
        "**For each, ask yourself:**\n",
        "- Should it be 1 token or multiple?\n",
        "- What would a search engine need?\n",
        "- What would a language model need?\n",
        "\n",
        "We'll test your predictions with real tokenizers in the next sections!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Understanding the Long Tail - Why Preprocessing Matters\n",
        "\n",
        "Natural language exhibits a remarkable property: **most words are rare**.\n",
        "\n",
        "A few words (the, a, is, and, ...) appear extremely frequently, while most words appear only occasionally. This is called **Zipf's Law** or the \"long tail distribution\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why This Matters for IR and NLP\n",
        "\n",
        "**For Information Retrieval (Historical Perspective):**\n",
        "- In the 1970s-1990s, computer memory was expensive and limited\n",
        "- Idea: Remove the most frequent words (\"stop words\") to save space and improve relevance\n",
        "- Result: Faster search, smaller indexes, but loss of search capability for phrases like \"to be or not to be\"\n",
        "\n",
        "**For Modern NLP:**\n",
        "- GPU memory is still valuable, but rarely the limiting factor\n",
        "- Frequency information is actually useful for context and meaning\n",
        "- Neural models (transformers) learn that context words help determine meaning\n",
        "- Modern best practice: **Keep stop words** with neural models\n",
        "\n",
        "**The Long Tail Property Means:**\n",
        "- Most words are rare → algorithms need sufficient data to learn statistics\n",
        "- Morphological processing helps group related rare words\n",
        "- Rare words are often the most informative (e.g., \"diabetes\" > \"the\")\n",
        "- Better preprocessing → Better statistical significance → Better algorithm performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgRVOWBFrjcl"
      },
      "source": [
        "Text extraction and cleanup is an important component of real-world NLP systems. Text extraction allows one to extract text from various electronic file formats (TXT, HTML, XML, PDF, DOCX, XLSX, PPTX, ...) and deals with the encoding of the characters (Unicode, UTF-8, Code pages or ACSII).\n",
        "\n",
        "Libraries such as BeautifulSoup, Scapy or Selenium can assist you with webscraping and parsing text from HTML and XML.\n",
        "\n",
        "You can run the example hereunder to see how a Webpage is scraped and parsed into tags, which can subsequently be questioned (remove the # before this line:\"pprint(soupified.prettify())\" to see the entire HTML file (it is long)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6Ev5_K93aTA",
        "outputId": "1e93f963-930a-4e9e-fb5b-19619c1df660"
      },
      "outputs": [],
      "source": [
        "# making the necessary imports,\n",
        "from pprint import pprint\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen, Request\n",
        "import urllib\n",
        "\n",
        "myurl = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\"\n",
        "\n",
        "# Create a Request object with a user agent to mimic a browser\n",
        "req = Request(myurl, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "\n",
        "# Now use the Request object with urlopen\n",
        "html = urlopen(req).read() # query the website so that it returns a html page  \\n\"\n",
        "soupified = BeautifulSoup(html, 'html.parser') # parse the html in the 'html' variable, and store it in Beautiful Soup format\"\n",
        "\n",
        "#pprint(soupified.prettify())      # for printing the full HTML structure of the webpage\n",
        "\n",
        "question = soupified.find(\"div\", {\"class\": \"question\"}) # find the nevessary tag and class which it belongs to\n",
        "questiontext = question.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
        "print(\"Question: \\n\", questiontext.get_text().strip())\n",
        "answer = soupified.find(\"div\", {\"class\": \"answer\"}) # find the nevessary tag and class which it belongs to\n",
        "answertext = answer.find(\"div\", {\"class\": \"s-prose js-post-body\"})\n",
        "print(\"Best answer: \\n\", answertext.get_text().strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pboMwIeQwxil"
      },
      "source": [
        "PDF can be quite challenging, especially from a formatting point of view. There are also many PDF reverse engineered formats that do not follow the official PDF guideliness completely. For popular formats from Microsoft, Google, Open Office and other vendors, there are several open source libraries to exract text and meta data. For more obscure file types, one has to fall back to commercial solutions such as Oracle Outside In, but these can be expensive.\n",
        "\n",
        "Encoding normalization is important to map various variants of code pages (https://en.wikipedia.org/wiki/Code_page ), ASCII and other encodings to one common Unicode format (https://home.unicode.org/). UTF-8 is the most used one.\n",
        "\n",
        "In this tutorial, we presume all this has been done and we can start with UTF-8 text files that only contain basic line (CR-LF) and tab formatting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPfiHmSf7dmT"
      },
      "source": [
        "#NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tl18k70t1-EM"
      },
      "source": [
        "First we load NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c44v65PO2AeG",
        "outputId": "853991f0-d7c6-48c9-a985-9bfe238bb1ee"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # load tokenization\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD3v4P3lOZyE"
      },
      "source": [
        "NLTK also contains many text corpora. Let's import the movie reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "Gk1RSb6VOnDj",
        "outputId": "03ba9dab-3c9d-4894-fc36-7749a830bd5b"
      },
      "outputs": [],
      "source": [
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "movie_reviews.readme()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVqQvmAZPDeL"
      },
      "source": [
        "Let's see what is in there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POr_DqyYPFKK",
        "outputId": "66675173-5fa0-4861-8b65-194aff8a4aba"
      },
      "outputs": [],
      "source": [
        "raw = movie_reviews.raw()\n",
        "print(raw[0:1000:1]) # print first 1000 chars\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdCLI7XPPzLi"
      },
      "source": [
        "Let's see if we can detect the long tail that is typical for natural language. First we seperate the text in indivudual words, then we run a frequency analsyis on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "q-iCAp-ZP3ZZ",
        "outputId": "17f61da4-e096-4a51-8aff-2455d01d766b"
      },
      "outputs": [],
      "source": [
        "corpus = movie_reviews.words()\n",
        "print(corpus)\n",
        "freq_dist = nltk.FreqDist(corpus)\n",
        "print(freq_dist)\n",
        "print(freq_dist.most_common(50))\n",
        "freq_dist.plot(500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Observations from the Long Tail\n",
        "\n",
        "What you're seeing is **Zipf's Law in action**:\n",
        "- A few words (the, and, a, ...) dominate the corpus\n",
        "- Most words appear very rarely (thousands of unique words appear only once)\n",
        "- This creates a characteristic \"long tail\" distribution when plotted\n",
        "\n",
        "**Historical Consequence for IR:**\n",
        "For decades, systems removed these frequent words (\"stop words\") because they:\n",
        "- Waste storage space\n",
        "- Add noise to relevance calculations\n",
        "- Don't help distinguish between documents\n",
        "\n",
        "**Modern Consequence for NLP:**\n",
        "Today we know:\n",
        "- Frequency information carries meaning\n",
        "- Context words help disambiguate rare words\n",
        "- Stop word removal often HURTS neural network performance\n",
        "- Better to keep them and let the model learn what's important"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID49Xu16QW8z"
      },
      "source": [
        "## Exercise 1: Analyzing the Long Tail Distribution\n",
        "\n",
        "a. **What do you observe in the frequency distribution?**\n",
        "   - How many unique words appear only once?\n",
        "   - How many words make up 50% of the corpus?\n",
        "   - Describe the shape of the distribution\n",
        "\n",
        "b. **What are the most frequent words? Are they meaningful?**\n",
        "   - List the top 10 most frequent words\n",
        "   - Which of these would help you understand what a movie review is about?\n",
        "   - Which ones would mislead you?\n",
        "\n",
        "c. **Why is this property relevant for Information Retrieval?**\n",
        "   - If you removed the 100 most frequent words, how much would corpus size shrink?\n",
        "   - If you removed them, what queries would become impossible?\n",
        "   - Example: \"To be or not to be\" - what remains after stop word removal?\n",
        "\n",
        "d. **How can we use this property to optimize our algorithms?**\n",
        "   - What if we gave rare words higher weight in similarity calculations?\n",
        "   - What if we used morphological processing to group similar words?\n",
        "   - How would grouping \"reviewed\", \"reviews\", \"reviewer\" help?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4Tq_8i-Ut3B"
      },
      "source": [
        "ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApQiVxWzQgqs"
      },
      "source": [
        "As you can also observe, punctuation characters such as .  and , and other ones (: ; \" \" ? ! ) are still in there. This is where tokenization comes in. Tokenization removes punctuations where they are used as sentence and phrase seperation, but leaves them where they are part of a token (e.g. an email address or abbreviation).  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AfVoIBc7gh6"
      },
      "source": [
        "#Sentence Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2oTaS-W2KuL"
      },
      "source": [
        "Next, we load the NLTK tokenizer for sentences (sent_tokenize) and for words (word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9gnIhNd2JDD"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6CLqSUA29BU",
        "outputId": "8b35e77a-98fa-44b8-d754-f97297d58fe9"
      },
      "outputs": [],
      "source": [
        "my_text = \"The Department of Advanced Computing Sciences - sometimes abbreviated as DACS - \\n is Maastricht University’s largest and oldest department \\n broadly covering the fields of artificial intelligence, data science, computer science, \\n mathematics and robotics. We maintain a large network of public and \\n private partners through our research collaborations and through the \\n award-winning KE@Work programme. In addition, our staff teaches approximately 800 bachelor’s and master’s \\n students in 3 specialized study programmes in Data Science \\n and Artificial Intelligence. The Department of Advanced Computing Sciences \\n  is the new joint identity of the Institute of Data Science (IDS) and the former \\n Department of Data Science and Knowledge Engineering (DKE).\"\n",
        "print(my_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P48pIjx8XW2p",
        "outputId": "35332b5b-d3c9-413b-a973-e8987010d277"
      },
      "outputs": [],
      "source": [
        "my_sentences = sent_tokenize(my_text)\n",
        "# print(my_sentences) # print entire list unformatted\n",
        "print(\"\\n\")\n",
        "for x in range(len(my_sentences)):\n",
        "    print(my_sentences[x]+\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA5suVwx7lHD"
      },
      "source": [
        "# Word Detection aka Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Level 1 - Word-Level Tokenization\n",
        "\n",
        "## Common Misconceptions About Tokenization\n",
        "\n",
        "**Misconception 1: \"All tokenizers work the same way\"**\n",
        "- FALSE - Different tokenizers make different design choices\n",
        "- TRUTH - NLTK, spaCy, transformers, and regex tokenizers produce different results\n",
        "- Implication: Choice of tokenizer affects downstream IR/NLP performance\n",
        "\n",
        "**Misconception 2: \"Punctuation should always be removed\"**\n",
        "- FALSE - Context matters!\n",
        "- TRUTH - Some punctuation is meaningful (emails: user@domain.com, abbreviations: \"Dr.\", URLs)\n",
        "- Implication: Blind punctuation removal can lose information\n",
        "\n",
        "**Misconception 3: \"Tokenization is one-size-fits-all\"**\n",
        "- FALSE - Different domains need different strategies\n",
        "- TRUTH - Medical text, legal text, and code require domain-specific tokenizers\n",
        "- Implication: Task and domain should guide tokenization strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization Design Choices\n",
        "\n",
        "When building a tokenizer, designers must answer:\n",
        "\n",
        "1. **What is a token boundary?** (spaces, punctuation, morphemes?)\n",
        "2. **What to do with punctuation?** (remove, keep attached, separate?)\n",
        "3. **Case sensitivity?** (preserve or normalize?)\n",
        "4. **Contractions and possessives?** (\"don't\" → [\"do\", \"n't\"] or [\"don't\"]?)\n",
        "5. **Hyphenated words?** (\"award-winning\" → [\"award\", \"-\", \"winning\"] or [\"award-winning\"]?)\n",
        "6. **Special symbols?** (emails, URLs, @mentions, #hashtags?)\n",
        "\n",
        "Different answers lead to different tokenization outputs. Let's compare several strategies on the same text:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqK3BNkLX5X_"
      },
      "source": [
        "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.\n",
        "A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing. A type is the class of all tokens containing the same character sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISHpq9JAY4XX",
        "outputId": "ccd9f14b-cc34-4283-adeb-8c7822fc2d56"
      },
      "outputs": [],
      "source": [
        "for sentence in my_sentences:\n",
        "    print(\"Sentence: \"+str(sentence))\n",
        "    my_words = word_tokenize(sentence)\n",
        "    print(\"Tokens: \")\n",
        "    for x in range(len(my_words)):\n",
        "      print(\"    \"+str(my_words[x]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnvfN0OsairB"
      },
      "source": [
        "As you can observe, there are still punctuation in the list of tokens. In NLTK these can be removed by using a regular expression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uERolN74aqXt",
        "outputId": "e4141573-b317-4ce8-e03a-6a0180dca9b9"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "new_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for sentence in my_sentences:\n",
        "    print(\"Sentence: \"+str(sentence))\n",
        "    my_words = new_tokenizer.tokenize(sentence)\n",
        "    print(\"Tokens: \")\n",
        "    for x in range(len(my_words)):\n",
        "      print(\"    \"+str(my_words[x]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKEBriZ68oXI"
      },
      "source": [
        "**We can make a couple of observations:**\n",
        "\n",
        "*Standard Word Tokenization:* The tokenizer has split words effectively, separating individual words from the sentences.\n",
        "\n",
        "It correctly separates punctuation from words (e.g., no , or . in the token list).\n",
        "\n",
        "*Issues with Apostrophes and Hyphens:*\n",
        "\n",
        "a. The possessive form (bachelor’s, master’s) is split into two tokens: bachelor, s and master, s.\n",
        "\n",
        "b. award-winning is tokenized into two separate words: award and winning, instead of preserving it as a single hyphenated term.\n",
        "\n",
        "c. KE@Work was split into KE and Work, losing the @ symbol.\n",
        "\n",
        "*Inconsistencies in Named Entity Tokenization:*\n",
        "\n",
        "a. DACS and DKE are preserved correctly as single tokens.\n",
        "\n",
        "b. IDS (Institute of Data Science) is also tokenized correctly.\n",
        "\n",
        "*Potentially Incorrect Splitting of Multi-word Terms*\n",
        "\n",
        "a. Artificial Intelligence, Data Science, and Knowledge Engineering are split into separate words.\n",
        "\n",
        "b. This could be problematic in contexts where multi-word expressions (MWEs) are important.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhgIu4uKcbmv"
      },
      "source": [
        "#Exercise 2:\n",
        "\n",
        "a. How can we improve this tokenization?\n",
        "\n",
        "b. Why is this relevant for Information Retrieval (IR) and Natural Language Processing (NLP)?\n",
        "\n",
        "c. How can we use this knowledge to optimize IR and NLP algorithms?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL39SsVJc3QQ"
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Case Studies: When Tokenization Fails (or Succeeds)\n",
        "\n",
        "**E-commerce:**\n",
        "- Product: \"iPhone 13 Pro Max\"\n",
        "- Poor tokenization: splits \"Pro\" and \"Max\" apart, hurting product search\n",
        "- Better tokenization: keeps \"Pro Max\" as a tier concept; queries for \"Pro Max\" return the right devices\n",
        "\n",
        "**Medical:**\n",
        "- Term: \"Type 2 Diabetes Mellitus\"\n",
        "- Poor tokenization: [\"Type\", \"2\", \"Diabetes\", \"Mellitus\"] treats them independently\n",
        "- Better tokenization: recognizes it as one clinical concept → improves coding, recommendations\n",
        "\n",
        "**Legal:**\n",
        "- Phrase: \"Act of God\"\n",
        "- Poor tokenization: three unrelated words\n",
        "- Better tokenization: one legal/insurance concept → affects risk and liability analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-OCi_nxIPYP"
      },
      "source": [
        "# Generate a Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STaUFJPJJIrl"
      },
      "source": [
        "A vocabulary is a data structure containing every unique word used in the corpus only once and in alphabetical order. This can be used as a dictionairy in NLP or as the basis of a search index in information retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guS6cWr6ISTU",
        "outputId": "411f90c1-7632-475f-92ab-8cfae2bdb5c4"
      },
      "outputs": [],
      "source": [
        "corpus_tokens = new_tokenizer.tokenize(my_text.lower()) #use the tokenizer that removes punctuation\n",
        "vocab = sorted(set(corpus_tokens))\n",
        "print(vocab)\n",
        "print(\"Tokens:\", len(corpus_tokens))\n",
        "print(\"Vocabulary:\", len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51E04B3g7pz6"
      },
      "source": [
        "#Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxnKcmMt4tDr"
      },
      "source": [
        "In the past, when computer resources were still limited, highly frequent words were often removed in information retrieval applications. These are named stop-words or noise-words. These are words such as \"the, on, in, a, be, or, and, an, for, to, ...\". If such a word is removed, one can no longer search for them. Imagine searching for \"to be or not to be\", which is no longer after noise word removal.\n",
        "\n",
        "In text-mining and advanced NLP, these words often contain important clues on the meaning of language.\n",
        "\n",
        "So, these days as computr resources are much larger, noise words are more often not removed.\n",
        "\n",
        "But let's try how to remoce them using NLTK."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## When NOT to Remove Stop Words (Modern Perspective)\n",
        "\n",
        "- Queries can break: \"to be or not to be\" loses meaning if stop words are removed\n",
        "- Negation matters: \"not good\" vs \"good\" → removing \"not\" flips sentiment\n",
        "- Questions: \"what is AI?\" → removing \"what\" hides the intent\n",
        "- Neural models (BERT, GPT, etc.) use stop words as context; removing them usually hurts performance\n",
        "\n",
        "**Guideline:**\n",
        "- Keep stop words for modern neural models and contextual tasks\n",
        "- Consider removal only for classic IR pipelines where storage/latency is critical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95Xp0Zlc5qOM",
        "outputId": "e6404fb5-98d1-46a4-b647-b5aa1839baf0"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "corpus_tokens = new_tokenizer.tokenize(my_text)\n",
        "print(\"Stopwords from NLTK:\", stopwords.words('english'))\n",
        "stop_words_nltk = set(stopwords.words('english'))\n",
        "# we use the token list without punctuations\n",
        "print(\"Tokenized corpus:\",corpus_tokens)\n",
        "#now remove stopwords\n",
        "tokenized_corpus_without_stopwords = [i for i in corpus_tokens if not i in stop_words_nltk]\n",
        "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stop Words in Context: Small Experiments\n",
        "\n",
        "Try these thought experiments:\n",
        "- Sentiment: \"This movie is not good\" → removing \"not\" changes polarity\n",
        "- Question answering: \"What is artificial intelligence?\" → removing \"what\" hides that it is a question\n",
        "- Query exactness: \"to be or not to be\" → removing stop words destroys the phrase\n",
        "\n",
        "**Takeaway:** Keep stop words when context is important (most modern NLP tasks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7bgF2uT7P0E"
      },
      "source": [
        "#Exercise 3:\n",
        "\n",
        "a. What do you observe with respect to case sensitivity?\n",
        "\n",
        "b. How can we solve that?\n",
        "\n",
        "c. Could this actions also lead to unwanted side effects?\n",
        "\n",
        "d. When using highly-context sensitive models such as BERT, expplain why removing stopwords is not a good idea.\n",
        "\n",
        "\n",
        "\n",
        "ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing Between Stemming, Lemmatization, or No Processing\n",
        "\n",
        "| Word           | No Processing | Stemming  | Lemmatization | Correct Lemma |\n",
        "|----------------|---------------|-----------|---------------|---------------|\n",
        "| better         | better        | better    | better        | good          |\n",
        "| is             | is            | is        | is            | be            |\n",
        "| running        | running       | run       | running       | run (verb)    |\n",
        "| richer         | richer        | richer    | richer        | rich (adj)    |\n",
        "\n",
        "**When to use which?**\n",
        "- **Stemming:** fast, good for search engines where speed > linguistic accuracy.\n",
        "- **Lemmatization:** better linguistic correctness, needs POS tags; good for text analysis/extraction.\n",
        "- **No processing (keep as-is):** best for modern neural models (BERT/GPT) which handle morphology internally."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsjAz4lF7Y0K"
      },
      "source": [
        "#Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UYjccj-RvK8"
      },
      "source": [
        "Stemming is the process of removing suffixes and reducing the word to some base form such that all different variations of a word can be represented by one form. Stemming uses rules and may not always result in the correct linguistic base form. However, it is fast and therefor often used by search engines. As we discussed in the lecture, a well-known stemmer for the English language is the Porter stemmer.\n",
        "\n",
        "Let's try it ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o3TscOrSXGT",
        "outputId": "771b9cc9-d74f-4044-a98c-4f72f11d3505"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer= PorterStemmer()\n",
        "print(\"before stemming -> after stemming\")\n",
        "for word in corpus_tokens:\n",
        "  print(str(word) + \" -> \" + str(stemmer.stem(word)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XnibDb_Xs1y"
      },
      "source": [
        "As you can see, \"students\" is converted into \"student\", but \"Science\" is converted into \"scien\". There are other non-linguistically correct transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDc2SVqq7w2a"
      },
      "source": [
        "#Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kav2cXbpYKa9"
      },
      "source": [
        "This why we prefer to use lemmatization for linguistic applications other than search engines. Lemmatization is the process of mapping all tokens to its base-linguistic form: the \"lemma\". So \"better\" should be converted to \"good\" and \"is\" to \"be\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv5796gKY8C7",
        "outputId": "8990dede-49d9-4923-c746-9a986232f582"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # downloading wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "print(\"before lemmatization -> after lemmarization\")\n",
        "for word in corpus_tokens:\n",
        "  print(str(word) + \" -> \" + str(lemmatizer.lemmatize(word)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4l3z9ajbeIs"
      },
      "source": [
        "As you can observe, only plurals and other basic operations are performed. But \"is\" not converted to \"be\". Neither are several verb inflections. This is because Lemmatization requires more linguistic knowledge: it need to know whether we are dealing with, for instance, a verb, noun or a adjectice. We call these gramatical roles \"part-of-speech\" or POS tags. These will be discussed in the next lecture: Syntax and Semantics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Cq2Xx2b4I6",
        "outputId": "adf6904c-53a7-43e4-e983-6a276252c216"
      },
      "outputs": [],
      "source": [
        "print(lemmatizer.lemmatize('better'))\n",
        "print(lemmatizer.lemmatize('better',pos='a')) # a for Adjective\n",
        "print(lemmatizer.lemmatize('is'))\n",
        "print(lemmatizer.lemmatize('is',pos='v'))  # v for Verb\n",
        "print(lemmatizer.lemmatize('is',pos='a'))\n",
        "print(lemmatizer.lemmatize('is',pos='n'))  # n for Noun\n",
        "print(lemmatizer.lemmatize('richer',pos='n'))\n",
        "print(lemmatizer.lemmatize('richer',pos='a'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 5: The Tokenization Revolution – From Words to Subwords\n",
        "\n",
        "Traditional tokenizers split on whitespace/punctuation and rely on fixed vocabularies. Modern transformer models (BERT, RoBERTa, etc.) use **subword tokenization** (e.g., WordPiece) to:\n",
        "- Handle rare and out-of-vocabulary (OOV) words gracefully\n",
        "- Capture morphology (play + ##ing, play + ##er)\n",
        "- Support multilingual text with a compact vocabulary\n",
        "- Reduce [UNK] tokens by breaking words into known pieces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## WordPiece Tokenization (Mandatory)\n",
        "\n",
        "WordPiece is the subword tokenizer used by BERT-family models. It:\n",
        "- Uses a ~30k subword vocabulary\n",
        "- Splits unseen/rare words into known pieces (prefix `##` marks continuation)\n",
        "- Greatly reduces `[UNK]` tokens\n",
        "- Is standard for modern transformer pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install once if not available (uncomment when running in a fresh environment)\n",
        "# !pip install transformers torch\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "wordpiece_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "text = \"Playing football is unfathomable.\"\n",
        "print(\"Text:\", text)\n",
        "print(\"WordPiece tokens:\", wordpiece_tokenizer.tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word-level vs WordPiece comparison on tricky examples\n",
        "\n",
        "examples = {\n",
        "    \"Simple\": \"The cat sat on the mat.\",\n",
        "    \"Morphology\": \"The players are playing, having played.\",\n",
        "    \"Rare\": \"The unfathomable astrophysicist invented something bewildering.\",\n",
        "    \"Possessive\": \"John's dog's toy is here.\",\n",
        "    \"Contractions\": \"don't won't can't shouldn't\",\n",
        "    \"Hyphenated\": \"award-winning state-of-the-art cutting-edge\",\n",
        "    \"Email\": \"Contact us at john.doe@example.com for info.\",\n",
        "    \"Numbers\": \"The cost is $49.99 and USD 50.\",\n",
        "}\n",
        "\n",
        "print(\"Name | NLTK | WordPiece\")\n",
        "print(\"=\" * 70)\n",
        "for name, text in examples.items():\n",
        "    nltk_tokens = word_tokenize(text)\n",
        "    wp_tokens = wordpiece_tokenizer.tokenize(text)\n",
        "    print(f\"\\n{name}: {text}\")\n",
        "    print(f\"  NLTK:      {nltk_tokens}\")\n",
        "    print(f\"  WordPiece: {wp_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What to Notice\n",
        "- Morphology: WordPiece captures roots (play + ##ing/##ed/##er); NLTK keeps full forms\n",
        "- Rare words: WordPiece splits into subwords instead of `[UNK]`\n",
        "- Contractions: WordPiece splits more aggressively; choose based on task\n",
        "- Emails/URLs: Both split; domain-specific tokenizers may be better\n",
        "- Numbers/currency: WordPiece keeps punctuation separated, which models expect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect token IDs (what models actually consume)\n",
        "text = \"Playing football is unfathomable.\"\n",
        "wp_tokens = wordpiece_tokenizer.tokenize(text)\n",
        "wp_ids = wordpiece_tokenizer.convert_tokens_to_ids(wp_tokens)\n",
        "print(\"Tokens:\", wp_tokens)\n",
        "print(\"IDs:\", wp_ids)\n",
        "print(\"Decoded back:\", wordpiece_tokenizer.convert_ids_to_tokens(wp_ids))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Which Tokenizer to Use?\n",
        "- **NLTK word_tokenize:** legacy IR, quick prototyping, when you need readable tokens\n",
        "- **NLTK + lemmatization:** classical ML pipelines, linguistically interpretable features\n",
        "- **WordPiece (transformers):** modern NLP tasks, robustness to rare/OOV words, multilingual support, production models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE6ntyME72S0"
      },
      "source": [
        "# Text-Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyicUNL2enhr"
      },
      "source": [
        "In social media, one can run in short-cuts, slang, hash-tags, or emoticons. These can be concerted to their textual forms. Phone numbers, dates and monataire amounts can be written in many different forms. Sometimes, one can even decide to convert all text to either lower case or upper case. This may cause problems in some applications and should be used carefully. We will discuss this in more detail in the course Text Mining, where this is more important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeGSKRnx78Ry"
      },
      "source": [
        "#Language Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhONHqV2fiU6"
      },
      "source": [
        "Almost all NLP models and algorithms are very language specific: this means that one can only use them with the intenred language. Using them on other language will result in random behavior.  \n",
        "\n",
        "So, language detection (often per sentence or minimally per paragrpah) is essential for any type of NLP application to perform correctly!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njzaEIDBgFAZ",
        "outputId": "71f35e26-58fa-4852-ad3f-09daaaeaca57"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect\n",
        "from langdetect import detect, detect_langs\n",
        "def language_detection(text, method = \"single\"):\n",
        "  if(method.lower() != \"single\"):\n",
        "    result = detect_langs(text)\n",
        "  else:\n",
        "    result = detect(text)\n",
        "  return result\n",
        "\n",
        "multilingual_text = \"Elle est vraiment éfficace dans la détection de langue.\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"Het is enorm makkelijk om een taal te herkennen!\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"Es ist wirklich effektiv bei der Spracherkennung.\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"Nó thực sự hiệu quả trong việc phát hiện ngôn ngữ.\"\n",
        "print(language_detection(multilingual_text))\n",
        "multilingual_text = \"إنه فعال حقًا في اكتشاف اللغة.\"\n",
        "print(language_detection(multilingual_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmCN9E977_mi"
      },
      "source": [
        "# Transliteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "munkN882hiya"
      },
      "source": [
        "Transliteration refers to the method of mapping from one system of writing to another based on phonetic similarity. With this tool, you type in Latin letters (e.g. a, b, c etc.), which are converted to characters that have similar pronunciation in the target language. For transliteration, you need to select the target language. So, results for a transliteration of a Arabic name into English, French or German can be very different for similar names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ciweSsXiu0B"
      },
      "source": [
        "Лев Николаевич Толстой\n",
        "\n",
        "results in different forms of transliteration for different target languages:\n",
        "\n",
        "Lev Nikolayevich Tolstoy\n",
        "\n",
        "Léon Tolstoï\n",
        "\n",
        "Lev Tolstoj\n",
        "\n",
        "León Tolstó\n",
        "\n",
        "Lev Tolstoy\n",
        "\n",
        "Lav Tolstoj\n",
        "\n",
        "Lev Tolsto\n",
        "\n",
        "Liuni Tolstoi\n",
        "\n",
        "Ļevs Tolstojs\n",
        "\n",
        "Levs Tuolstuos\n",
        "\n",
        "...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practical Example: Transliterating Russian to Different Target Languages\n",
        "\n",
        "Let's see how the same Russian text gets transliterated differently depending on the target language rules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install transliteration library (uncomment if needed)\n",
        "# !pip install transliterate\n",
        "\n",
        "from transliterate import translit\n",
        "\n",
        "# Original Russian text\n",
        "russian_text = \"Лев Николаевич Толстой\"\n",
        "print(f\"Original (Russian/Cyrillic): {russian_text}\\n\")\n",
        "\n",
        "# Transliterate to different target languages using different standards\n",
        "# English transliteration (most common)\n",
        "english = translit(russian_text, 'ru', reversed=True)\n",
        "print(f\"English transliteration: {english}\")\n",
        "\n",
        "# For demonstration, let's also show different romanization schemes\n",
        "# Using language_code parameter to show variations\n",
        "\n",
        "# Note: The transliterate library primarily uses one standard scheme\n",
        "# For more language-specific rules, we'd need specialized libraries or APIs\n",
        "\n",
        "# Let's demonstrate with examples showing how output differs by phonetic conventions:\n",
        "examples = {\n",
        "    \"Russian\": \"Лев Николаевич Толстой\",\n",
        "    \"Arabic\": \"محمد\",\n",
        "    \"Greek\": \"Αριστοτέλης\",\n",
        "    \"Chinese\": \"李白\"\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Transliteration Examples:\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for lang, text in examples.items():\n",
        "    try:\n",
        "        if lang == \"Russian\":\n",
        "            result = translit(text, 'ru', reversed=True)\n",
        "        elif lang == \"Greek\":\n",
        "            result = translit(text, 'el', reversed=True)\n",
        "        else:\n",
        "            result = f\"(Requires language-specific module for {lang})\"\n",
        "        print(f\"{lang:12} | {text:20} → {result}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{lang:12} | {text:20} → Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Manual Demonstration: Language-Specific Transliteration Rules\n",
        "\n",
        "The same Russian name \"Лев Николаевич Толстой\" becomes different when transliterated according to different target language phonetic rules:\n",
        "\n",
        "**Target: English**\n",
        "- Result: \"Lev Nikolayevich Tolstoy\"\n",
        "- Rules: English phonetics (y for й, oy for ой)\n",
        "\n",
        "**Target: French**\n",
        "- Result: \"Léon Tolstoï\" \n",
        "- Rules: French phonetics (é for е, ï for ой, drops patronymic)\n",
        "\n",
        "**Target: German**\n",
        "- Result: \"Leo Tolstoi\" or \"Lew Tolstoi\"\n",
        "- Rules: German phonetics (Leo for Лев, i for й)\n",
        "\n",
        "**Target: Spanish**\n",
        "- Result: \"León Tolstói\"\n",
        "- Rules: Spanish phonetics (ó for ой)\n",
        "\n",
        "**Why this matters for NLP:**\n",
        "- Named entity recognition across languages\n",
        "- Search systems need to match variants (\"Tolstoy\" = \"Tolstoi\" = \"Tolstoï\")\n",
        "- Machine translation quality\n",
        "- Cross-lingual information retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# More practical example: showing how different systems handle the same text\n",
        "# Using unidecode as an alternative that shows ASCII approximation\n",
        "\n",
        "# !pip install unidecode\n",
        "from unidecode import unidecode\n",
        "\n",
        "test_names = {\n",
        "    \"Russian\": \"Лев Николаевич Толстой\",\n",
        "    \"Arabic\": \"محمد بن عبد الله\",\n",
        "    \"Chinese\": \"毛泽东\",\n",
        "    \"Japanese\": \"山田太郎\",\n",
        "    \"Greek\": \"Αριστοτέλης\",\n",
        "    \"Hebrew\": \"משה רבנו\"\n",
        "}\n",
        "\n",
        "print(\"Universal ASCII Transliteration (using unidecode):\")\n",
        "print(\"=\"*70)\n",
        "for lang, name in test_names.items():\n",
        "    ascii_version = unidecode(name)\n",
        "    print(f\"{lang:12} | {name:25} → {ascii_version}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Note: Different target languages would have different conventions!\")\n",
        "print(\"- English: 'Lev Tolstoy'\")\n",
        "print(\"- French:  'Léon Tolstoï'\") \n",
        "print(\"- German:  'Leo Tolstoi'\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvS5d3LEiNcp"
      },
      "source": [
        "A Python library for transliteration can be found here: https://pypi.org/project/transliterate/. We will discuss this in more detail in the lecture on Machine Translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assessment: Tokenization Mastery Exercises\n",
        "\n",
        "**Total Points: 90 + 10 bonus**\n",
        "\n",
        "This assessment section tests your understanding of tokenization concepts through hands-on coding exercises. Each exercise includes:\n",
        "- **Auto-graded code** (tested automatically)\n",
        "- **Manual explanation** (graded by instructor)\n",
        "\n",
        "**Estimated time:** 2-3 hours\n",
        "\n",
        "**Instructions:**\n",
        "1. Write your code in cells marked `# YOUR CODE HERE`\n",
        "2. Remove the line `raise NotImplementedError()` when you add your solution\n",
        "3. Write explanations in markdown cells marked `YOUR ANSWER:`\n",
        "4. Run all cells to verify your code works before submitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise A1: Basic Tokenization Comparison (10 points)\n",
        "\n",
        "Compare different tokenization approaches on challenging text examples.\n",
        "\n",
        "**Learning objectives:**\n",
        "- Understand when simple splitting fails\n",
        "- Compare NLTK word_tokenize vs RegexpTokenizer\n",
        "- Identify edge cases in tokenization\n",
        "\n",
        "**Points:** 6 auto-graded + 4 manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "A1_solution",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "\n",
        "# Test cases with challenging tokenization scenarios\n",
        "test_cases = [\n",
        "    \"Don't split contractions incorrectly!\",\n",
        "    \"Email: john.doe@example.com\",\n",
        "    \"Dr. Smith works at U.S.A. headquarters.\",\n",
        "    \"Cost: $49.99 (approximately 50 USD)\",\n",
        "]\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Task 1: Tokenize each test case using THREE methods:\n",
        "# 1. Simple .split()\n",
        "# 2. NLTK word_tokenize()  \n",
        "# 3. RegexpTokenizer with pattern r'\\w+'\n",
        "\n",
        "# Store results in these variables:\n",
        "simple_split_results = []\n",
        "nltk_results = []\n",
        "regex_results = []\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "for text in test_cases:\n",
        "    simple_split_results.append(text.split())\n",
        "    nltk_results.append(word_tokenize(text))\n",
        "    \n",
        "regex_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "for text in test_cases:\n",
        "    regex_results.append(regex_tokenizer.tokenize(text))\n",
        "### END SOLUTION\n",
        "\n",
        "# Display results (for your reference)\n",
        "for i, text in enumerate(test_cases):\n",
        "    print(f\"\\nText {i+1}: {text}\")\n",
        "    print(f\"  Simple split:  {simple_split_results[i]}\")\n",
        "    print(f\"  NLTK:          {nltk_results[i]}\")\n",
        "    print(f\"  Regex r'\\\\w+':  {regex_results[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "A1_tests",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# AUTO-GRADED TESTS (6 points)\n",
        "# Do not modify this cell\n",
        "\n",
        "### BEGIN HIDDEN TESTS\n",
        "# Test 1: Check all lists are populated (2 pts)\n",
        "assert len(simple_split_results) == 4, \"simple_split_results should have 4 results\"\n",
        "assert len(nltk_results) == 4, \"nltk_results should have 4 results\"\n",
        "assert len(regex_results) == 4, \"regex_results should have 4 results\"\n",
        "\n",
        "# Test 2: NLTK handles contractions better than simple split (2 pts)\n",
        "# \"Don't\" should be split by NLTK but not by simple split\n",
        "assert \"Don't\" in simple_split_results[0] or \"Don\" in simple_split_results[0], \"Check simple split result\"\n",
        "assert \"Do\" in nltk_results[0] or \"Don\" in nltk_results[0], \"NLTK should handle contractions\"\n",
        "\n",
        "# Test 3: Regex removes punctuation (2 pts)\n",
        "# The regex pattern r'\\w+' should not include punctuation tokens\n",
        "assert \"$\" not in regex_results[3], \"Regex r'\\\\w+' should remove $ symbol\"\n",
        "assert \".\" not in regex_results[2], \"Regex r'\\\\w+' should remove periods\"\n",
        "### END HIDDEN TESTS\n",
        "\n",
        "print(\"All auto-graded tests passed! ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A1 Explanation (4 points - manually graded)\n",
        "\n",
        "**Question 1:** For each test case, which tokenization method works best and why?\n",
        "\n",
        "**Question 2:** What information is lost when using the regex pattern `r'\\w+'` compared to NLTK word_tokenize?\n",
        "\n",
        "**Question 3:** In what scenarios would simple `.split()` actually be the right choice?\n",
        "\n",
        "**YOUR ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise A2: Regex Pattern Design (15 points)\n",
        "\n",
        "Design a regex tokenizer that handles emails and URLs correctly.\n",
        "\n",
        "**Learning objectives:**\n",
        "- Create custom regex patterns for tokenization\n",
        "- Handle special cases (emails, URLs)\n",
        "- Understand regex complexity vs accuracy trade-offs\n",
        "\n",
        "**Points:** 10 auto-graded + 5 manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "A2_solution",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# Test text with emails, URLs, and regular words\n",
        "test_text = \"Contact support@company.com or visit https://example.com. Don't forget www.site.org!\"\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Create a RegexpTokenizer that:\n",
        "# 1. Keeps email addresses as single tokens\n",
        "# 2. Keeps URLs (http://, https://, www.) as single tokens\n",
        "# 3. Handles regular words\n",
        "# 4. Handles contractions (your choice: split or keep together)\n",
        "#\n",
        "# Hint: Use the | operator to combine multiple patterns\n",
        "# Pattern order matters! Check emails and URLs before words\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Pattern explanation:\n",
        "# 1. [\\w.+-]+@[\\w.-]+\\.[A-Za-z]{2,}  - matches email addresses\n",
        "# 2. https?://[\\w./-]+                - matches http:// or https:// URLs\n",
        "# 3. www\\.[\\w./-]+                    - matches www. URLs\n",
        "# 4. \\w+                              - matches regular words\n",
        "# 5. \\S                               - matches any remaining non-whitespace (fallback)\n",
        "\n",
        "improved_pattern = r'[\\w.+-]+@[\\w.-]+\\.[A-Za-z]{2,}|https?://[\\w./-]+|www\\.[\\w./-]+|\\w+|\\S'\n",
        "improved_tokenizer = RegexpTokenizer(improved_pattern)\n",
        "### END SOLUTION\n",
        "\n",
        "tokens = improved_tokenizer.tokenize(test_text)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(f\"Total tokens: {len(tokens)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "A2_tests",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# AUTO-GRADED TESTS (10 points)\n",
        "# Do not modify this cell\n",
        "\n",
        "### BEGIN HIDDEN TESTS\n",
        "# Test 1: Email preserved as single token (3 pts)\n",
        "assert 'support@company.com' in tokens, \"Email should be kept as single token\"\n",
        "\n",
        "# Test 2: HTTPS URL preserved (3 pts)\n",
        "assert any('https://example.com' in t for t in tokens), \"HTTPS URL should be preserved\"\n",
        "\n",
        "# Test 3: WWW URL preserved (2 pts)\n",
        "assert any('www.site.org' in t for t in tokens), \"WWW URL should be preserved\"\n",
        "\n",
        "# Test 4: Regular words still tokenized (2 pts)\n",
        "assert 'Contact' in tokens or 'contact' in tokens, \"Regular words should be tokenized\"\n",
        "assert 'visit' in tokens or 'Visit' in tokens, \"Regular words should be tokenized\"\n",
        "### END HIDDEN TESTS\n",
        "\n",
        "print(\"All auto-graded tests passed! ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A2 Explanation (5 points - manually graded)\n",
        "\n",
        "**Question 1:** Explain your regex pattern design. What does each part do?\n",
        "\n",
        "**Question 2:** Why is the order of patterns important in your regex (hint: what happens if you put `\\w+` first)?\n",
        "\n",
        "**Question 3:** What edge cases might your pattern still fail on? Give examples.\n",
        "\n",
        "**YOUR ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise A3: Stop Words Impact Analysis (10 points)\n",
        "\n",
        "Analyze when stop word removal helps vs. hurts.\n",
        "\n",
        "**Learning objectives:**\n",
        "- Implement stop word removal\n",
        "- Identify queries that break after stop word removal\n",
        "- Understand when NOT to remove stop words\n",
        "\n",
        "**Points:** 5 auto-graded + 5 manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "A3_solution",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download stopwords if needed\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "# Test queries that may break after stop word removal\n",
        "queries = [\n",
        "    \"to be or not to be\",\n",
        "    \"the who concert tickets\",\n",
        "    \"not good at all\",\n",
        "    \"what is artificial intelligence\",\n",
        "    \"flights to New York\"\n",
        "]\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Task: For each query:\n",
        "# 1. Tokenize it\n",
        "# 2. Remove stop words\n",
        "# 3. Store remaining tokens\n",
        "# 4. Calculate how many tokens were removed\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Store results in these lists:\n",
        "original_tokens = []\n",
        "filtered_tokens = []\n",
        "tokens_removed_count = []\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "for query in queries:\n",
        "    tokens = word_tokenize(query.lower())\n",
        "    filtered = [t for t in tokens if t not in stop_words]\n",
        "    \n",
        "    original_tokens.append(tokens)\n",
        "    filtered_tokens.append(filtered)\n",
        "    tokens_removed_count.append(len(tokens) - len(filtered))\n",
        "### END SOLUTION\n",
        "\n",
        "# Display results\n",
        "for i, query in enumerate(queries):\n",
        "    print(f\"\\nQuery: '{query}'\")\n",
        "    print(f\"  Original: {original_tokens[i]}\")\n",
        "    print(f\"  After removal: {filtered_tokens[i]}\")\n",
        "    print(f\"  Removed: {tokens_removed_count[i]} tokens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "A3_tests",
          "locked": true,
          "points": 10,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# AUTO-GRADED TESTS (5 points)\n",
        "# Do not modify this cell\n",
        "\n",
        "### BEGIN HIDDEN TESTS\n",
        "# Test 1: Lists have correct length (1 pt)\n",
        "assert len(filtered_tokens) == 5, \"Should have 5 filtered results\"\n",
        "\n",
        "# Test 2: Stop words actually removed (2 pts)\n",
        "assert len(filtered_tokens[0]) < len(original_tokens[0]), \"Tokens should be removed from query 1\"\n",
        "assert sum(tokens_removed_count) > 0, \"Some tokens should be removed overall\"\n",
        "\n",
        "# Test 3: Query 1 \"to be or not to be\" should lose most tokens (2 pts)\n",
        "assert len(filtered_tokens[0]) <= 2, \"Query 1 should lose most words (almost all are stop words)\"\n",
        "assert tokens_removed_count[0] >= 4, \"Query 1 should remove at least 4 stop words\"\n",
        "### END HIDDEN TESTS\n",
        "\n",
        "print(\"All auto-graded tests passed! ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A3 Explanation (5 points - manually graded)\n",
        "\n",
        "**Question 1:** Which queries become meaningless or impossible to search after stop word removal? Explain why.\n",
        "\n",
        "**Question 2:** For query \"not good at all\", what critical information is lost by removing \"not\"?\n",
        "\n",
        "**Question 3:** In what types of NLP applications would you NEVER remove stop words? Give at least 3 examples with justification.\n",
        "\n",
        "**YOUR ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise A4: Morphological Processing Decisions (15 points)\n",
        "\n",
        "Apply stemming and lemmatization, then decide which approach suits different use cases.\n",
        "\n",
        "**Learning objectives:**\n",
        "- Implement both stemming and lemmatization\n",
        "- Compare their outputs and accuracy\n",
        "- Make justified decisions for different scenarios\n",
        "\n",
        "**Points:** 8 auto-graded + 7 manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "A4_solution",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Test words with different morphological variations\n",
        "test_words = ['running', 'runs', 'ran', 'runner', 'better', 'best', 'good', \n",
        "              'studies', 'studying', 'studied', 'science', 'sciences', 'scientific']\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Task: Process each word using both stemming and lemmatization\n",
        "# Store results in these dictionaries:\n",
        "\n",
        "stemmed_results = {}\n",
        "lemmatized_results = {}\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "for word in test_words:\n",
        "    stemmed_results[word] = stemmer.stem(word)\n",
        "    lemmatized_results[word] = lemmatizer.lemmatize(word)\n",
        "### END SOLUTION\n",
        "\n",
        "# Display comparison\n",
        "print(f\"{'Word':<15} {'Stemmed':<15} {'Lemmatized':<15}\")\n",
        "print(\"=\"*50)\n",
        "for word in test_words:\n",
        "    print(f\"{word:<15} {stemmed_results[word]:<15} {lemmatized_results[word]:<15}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "A4_tests",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# AUTO-GRADED TESTS (8 points)\n",
        "# Do not modify this cell\n",
        "\n",
        "### BEGIN HIDDEN TESTS\n",
        "# Test 1: Dictionaries populated (2 pts)\n",
        "assert len(stemmed_results) == len(test_words), \"All words should be stemmed\"\n",
        "assert len(lemmatized_results) == len(test_words), \"All words should be lemmatized\"\n",
        "\n",
        "# Test 2: Stemming groups morphological variants (3 pts)\n",
        "# running, runs, ran should stem to similar forms\n",
        "assert stemmed_results['running'][:3] == stemmed_results['runs'][:3], \"run-related words should stem similarly\"\n",
        "\n",
        "# Test 3: Stemming may be inaccurate (1 pt)\n",
        "# \"science\" might become \"scienc\" (not linguistically correct)\n",
        "assert stemmed_results['science'] != 'science' or stemmed_results['sciences'] == 'scienc', \"Stemming may truncate\"\n",
        "\n",
        "# Test 4: Lemmatization preserves valid words (2 pts)\n",
        "assert lemmatized_results['science'] == 'science', \"Lemmatization should keep 'science' as is\"\n",
        "assert lemmatized_results['running'] == 'running', \"Without POS, 'running' stays as-is\"\n",
        "### END HIDDEN TESTS\n",
        "\n",
        "print(\"All auto-graded tests passed! ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A4 Explanation (7 points - manually graded)\n",
        "\n",
        "For each use case below, choose: **No processing**, **Stemming**, or **Lemmatization**. Justify your choice.\n",
        "\n",
        "**Use Case 1: Search Engine**  \n",
        "Users search for \"running shoes\" but products are tagged with \"run\", \"running\", \"runner\".  \n",
        "Your choice: ___________  \n",
        "Justification:\n",
        "\n",
        "**Use Case 2: Sentiment Analysis**  \n",
        "Analyzing if \"better\", \"best\", and \"good\" all express positive sentiment.  \n",
        "Your choice: ___________  \n",
        "Justification:\n",
        "\n",
        "**Use Case 3: Named Entity Recognition**  \n",
        "Extracting company names like \"Bloomberg\" or \"Maastricht\" (must not become \"Bloom\" or \"Maastricht\").  \n",
        "Your choice: ___________  \n",
        "Justification:\n",
        "\n",
        "**Use Case 4: BERT Text Classification**  \n",
        "Feeding text to a pre-trained BERT model for classification.  \n",
        "Your choice: ___________  \n",
        "Justification:\n",
        "\n",
        "**YOUR ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise A5: WordPiece vs Word-Level Comparison (15 points)\n",
        "\n",
        "Compare how WordPiece handles rare/unknown words versus traditional tokenization.\n",
        "\n",
        "**Learning objectives:**\n",
        "- Implement WordPiece tokenization\n",
        "- Understand subword tokenization benefits\n",
        "- Compare vocabulary sizes and OOV handling\n",
        "\n",
        "**Points:** 10 auto-graded + 5 manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "A5_solution",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Note: Uncomment the next line if transformers is not installed\n",
        "# !pip install transformers -q\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Test sentences with rare/technical words\n",
        "test_sentences = [\n",
        "    \"The unfathomable astrophysicist discovered antimatter.\",\n",
        "    \"COVID-19 pandemic caused unprecedented lockdowns.\",\n",
        "    \"Antidisestablishmentarianism is a long word.\",\n",
        "]\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Task: Tokenize each sentence using:\n",
        "# 1. NLTK word_tokenize\n",
        "# 2. WordPiece (bert-base-uncased)\n",
        "# Compare the number of tokens and vocabulary sizes\n",
        "\n",
        "nltk_tokens_list = []\n",
        "wordpiece_tokens_list = []\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "wordpiece_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    nltk_tokens = word_tokenize(sentence)\n",
        "    wp_tokens = wordpiece_tokenizer.tokenize(sentence)\n",
        "    \n",
        "    nltk_tokens_list.append(nltk_tokens)\n",
        "    wordpiece_tokens_list.append(wp_tokens)\n",
        "### END SOLUTION\n",
        "\n",
        "# Display results\n",
        "for i, sentence in enumerate(test_sentences):\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(f\"  NLTK ({len(nltk_tokens_list[i])} tokens): {nltk_tokens_list[i]}\")\n",
        "    print(f\"  WordPiece ({len(wordpiece_tokens_list[i])} tokens): {wordpiece_tokens_list[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "A5_tests",
          "locked": true,
          "points": 15,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# AUTO-GRADED TESTS (10 points)\n",
        "# Do not modify this cell\n",
        "\n",
        "### BEGIN HIDDEN TESTS\n",
        "# Test 1: Both tokenizers used (2 pts)\n",
        "assert len(nltk_tokens_list) == 3, \"Should tokenize all 3 sentences with NLTK\"\n",
        "assert len(wordpiece_tokens_list) == 3, \"Should tokenize all 3 sentences with WordPiece\"\n",
        "\n",
        "# Test 2: WordPiece splits rare words (4 pts)\n",
        "# Check if \"unfathomable\" or similar rare words are split\n",
        "wp_sent1 = ' '.join(wordpiece_tokens_list[0])\n",
        "assert '##' in wp_sent1, \"WordPiece should use ## for subwords in rare words\"\n",
        "\n",
        "# Test 3: WordPiece generally produces more tokens for rare words (2 pts)\n",
        "# For sentence with \"Antidisestablishmentarianism\"\n",
        "assert len(wordpiece_tokens_list[2]) >= len(nltk_tokens_list[2]), \"WordPiece may split long words more\"\n",
        "\n",
        "# Test 4: Check tokenization actually happened (2 pts)\n",
        "assert len(nltk_tokens_list[0]) > 0, \"NLTK should produce tokens\"\n",
        "assert len(wordpiece_tokens_list[0]) > 0, \"WordPiece should produce tokens\"\n",
        "### END HIDDEN TESTS\n",
        "\n",
        "print(\"All auto-graded tests passed! ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A5 Explanation (5 points - manually graded)\n",
        "\n",
        "**Question 1:** How does WordPiece handle the word \"unfathomable\" compared to NLTK? What are the benefits of this approach?\n",
        "\n",
        "**Question 2:** For which scenario is WordPiece better: (a) a traditional search engine, or (b) a BERT-based text classifier? Explain why.\n",
        "\n",
        "**Question 3:** What happens when NLTK encounters a completely unknown word not in its vocabulary? What happens with WordPiece?\n",
        "\n",
        "**YOUR ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise A6: Complete Tokenization Pipeline Design (25 points)\n",
        "\n",
        "Design and implement a complete tokenization pipeline for a specific domain.\n",
        "\n",
        "**Scenario:** You're building a medical information retrieval system that processes doctor's notes.\n",
        "\n",
        "**Requirements:**\n",
        "- Handle medical terms: \"Type 2 Diabetes Mellitus\", \"ICD-10: E11.9\"\n",
        "- Handle abbreviations: \"Dr. Smith\", \"pt. presents with...\"\n",
        "- Handle measurements: \"120/80 mmHg\", \"5mg dosage\"\n",
        "- Do NOT break important medical entities\n",
        "\n",
        "**Learning objectives:**\n",
        "- Integrate multiple tokenization strategies\n",
        "- Make architecture decisions for a real system\n",
        "- Handle domain-specific challenges\n",
        "\n",
        "**Points:** 10 auto-graded + 15 manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "A6_solution",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# Sample medical texts to process\n",
        "medical_texts = [\n",
        "    \"Patient presents with Type 2 Diabetes Mellitus (ICD-10: E11.9).\",\n",
        "    \"Dr. Johnson prescribed 5mg of medication. BP: 120/80 mmHg.\",\n",
        "    \"Pt. diagnosed with COPD and prescribed inhaler treatment.\",\n",
        "]\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Design your tokenization pipeline:\n",
        "# 1. Choose tokenizer(s)\n",
        "# 2. Decide on preprocessing steps (case, punctuation, stop words)\n",
        "# 3. Decide on morphological processing (if any)\n",
        "# 4. Handle medical abbreviations and codes\n",
        "#\n",
        "# Implement your pipeline in the function below:\n",
        "\n",
        "def medical_tokenization_pipeline(text):\n",
        "    \"\"\"\n",
        "    Tokenize medical text while preserving important medical entities.\n",
        "    \n",
        "    Args:\n",
        "        text (str): Medical text to tokenize\n",
        "    \n",
        "    Returns:\n",
        "        list: List of tokens\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    \n",
        "    ### BEGIN SOLUTION\n",
        "    # Solution approach:\n",
        "    # 1. Use regex to capture medical patterns first\n",
        "    # 2. Then tokenize remaining text\n",
        "    # 3. Preserve medical abbreviations and codes\n",
        "    \n",
        "    from nltk.tokenize import RegexpTokenizer\n",
        "    \n",
        "    # Pattern captures: ICD codes, measurements, abbreviations, regular words\n",
        "    pattern = r'ICD-\\d+:\\s*[A-Z]\\d+\\.?\\d*|BP:\\s*\\d+/\\d+\\s*mmHg|\\d+/\\d+\\s*mmHg|\\d+mg|Dr\\.|Pt\\.|[A-Z]{2,}|\\w+'\n",
        "    tokenizer = RegexpTokenizer(pattern)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    \n",
        "    # Keep case for medical terms but could normalize others if needed\n",
        "    ### END SOLUTION\n",
        "    \n",
        "    return tokens\n",
        "\n",
        "# Apply your pipeline\n",
        "results = []\n",
        "for text in medical_texts:\n",
        "    tokens = medical_tokenization_pipeline(text)\n",
        "    results.append(tokens)\n",
        "    print(f\"\\nText: {text}\")\n",
        "    print(f\"Tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": true,
          "grade_id": "A6_tests",
          "locked": true,
          "points": 25,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# AUTO-GRADED TESTS (10 points)\n",
        "# Do not modify this cell\n",
        "\n",
        "### BEGIN HIDDEN TESTS\n",
        "# Test 1: All texts processed (2 pts)\n",
        "assert len(results) == 3, \"Should process all 3 medical texts\"\n",
        "\n",
        "# Test 2: Medical terms preserved (3 pts)\n",
        "# \"Type 2 Diabetes Mellitus\" should be captured (possibly as separate tokens, but \"Type\", \"2\", \"Diabetes\" should be there)\n",
        "text1_joined = ' '.join(results[0])\n",
        "assert 'Diabetes' in text1_joined or 'diabetes' in text1_joined.lower(), \"Should capture 'Diabetes'\"\n",
        "\n",
        "# Test 3: ICD codes handled (2 pts)\n",
        "# ICD-10: E11.9 or parts of it should be captured\n",
        "assert any('ICD' in token or 'E11' in token for token in results[0]), \"ICD codes should be captured\"\n",
        "\n",
        "# Test 4: Measurements handled (2 pts)\n",
        "# BP: 120/80 mmHg or similar should be captured\n",
        "text2_joined = ' '.join(results[1])\n",
        "assert '120' in text2_joined or 'mmHg' in text2_joined, \"Blood pressure measurements should be captured\"\n",
        "\n",
        "# Test 5: Abbreviations handled (1 pt)\n",
        "# Dr. or Pt. should be kept\n",
        "assert any('Dr' in token for token in results[1]), \"Dr. abbreviation should be handled\"\n",
        "### END HIDDEN TESTS\n",
        "\n",
        "print(\"All auto-graded tests passed! ✓\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A6 Comprehensive Explanation (15 points - manually graded)\n",
        "\n",
        "Provide a complete justification of your tokenization pipeline design.\n",
        "\n",
        "**Question 1: Architecture Decision (5 pts)**  \n",
        "Which tokenizer(s) did you choose and why? (NLTK word_tokenize, RegexpTokenizer, WordPiece, or custom combination?)\n",
        "\n",
        "**Question 2: Preprocessing Choices (4 pts)**  \n",
        "- Case sensitivity: Did you normalize case? Why or why not?\n",
        "- Stop words: Did you remove them? Justify your decision for medical text.\n",
        "- Punctuation: How did you handle it, especially in medical codes (ICD-10: E11.9)?\n",
        "\n",
        "**Question 3: Medical Entity Handling (4 pts)**  \n",
        "How does your pipeline ensure that important medical entities are not broken?  \n",
        "Examples: \"Type 2 Diabetes Mellitus\", \"120/80 mmHg\", \"ICD-10: E11.9\"\n",
        "\n",
        "**Question 4: Trade-offs (2 pts)**  \n",
        "What are the limitations of your approach? What edge cases might still fail?\n",
        "\n",
        "**YOUR ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assessment Summary\n",
        "\n",
        "**Congratulations!** You've completed the Tokenization Mastery exercises.\n",
        "\n",
        "**Points Breakdown:**\n",
        "- Exercise A1: Basic Tokenization Comparison - 10 points\n",
        "- Exercise A2: Regex Pattern Design - 15 points\n",
        "- Exercise A3: Stop Words Impact Analysis - 10 points\n",
        "- Exercise A4: Morphological Processing Decisions - 15 points\n",
        "- Exercise A5: WordPiece vs Word-Level Comparison - 15 points\n",
        "- Exercise A6: Complete Tokenization Pipeline Design - 25 points\n",
        "\n",
        "**Total: 90 points**\n",
        "\n",
        "**Key Takeaways:**\n",
        "1. Tokenization is not trivial - different approaches have different trade-offs\n",
        "2. Domain and task determine the best tokenization strategy\n",
        "3. Stop word removal can break queries and lose critical information\n",
        "4. Modern NLP uses subword tokenization (WordPiece) for robustness\n",
        "5. Real-world systems require carefully designed, domain-specific pipelines\n",
        "\n",
        "**Before submitting:**\n",
        "- Ensure all cells run without errors\n",
        "- Check that you've removed all `raise NotImplementedError()` lines\n",
        "- Complete all explanation sections marked \"YOUR ANSWER\"\n",
        "- Run \"Restart & Run All\" to verify everything works"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}