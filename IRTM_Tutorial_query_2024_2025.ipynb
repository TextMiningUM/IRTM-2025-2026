{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SupadwS7wtIm"
      },
      "source": [
        "Start by copying this into your Google Drive!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY1A4vsbuqDq"
      },
      "source": [
        "# Information Retrieval & Text Mining\n",
        "![maastricht-university-logo.jpeg](data:image/jpeg;base64,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)\n",
        "## Notebook 3\n",
        "\n",
        "Version 2024-2025\n",
        "\n",
        "Adapted and updated from [here](https://github.com/castorini/anserini-notebooks-afirm2020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQoMY5uxF2GU"
      },
      "source": [
        "# Indexing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wYOsdnhH_XM"
      },
      "source": [
        "We start by reindexing the dataset as we did in the previous notebook so we can query it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjbq9md9GNN6"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/castorini/anserini.git\n",
        "!cd anserini && git checkout ad5ba1c76196436f8a0e28efdb69960d4873efe3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y openjdk-11-jdk-headless\n",
        "%env JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64"
      ],
      "metadata": {
        "id": "hKSW9sx0fEMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFmENb2zGLVr"
      },
      "outputs": [],
      "source": [
        "!apt-get install -q maven"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZWf6geaGGZ7"
      },
      "outputs": [],
      "source": [
        "!cd anserini && mvn clean package appassembler:assemble | grep \"BUILD\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWxu2O4SGFXZ"
      },
      "outputs": [],
      "source": [
        "!mkdir -p data\n",
        "!wget https://msmarco.z22.web.core.windows.net/msmarcoranking/collectionandqueries.tar.gz\n",
        "!tar -xvf collectionandqueries.tar.gz -C data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97opIbyvF-Wm"
      },
      "outputs": [],
      "source": [
        "!cd anserini && python ./src/main/python/msmarco/convert_collection_to_jsonl.py \\\n",
        " --collection_path ../data/collection.tsv --output_folder ../data/collection_jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AFQqCRtGGvgl"
      },
      "outputs": [],
      "source": [
        "!cd anserini && sh target/appassembler/bin/IndexCollection -collection JsonCollection -input ../data/collection_jsonl \\\n",
        " -index ../indexes/lucene-index.msmarco-passage.pos+docvectors+rawdocs -generator DefaultLuceneDocumentGenerator -threads 9 \\\n",
        " -storePositions -storeDocvectors -storeRaw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URfNSXe6L8sH"
      },
      "source": [
        "# Querying\n",
        "\n",
        "In this exercise, we are going to first interactively query the index and then produce a TREC run with [Pyserini](https://github.com/castorini/pyserini), the Python interface to Anserini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1y2srQHdOA3"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoP7jph2d78z"
      },
      "source": [
        "Install Python dependencies (again - remember that each notebook instantiates a virtual machine of its own):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFGfQRYFMO2A"
      },
      "outputs": [],
      "source": [
        "!pip install pyjnius==1.2.1\n",
        "!pip install pyserini==0.9.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9fakgDhPdI5"
      },
      "source": [
        "Instead of building anserini from scratch, we download the fatjar from the maven repository and store it locally:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1pGCkkiMVGG"
      },
      "outputs": [],
      "source": [
        "!wget -O anserini-0.9.2-fatjar.jar https://search.maven.org/remotecontent?filepath=io/anserini/anserini/0.9.2/anserini-0.9.2-fatjar.jar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Wcr7BpfVgSg"
      },
      "source": [
        "Let's point Pyserini to the Anserini jar that we have just installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuAwdnkpViBH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['ANSERINI_CLASSPATH'] = '.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsEU-U3ketRL"
      },
      "source": [
        "## Interactive Querying"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wokA-RljLy0V"
      },
      "outputs": [],
      "source": [
        "from pyserini.search import pysearch\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xMlOOqhe5xp"
      },
      "source": [
        "The hits data structure holds the docid, the retrieval score, as well as the document content.\n",
        "Let's look at the top 10 passages for the query `south african football teams`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpsnrIaoMYg_"
      },
      "outputs": [],
      "source": [
        "from IPython.core.display import display, HTML\n",
        "\n",
        "searcher = pysearch.SimpleSearcher('indexes/lucene-index.msmarco-passage.pos+docvectors+rawdocs')\n",
        "interactive_hits = searcher.search('south african football teams')\n",
        "\n",
        "for i in range(0, 10):\n",
        "    print('Rank: {} | Passage ID: {} | BM25 Score: {}'.format(i+1, interactive_hits[i].docid, interactive_hits[i].score))\n",
        "    display(HTML('<div style=\"font-family: Times New Roman; padding-bottom:10px\">' + interactive_hits[i].raw + '</div>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogrmRRfobshT"
      },
      "source": [
        "The above example uses default parameters.\n",
        "Let's try setting tuned parameters for this collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIP34ZtsCaB5"
      },
      "outputs": [],
      "source": [
        "searcher.set_bm25_similarity(0.82, 0.68)\n",
        "interactive_hits_tuned = searcher.search('south african football teams')\n",
        "\n",
        "for i in range(0, 10):\n",
        "    print('Rank: {} | Passage ID: {} | BM25 Score: {}'.format(i+1, interactive_hits_tuned[i].docid, interactive_hits_tuned[i].score))\n",
        "    display(HTML('<div style=\"font-family: Times New Roman; padding-bottom:10px\">' + interactive_hits[i].raw + '</div>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJpEXdsx3uBJ"
      },
      "source": [
        "###Exercise #1\n",
        "Compare the rankings with and without tuned parameters.\n",
        "Add a new cell to query the index with a different query of your choice, both with untuned and tuned parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g5YcAjkgA78"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdZeRTf3ClAR"
      },
      "source": [
        "Note how the ranking has changed.\n",
        "We can also enable RM3 query expansion to see if it helps with our collection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH3EVKaTMcgg"
      },
      "outputs": [],
      "source": [
        "searcher.set_rm3_reranker(10, 10, 0.5)\n",
        "interactive_hits_tuned_rm3 = searcher.search('south african football teams')\n",
        "\n",
        "for i in range(0, 10):\n",
        "    print('Rank: {} | Passage ID: {} | BM25 Score: {}'.format(i+1, interactive_hits_tuned_rm3[i].docid, interactive_hits_tuned_rm3[i].score))\n",
        "    display(HTML('<div style=\"font-family: Times New Roman; padding-bottom:10px\">' + interactive_hits_tuned_rm3[i].raw + '</div>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHX_l-5jKZ3P"
      },
      "source": [
        "## Batch Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k08tEtsmDCyv"
      },
      "source": [
        "Previously we interactively queried the index.\n",
        "However, in a typical experimental setting, you would evaluate over a larger number of queries to test different information needs.\n",
        "\n",
        "Let's begin by constructing the dev queries and corresponding query IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSlbbOMecd2d"
      },
      "outputs": [],
      "source": [
        "topics = {}\n",
        "with open('data/queries.dev.small.tsv') as file:\n",
        "    for line in file:\n",
        "       id, q = line.strip().split('\\t')\n",
        "       topics[int(id)] = q\n",
        "\n",
        "print('{} queries total'.format(len(topics)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpSWU35Bd9Bp"
      },
      "outputs": [],
      "source": [
        "queries = list(topics.values())\n",
        "qids = list([str(t) for t in topics.keys()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtYICC936IMU"
      },
      "source": [
        "### Exercise #2\n",
        "We have previously looked at these queries in the last activity.\n",
        "Again find the queries that contain `football`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW6cX15t5cGd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F37KoOfsfCSe"
      },
      "source": [
        "Now, let's run all the queries from the dev set.\n",
        "For the sake of speed, let's again only retrieve the top 5 documents for each query.\n",
        "Note that this step may still take a while."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C74qogSKfFtD"
      },
      "outputs": [],
      "source": [
        "searcher = pysearch.SimpleSearcher('indexes/lucene-index.msmarco-passage.pos+docvectors+rawdocs')\n",
        "bm25_hits = searcher.batch_search(queries, qids, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HTohrkIO5N2"
      },
      "source": [
        "Note that the above runs batch retrieval with untuned BM25.\n",
        "We can repeat with tuned parameters, just like we did for the interactive queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6clfMWkD3ZU"
      },
      "outputs": [],
      "source": [
        "searcher.set_bm25_similarity(0.82, 0.68)\n",
        "bm25_hits_tuned = searcher.batch_search(queries, qids, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNMa1DKsD1fc"
      },
      "source": [
        "Now let's repeat with RM3 query expansion (May take a while):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9o_IXRTk5mf"
      },
      "outputs": [],
      "source": [
        "searcher.set_rm3_reranker(10, 10, 0.5)\n",
        "bm25_hits_tuned_rm3 = searcher.batch_search(queries, qids, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZbakkgXiPSO"
      },
      "source": [
        "### Exercise #3\n",
        "Produce a run for untuned BM25 with RM3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZvRqPFAgCXw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J-CuGlz9hjc"
      },
      "source": [
        "### Exercise #4\n",
        "So far we have downloaded and retrieved the top passages for the dev queries.\n",
        "Now use the eval queries (data/queries.eval.small.tsv) and repeat the process for eval queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7npsIcEU91Ji"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRxS0rzG--tB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU1jjvYfKYgA"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "A crucial component of information retrieval research is evaluation and metrics.\n",
        "The most common tool used to achieve this goal is `trec_eval` developed by [NIST](https://www.nist.gov/).\n",
        "\n",
        "`trec_eval` defines a number of standard retrieval measures, the details of which can be seen [here](http://www.rafaelglater.com/en/post/learn-how-to-use-trec_eval-to-evaluate-your-information-retrieval-system).\n",
        "\n",
        "### TREC Format\n",
        "\n",
        "`trec_eval` requires the runs from various experiments to be expressed in a standard TREC format:\n",
        "\n",
        "`query_id iter docno rank similarity run_id` delimited by spaces\n",
        "\n",
        "- `query_id`: query ID\n",
        "- `iter`: constant, often either 0 or Q0 - required but ignored by `trec_eval`\n",
        "- `docno`: string values that uniquely identify a document in the collection\n",
        "- `rank`: integer, often zero indexed\n",
        "- `similarity`: float value that represents the similarity of the document to the query specified by `query_id`\n",
        "- `run_id`: string that identifies runs, used to keep track of different experiments - also ignored by `trec_eval`\n",
        "\n",
        "Evaluation also requires the ground truth in the form of relevance judgements in the qrels file.\n",
        "The qrels file follows the following format:\n",
        "\n",
        "`query_id iter docno label`\n",
        "\n",
        "- `label`: binary code (0 for not relevant and 1 for relevant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnhjuvsQhrVB"
      },
      "source": [
        "Convert the hits for both BM25 (tuned and untuned) and BM25+RM3 runs into the TREC format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_I_F6K9kW3L"
      },
      "outputs": [],
      "source": [
        "def convert_to_trec_run(experiment, run_dict):\n",
        "  with open('run.{}.txt'.format(experiment), 'w') as run_file:\n",
        "    for qid in run_dict:\n",
        "      for rank, doc in enumerate(run_dict[qid]):\n",
        "        run_file.write('{} Q0 {} {} {} {}\\n'.format(qid, doc.docid, rank, doc.score, experiment))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDPIwvy5iqMK"
      },
      "outputs": [],
      "source": [
        "convert_to_trec_run('msmarco_passage_dev_bm25', bm25_hits)\n",
        "convert_to_trec_run('msmarco_passage_dev_bm25_tuned', bm25_hits_tuned)\n",
        "convert_to_trec_run('msmarco_passage_dev_bm25_tuned_rm3', bm25_hits_tuned_rm3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtKO5XXWnWZi"
      },
      "source": [
        "Let's pull `trec_eval` again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQWfpv1gnYaf"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/castorini/anserini-tools\n",
        "!cd anserini-tools/eval && tar xvfz trec_eval.9.0.4.tar.gz && cd trec_eval.9.0.4 && make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_1ERgvpWIz2"
      },
      "outputs": [],
      "source": [
        "!mv anserini-tools/eval/trec_eval.9.0.4 ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8dRwjl3aGsn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmWLvAl7iqbz"
      },
      "source": [
        "Now that we have our runs in the TREC format, we can evaluate them with `trec_eval`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QWvGHiXVx0V"
      },
      "outputs": [],
      "source": [
        "!head -5 run.msmarco_passage_dev_bm25.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPNjxxLWO0X-"
      },
      "outputs": [],
      "source": [
        "!chmod -R +x trec_eval.9.0.4/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iQOW2wkgcyV"
      },
      "outputs": [],
      "source": [
        "!trec_eval.9.0.4/trec_eval -m ndcg_cut.20 -c -m recall.1000 -c data/qrels.dev.small.tsv run.msmarco_passage_dev_bm25.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXWy1WHZGq_Y"
      },
      "outputs": [],
      "source": [
        "!trec_eval.9.0.4/trec_eval -m ndcg_cut.20 -c -m recall.1000 -c data/qrels.dev.small.tsv run.msmarco_passage_dev_bm25_tuned.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQV_2o-OoDkv"
      },
      "outputs": [],
      "source": [
        "!trec_eval.9.0.4/trec_eval -m ndcg_cut.20 -c -m recall.1000 -c data/qrels.dev.small.tsv run.msmarco_passage_dev_bm25_tuned_rm3.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdX1XQfKG7QB"
      },
      "source": [
        "### Exercise #5\n",
        "What can you infer based on these result?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gI2lbORbqCH5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}