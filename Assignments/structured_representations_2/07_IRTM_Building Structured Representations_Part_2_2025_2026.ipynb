{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230611e8",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f301d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c63ecf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0001",
   "metadata": {},
   "source": [
    "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
    "\n",
    "# Information Retrieval and Text Mining Course\n",
    "## Tutorial 07 — Information Extraction: Building Structured Representations from Text (Part 2)\n",
    "\n",
    "**Author:** Jan Scholtes and Gijs Wijngaard\n",
    "\n",
    "**Edition 2025-2026**\n",
    "\n",
    "Department of Advanced Computer Sciences — Maastricht University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0002",
   "metadata": {},
   "source": [
    "Welcome to Tutorial 07 on **Information Extraction**. In this tutorial, you will learn and practice several methods for extracting structured information from text. The topics covered are:\n",
    "\n",
    "1. **Named Entity Recognition (NER) with NLTK** — traditional POS tagging, chunking, and entity extraction.\n",
    "2. **BERT: Tokenizers and Embeddings** — understanding context-sensitive representations using the BERT Transformer architecture.\n",
    "3. **Pre-trained BERT for NER** — using a HuggingFace pipeline with the `dslim/bert-base-NER` model.\n",
    "4. **Fine-tuning BERT for NER** — training BERT on the CoNLL-2003 dataset.\n",
    "5. **Entity Normalization and String Matching** — Levenshtein, Jaro-Winkler, Jaccard, and Soft TF-IDF.\n",
    "6. **From Text to Knowledge Graphs** — extracting triples and building knowledge graphs with NetworkX.\n",
    "7. **Applied Pipeline: Knowledge Graph from Your Corpus** — building a KG from the Sherlock Holmes corpus you indexed in Tutorial 03.\n",
    "8. **RAG Retrieval Foundation: FAISS Vector Store** — embedding your corpus and building a dense retrieval index for use in Tutorial 11.\n",
    "9. **Atomic Facts & QA Test Set** — decomposing text into verifiable claims and generating question-answer pairs for RAGAS evaluation.\n",
    "\n",
    "At the end you will find the **Exercises** section with graded assignments.\n",
    "\n",
    "> **Note:** This course is about Information Retrieval, Text Mining, and Conversational Search — not about programming skills. The code cells below are meant to show you *how* these methods work in practice using Python libraries. Focus on understanding the **concepts** and **results**.\n",
    "\n",
    "> **Cross-notebook arc:** In Tutorial 03 you built a search engine over your own text corpus. In Sections 7–9 of this tutorial, we use that same corpus to build a Knowledge Graph, a vector store, and a test set. In Tutorial 11, you will use all of these to build and evaluate a RAG chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0003",
   "metadata": {},
   "source": [
    "## Library Installation\n",
    "\n",
    "We install all required packages in a single cell. Run this cell once at the beginning of your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    \"svgling\", \"accelerate\", \"datasets\", \"seqeval\", \"evaluate\",\n",
    "    \"python-Levenshtein\", \"networkx\",\n",
    "    \"sentence-transformers\", \"faiss-cpu\",\n",
    "]\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0005",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "All imports are grouped here so the notebook is easy to set up and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "from pprint import pprint\n",
    "\n",
    "# Data & visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "nltk.download('maxent_ne_chunker', quiet=True)\n",
    "nltk.download('maxent_ne_chunker_tab', quiet=True)\n",
    "nltk.download('words', quiet=True)\n",
    "from nltk import word_tokenize, pos_tag, sent_tokenize, ne_chunk, RegexpParser\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Visualization for parse trees\n",
    "import svgling\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "\n",
    "# HuggingFace Transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, BertTokenizerFast, BertModel,\n",
    "    BertForSequenceClassification, BertForTokenClassification,\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    Trainer, TrainingArguments, DataCollatorForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import evaluate\n",
    "\n",
    "# String matching\n",
    "import Levenshtein\n",
    "\n",
    "# Knowledge graphs\n",
    "import networkx as nx\n",
    "\n",
    "# Dense retrieval (for §7-§9: RAG foundation & atomic facts)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"All libraries loaded successfully.\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0007",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Named Entity Recognition with NLTK\n",
    "\n",
    "Named Entity Recognition (NER) is the task of locating and classifying named entities in text into predefined categories such as **person names**, **organizations**, **locations**, **dates**, **medical codes**, etc.\n",
    "\n",
    "**Information Extraction** aims to identify several types of structured elements from text:\n",
    "- **Entities**: people, companies, locations, products, genes, etc.\n",
    "- **Attributes**: properties of entities (age, title, address, etc.)\n",
    "- **Facts**: relationships between entities (e.g., a person works for a company)\n",
    "- **Events**: activities involving entities (e.g., a person travels to a location)\n",
    "- **Concepts**: harder-to-define patterns with often ambiguous meanings\n",
    "\n",
    "**Key challenges** include:\n",
    "- Unknown words (dictionaries are never complete)\n",
    "- Same words occurring as different entity types (e.g., \"Apple\" as company vs. fruit)\n",
    "- Boundary problems (e.g., \"City University of New York\")\n",
    "- Multiple textual forms for one entity (e.g., \"Barack Obama\", \"B. Obama\", \"The President\")\n",
    "- Abbreviations (e.g., IBM, HP, WSJ)\n",
    "\n",
    "State-of-the-art NER systems for English produce near-human performance (F-measure ~93% vs. ~97% for human annotators)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0008",
   "metadata": {},
   "source": [
    "## 1.1 POS Tagging\n",
    "\n",
    "Part-of-Speech (POS) tagging assigns a grammatical tag (noun, verb, adjective, etc.) to each word in a sentence. POS tags form the basis for many NER pipelines, since named entities are typically nouns or noun phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagging with NLTK\n",
    "sentence = \"IRTM is an interesting course on Information Retrieval, Text Mining, and Conversational Search\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged = pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0010",
   "metadata": {},
   "source": [
    "## 1.2 Chunking\n",
    "\n",
    "Chunking groups words into phrases based on their POS tags using regular-expression patterns. For example, we can extract **noun phrases (NP)** that consist of adjacent nouns.\n",
    "\n",
    "The **IOB** tagging scheme labels each token as:\n",
    "- **B** (Begin) — start of a chunk\n",
    "- **I** (Inside) — inside a chunk\n",
    "- **O** (Outside) — not part of any chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a chunking grammar: noun phrases = zero or more adjectives followed by one or more nouns\n",
    "grammar = \"NP: {<JJ>*<NN.*>+}\"\n",
    "parser = RegexpParser(grammar)\n",
    "\n",
    "tree = parser.parse(tagged)\n",
    "tree  # svgling will render this as a tree diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tree to IOB tags\n",
    "iob_tags = tree2conlltags(tree)\n",
    "pprint(iob_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0013",
   "metadata": {},
   "source": [
    "## 1.3 Named Entity Recognition with `ne_chunk`\n",
    "\n",
    "NLTK's `ne_chunk` function performs NER by identifying named entities in POS-tagged text. It can operate in two modes:\n",
    "- `binary=True` — detects named entities without classifying them\n",
    "- `binary=False` — categorizes entities into types: PERSON, ORGANIZATION, GPE (Geo-Political Entity), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try NER on a longer, more interesting text\n",
    "article = \"\"\"Bill Gates is an American business magnate, software developer, and philanthropist. \\\n",
    "He is best known as the co-founder of Microsoft Corporation with his childhood friend Paul Allen. \\\n",
    "During his career at Microsoft, Gates held the positions of chairman, chief executive officer (CEO), \\\n",
    "president, and chief software architect, while also being the largest individual shareholder until \\\n",
    "May 2014. Jan Scholtes teaches a course on Information Retrieval and Text Mining at Maastricht University \\\n",
    "and was founder and CEO of ZyLAB Technologies amd worked as board member with Mind District. \\\n",
    "Today, he works as venture partner AI with Endeit Capital in Amsterdam, the Netherlands and with IPRally in Helsinki, Finland as board memner. \\\n",
    "In December 2023, ChatGPT became the fastest-growing consumer application, recording over 100 million users only \\\n",
    "two months after its launch in November 2022. ChatGPT is a natural language processing tool powered by \\\n",
    "GPT, which is an LLM developed by OpenAI.\"\"\"\n",
    "\n",
    "print(article[:200], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary NER (just detect named entities)\n",
    "sentences = sent_tokenize(article)\n",
    "tokenized_sents = [word_tokenize(s) for s in sentences]\n",
    "tagged_sents = [pos_tag(t) for t in tokenized_sents]\n",
    "chunked_sents = [ne_chunk(t, binary=True) for t in tagged_sents]\n",
    "\n",
    "for tree in chunked_sents:\n",
    "    for chunk in tree:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorized NER (classify entity types)\n",
    "chunked_sents_cat = [ne_chunk(t, binary=False) for t in tagged_sents]\n",
    "\n",
    "for tree in chunked_sents_cat:\n",
    "    for chunk in tree:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(f\"{chunk.label():15s} {' '.join(c[0] for c in chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entity distribution\n",
    "entity_counts = defaultdict(int)\n",
    "for tree in chunked_sents_cat:\n",
    "    for chunk in tree:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entity_counts[chunk.label()] += 1\n",
    "\n",
    "labels = list(entity_counts.keys())\n",
    "sizes = list(entity_counts.values())\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\n",
    "plt.title(\"Named Entity Distribution (NLTK)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0018",
   "metadata": {},
   "source": [
    "**Observation:** NLTK's NER is rule-based and uses the MaxEnt classifier. While it provides a quick baseline, it misses many entities and sometimes mis-classifies them. Traditional approaches like HMM, MEMM, and CRF also struggle with long-range dependencies, ambiguity, and out-of-vocabulary words.\n",
    "\n",
    "This motivates the use of deep learning approaches like **BERT**, which we explore next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0019",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. BERT: Tokenizers and Embeddings\n",
    "\n",
    "**BERT** (Bidirectional Encoder Representations from Transformers) is a Transformer-based model that revolutionized NLP. Key properties:\n",
    "\n",
    "- **Bidirectional context**: unlike older models (LSTMs) that read text left-to-right or stitch two directions together, BERT sees the entire sentence simultaneously using *self-attention*.\n",
    "- **Contextualized embeddings**: the word \"Apple\" gets different vector representations depending on whether the sentence is about fruit or the company.\n",
    "- **WordPiece tokenization**: handles out-of-vocabulary words by splitting them into known subword units (e.g., \"HuggingFace\" → [\"Hugging\", \"##Face\"]).\n",
    "- **Pre-trained** on Masked Language Modeling (MLM) and Next Sentence Prediction (NSP) tasks, then **fine-tuned** for downstream tasks like NER, sentiment analysis, and question answering.\n",
    "\n",
    "The [HuggingFace Transformers](https://huggingface.co/docs/transformers/) library provides easy access to thousands of pre-trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0020",
   "metadata": {},
   "source": [
    "## 2.1 Vocabulary and Tokenizers\n",
    "\n",
    "A tokenizer converts raw text into numerical token IDs that the model can process. BERT uses a **WordPiece** tokenizer with special tokens:\n",
    "- `[CLS]` — placed at the beginning of every input (used for classification tasks)\n",
    "- `[SEP]` — separates sentence pairs or marks the end\n",
    "- `[PAD]` — pads shorter sequences to match the longest in a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize a sentence\n",
    "text = \"Information Retrieval and Text Mining is an exciting field.\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens and convert to IDs\n",
    "marked_text = ['[CLS]'] + tokenized_text + ['[SEP]']\n",
    "input_ids = tokenizer.convert_tokens_to_ids(marked_text)\n",
    "print(\"Marked tokens:\", marked_text)\n",
    "print(\"Token IDs:    \", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the vocabulary\n",
    "print(\"Vocab size:\", len(tokenizer.vocab))\n",
    "print(\"Sample entries (5000-5019):\", list(tokenizer.vocab.keys())[5000:5020])\n",
    "\n",
    "# Round-trip: IDs back to tokens\n",
    "reconstructed = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(\"Reconstructed:\", reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0024",
   "metadata": {},
   "source": [
    "## 2.2 Getting Embeddings from BERT\n",
    "\n",
    "BERT-base has 12 Transformer encoder layers, each producing 768-dimensional hidden states. We can extract these embeddings to understand how BERT represents words in context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()  # set to evaluation mode\n",
    "\n",
    "# Prepare input: \"I love AI\"\n",
    "sentence = \"I love AI\"\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "\n",
    "# Pad to length 7\n",
    "while len(tokens) < 7:\n",
    "    tokens.append('[PAD]')\n",
    "\n",
    "# Create attention mask (1 = real token, 0 = padding)\n",
    "attention_mask = [1 if t != '[PAD]' else 0 for t in tokens]\n",
    "\n",
    "# Convert to tensor\n",
    "token_ids = torch.tensor([tokenizer.convert_tokens_to_ids(tokens)])\n",
    "attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"IDs:    {token_ids[0].tolist()}\")\n",
    "print(f\"Mask:   {attention_mask[0].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    outputs = model(token_ids, attention_mask=attention_mask)\n",
    "\n",
    "last_hidden_state = outputs.last_hidden_state\n",
    "pooler_output = outputs.pooler_output\n",
    "\n",
    "print(f\"Last hidden state shape: {last_hidden_state.shape}\")  # [batch=1, seq_len=7, hidden=768]\n",
    "print(f\"Pooler output shape:     {pooler_output.shape}\")      # [batch=1, hidden=768]\n",
    "print(f\"\\nThe embedding for token '{tokens[3]}' (index 3) has shape: {last_hidden_state[0, 3, :].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0027",
   "metadata": {},
   "source": [
    "## 2.3 Extracting Embeddings from All Encoder Layers\n",
    "\n",
    "BERT-base produces 13 sets of hidden states:\n",
    "- `hidden_states[0]` = initial word embedding layer (non-contextual)\n",
    "- `hidden_states[1]` through `hidden_states[12]` = output of each of the 12 encoder layers\n",
    "\n",
    "Research has shown that different layers capture different linguistic features. Concatenating or averaging specific layers can produce better results for different tasks (see Devlin et al., 2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model with output_hidden_states=True\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(token_ids, attention_mask=attention_mask)\n",
    "\n",
    "hidden_states = outputs.hidden_states\n",
    "print(f\"Number of hidden state layers: {len(hidden_states)}\")  # 13\n",
    "print(f\"Shape of each layer: {hidden_states[0].shape}\")         # [1, 7, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embeddings from different layers for the word \"AI\" (index 3)\n",
    "print(\"Comparing 'AI' embedding across layers:\")\n",
    "for layer_idx in [0, 1, 6, 11, 12]:\n",
    "    embedding = hidden_states[layer_idx][0, 3, :]\n",
    "    print(f\"  Layer {layer_idx:2d}: mean={embedding.mean():.4f}, std={embedding.std():.4f}, norm={embedding.norm():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0030",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Pre-trained BERT for Named Entity Recognition\n",
    "\n",
    "Instead of training a model from scratch, we can use **pre-trained NER models** from HuggingFace. The `dslim/bert-base-NER` model is a `bert-base-cased` model fine-tuned on the CoNLL-2003 dataset. It recognizes four entity types:\n",
    "\n",
    "| Label | Entity Type |\n",
    "|-------|------------|\n",
    "| **PER** | Person |\n",
    "| **ORG** | Organization |\n",
    "| **LOC** | Location |\n",
    "| **MISC** | Miscellaneous |\n",
    "\n",
    "**Why BERT for NER?**\n",
    "- **Contextualized embeddings**: \"Apple\" gets different representations depending on context (company vs. fruit).\n",
    "- **True bidirectionality**: every layer sees every other word simultaneously via self-attention.\n",
    "- **WordPiece handles OOV**: unknown words are broken into meaningful subword units.\n",
    "- **Long-range dependencies**: self-attention connects words regardless of distance.\n",
    "\n",
    "**BERT vs. LLMs (GPT-4, etc.) for production NER:**\n",
    "- BERT is ~100x faster than large LLMs for tagging tasks\n",
    "- BERT can run locally (privacy-preserving)\n",
    "- BERT provides structured 1:1 token-to-label mapping (ideal for Knowledge Graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained NER pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# Test on our article text\n",
    "results = ner_pipeline(article)\n",
    "\n",
    "# Display results in a DataFrame\n",
    "df_ner = pd.DataFrame(results)\n",
    "print(df_ner[['entity_group', 'word', 'score']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare BERT NER results with NLTK NER on the same text\n",
    "print(\"=\" * 60)\n",
    "print(\"BERT NER Results:\")\n",
    "print(\"=\" * 60)\n",
    "for r in results:\n",
    "    print(f\"  {r['entity_group']:8s} {r['word']:30s} (score: {r['score']:.3f})\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print(\"NLTK NER Results (for comparison):\")\n",
    "print(\"=\" * 60)\n",
    "for tree in chunked_sents_cat:\n",
    "    for chunk in tree:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entity_text = ' '.join(c[0] for c in chunk)\n",
    "            print(f\"  {chunk.label():15s} {entity_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0033",
   "metadata": {},
   "source": [
    "**Observation:** BERT-based NER is significantly more accurate than NLTK's rule-based approach. BERT correctly identifies entities even in complex contexts, handles multi-word entities better, and provides confidence scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0034",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Fine-tuning BERT for Named Entity Recognition\n",
    "\n",
    "While pre-trained NER models work well for general entities, you may need to fine-tune BERT on your own data for domain-specific NER (e.g., medical entities, legal terms, financial instruments).\n",
    "\n",
    "Here we fine-tune BERT on the **CoNLL-2003** dataset, which contains POS tags, syntactic chunk tags, and NER tags. The NER tags use the IOB2 format with these entity types: PER, ORG, LOC, MISC.\n",
    "\n",
    "The key difference from sequence classification:\n",
    "- `BertForSequenceClassification` classifies the **entire sentence** (one label per sentence)\n",
    "- `BertForTokenClassification` classifies **each token** (one label per token — needed for NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CoNLL-2003 dataset\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataset structure\n",
    "print(\"Example tokens:\", dataset[\"train\"][0][\"tokens\"])\n",
    "print(\"Example NER tags:\", dataset[\"train\"][0][\"ner_tags\"])\n",
    "\n",
    "# Map tag indices to readable names\n",
    "ner_features = dataset[\"train\"].features[\"ner_tags\"].feature\n",
    "tag_names = ner_features.names\n",
    "print(\"\\nNER tag names:\", tag_names)\n",
    "\n",
    "# Print aligned tokens and tags\n",
    "for word, tag_id in zip(dataset[\"train\"][0][\"tokens\"], dataset[\"train\"][0][\"ner_tags\"]):\n",
    "    print(f\"  {word:15s} → {tag_names[tag_id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0037",
   "metadata": {},
   "source": [
    "## 4.1 Handling Subword Tokenization\n",
    "\n",
    "BERT's WordPiece tokenizer may split words into subwords (e.g., \"lamb\" → \"la\", \"##mb\"). This creates a **mismatch** between the number of tokens and the number of labels. We need to align labels with subword tokens:\n",
    "\n",
    "- Special tokens (`[CLS]`, `[SEP]`) get label `-100` (ignored by the loss function)\n",
    "- The first subword of a word gets the original label\n",
    "- Subsequent subwords of the same word also get label `-100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer for fine-tuning\n",
    "tokenizer_ft = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Demonstrate the subword alignment problem\n",
    "example_tokens = dataset[\"train\"][0][\"tokens\"]\n",
    "inputs = tokenizer_ft(example_tokens, is_split_into_words=True)\n",
    "word_ids = inputs.word_ids()\n",
    "\n",
    "print(\"Original tokens:\", example_tokens[:8])\n",
    "print(\"Subword IDs:    \", inputs[\"input_ids\"][:12])\n",
    "print(\"Word IDs:       \", word_ids[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to align labels with subword tokens\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id is None:\n",
    "            new_labels.append(-100)  # special tokens\n",
    "        elif word_id != current_word:\n",
    "            current_word = word_id\n",
    "            new_labels.append(labels[word_id])  # first subword gets the label\n",
    "        else:\n",
    "            new_labels.append(-100)  # subsequent subwords\n",
    "    return new_labels\n",
    "\n",
    "# Test alignment\n",
    "example_labels = dataset[\"train\"][0][\"ner_tags\"]\n",
    "aligned = align_labels_with_tokens(example_labels, word_ids)\n",
    "print(\"Original labels:\", example_labels[:8])\n",
    "print(\"Aligned labels: \", aligned[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply alignment to entire dataset\n",
    "def align_labels(examples):\n",
    "    tokenized = tokenizer_ft(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    all_labels = []\n",
    "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized.word_ids(batch_index=i)\n",
    "        all_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "    tokenized[\"labels\"] = all_labels\n",
    "    return tokenized\n",
    "\n",
    "tokenized_datasets = dataset.map(align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Subset training data for faster training (~4000 examples → ~2-3 min on GPU)\n",
    "tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(4000))\n",
    "print(f\"Training examples: {len(tokenized_datasets['train'])}, Validation examples: {len(tokenized_datasets['validation'])}\")\n",
    "print(\"Tokenized dataset columns:\", tokenized_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0041",
   "metadata": {},
   "source": [
    "## 4.2 Training the NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator: dynamically pads batches\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer_ft)\n",
    "\n",
    "# Load evaluation metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics_ner(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    true_labels = [[tag_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [tag_names[p] for p, l in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mappings\n",
    "id2label = {i: label for i, label in enumerate(tag_names)}\n",
    "label2id = {label: i for i, label in enumerate(tag_names)}\n",
    "\n",
    "# Load model for token classification\n",
    "model_ner = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(tag_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "print(f\"Model loaded with {len(tag_names)} labels: {tag_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_ner\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer_ner = Trainer(\n",
    "    model=model_ner,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    processing_class=tokenizer_ft,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics_ner,\n",
    ")\n",
    "\n",
    "# Train (~2-3 minutes on RTX 4070 GPU with 4000 training samples)\n",
    "print(\"Starting NER fine-tuning...\")\n",
    "trainer_ner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "eval_results = trainer_ner.evaluate()\n",
    "print(\"\\nEvaluation Results:\")\n",
    "for k, v in eval_results.items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0046",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Entity Normalization and String Matching\n",
    "\n",
    "After extracting entities, we often need to **normalize** them — mapping different textual representations to the same entity. For example, \"MSFT\", \"Microsoft Corp\", and \"Microsoft Corporation\" all refer to the same organization.\n",
    "\n",
    "**Why normalization matters:**\n",
    "- Without normalization, knowledge graphs contain duplicate nodes for the same real-world entity\n",
    "- The same name can be written in many forms (abbreviations, misspellings, different orderings)\n",
    "- OCR errors in digitized documents create spelling variations\n",
    "\n",
    "**Common string matching techniques:**\n",
    "- **Levenshtein (edit) distance**: number of insertions, deletions, and substitutions needed\n",
    "- **Jaro-Winkler similarity**: preference for matching prefixes (good for person names)\n",
    "- **Jaccard coefficient**: overlap of character n-grams or token sets\n",
    "- **Soft TF-IDF**: TF-IDF cosine similarity with fuzzy token matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0047",
   "metadata": {},
   "source": [
    "## 5.1 Levenshtein (Edit) Distance\n",
    "\n",
    "The Levenshtein distance counts the minimum number of single-character edits (insertions, deletions, substitutions) needed to transform one string into another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Levenshtein distance examples\n",
    "pairs = [\n",
    "    (\"Sony Ericsson\", \"Sony Ericssen\"),      # one substitution\n",
    "    (\"Sony Ericsson\", \"Ericsson Sony\"),       # word-order swap\n",
    "    (\"Microsoft\", \"MSFT\"),                    # abbreviation\n",
    "    (\"Bill Gates\", \"William Gates\"),          # name variation\n",
    "    (\"Maastricht\", \"Maastright\"),             # common misspelling\n",
    "    (\"Jan Scholtes\", \"J. Scholtes\"),          # abbreviated first name\n",
    "]\n",
    "\n",
    "print(f\"{'String 1':25s} {'String 2':25s} {'Edit Dist':>10s} {'Ratio':>8s}\")\n",
    "print(\"-\" * 70)\n",
    "for s1, s2 in pairs:\n",
    "    dist = Levenshtein.distance(s1, s2)\n",
    "    ratio = Levenshtein.ratio(s1, s2)\n",
    "    print(f\"{s1:25s} {s2:25s} {dist:10d} {ratio:8.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0049",
   "metadata": {},
   "source": [
    "## 5.2 Jaro-Winkler Similarity\n",
    "\n",
    "Jaro-Winkler similarity gives a preference to strings that match from the beginning. This makes it particularly suitable for **person names** and **company names** where the beginning of the string is most informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaro-Winkler similarity\n",
    "print(f\"{'String 1':25s} {'String 2':25s} {'Jaro':>8s} {'Jaro-Wink':>10s}\")\n",
    "print(\"-\" * 70)\n",
    "for s1, s2 in pairs:\n",
    "    jaro = Levenshtein.jaro(s1, s2)\n",
    "    jaro_winkler = Levenshtein.jaro_winkler(s1, s2)\n",
    "    print(f\"{s1:25s} {s2:25s} {jaro:8.3f} {jaro_winkler:10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0051",
   "metadata": {},
   "source": [
    "## 5.3 Jaccard Coefficient\n",
    "\n",
    "The Jaccard coefficient measures the overlap between two sets. For string matching, we can use **character n-grams** (commonly bigrams) as the set elements:\n",
    "\n",
    "$$J(A, B) = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "This approach is also related to **Dice's coefficient** and **Q-gram distance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jaccard coefficient using character bigrams\n",
    "def char_bigrams(s):\n",
    "    \"\"\"Generate character bigrams from a string.\"\"\"\n",
    "    s = s.lower()\n",
    "    return set(s[i:i+2] for i in range(len(s) - 1))\n",
    "\n",
    "def jaccard_bigram(s1, s2):\n",
    "    \"\"\"Compute Jaccard coefficient using character bigrams.\"\"\"\n",
    "    bg1 = char_bigrams(s1)\n",
    "    bg2 = char_bigrams(s2)\n",
    "    intersection = bg1 & bg2\n",
    "    union = bg1 | bg2\n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "print(f\"{'String 1':25s} {'String 2':25s} {'Jaccard':>10s}\")\n",
    "print(\"-\" * 62)\n",
    "for s1, s2 in pairs:\n",
    "    j = jaccard_bigram(s1, s2)\n",
    "    print(f\"{s1:25s} {s2:25s} {j:10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0053",
   "metadata": {},
   "source": [
    "## 5.4 Comparing String Matching Methods\n",
    "\n",
    "Different methods work better for different types of text:\n",
    "- **Jaro-Winkler** is best for names (prefers matching beginnings)\n",
    "- **Levenshtein** is good for detecting typos and OCR errors\n",
    "- **Jaccard** is robust to word-order changes and partial overlaps\n",
    "- **Soft TF-IDF** combines TF-IDF weighting with fuzzy matching (best for record linkage)\n",
    "\n",
    "In practice, combining multiple methods in a **voting algorithm** often yields the best results (see Cohen, Ravikumar & Fienberg, 2003)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "print(f\"{'Pair':55s} {'Levenshtein':>12s} {'Jaro-Wink':>10s} {'Jaccard':>10s}\")\n",
    "print(\"-\" * 90)\n",
    "for s1, s2 in pairs:\n",
    "    lev = Levenshtein.ratio(s1, s2)\n",
    "    jw = Levenshtein.jaro_winkler(s1, s2)\n",
    "    jac = jaccard_bigram(s1, s2)\n",
    "    print(f\"{s1 + ' vs ' + s2:55s} {lev:12.3f} {jw:10.3f} {jac:10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0055",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. From Text to Knowledge Graphs\n",
    "\n",
    "A **Knowledge Graph** represents knowledge as concepts and the relationships between them (facts). It uses a graph-structured data model where:\n",
    "- **Nodes** represent entities (people, organizations, locations, etc.)\n",
    "- **Edges** represent relationships between entities\n",
    "\n",
    "**The 4-Step Pipeline** for building Knowledge Graphs from text:\n",
    "1. **Co-reference Resolution** — resolve pronouns and other references to their referents\n",
    "2. **Named Entity Recognition & Normalization** — identify and normalize entities\n",
    "3. **Relationship Extraction** — identify relationships between entities\n",
    "4. **Knowledge Graph Construction** — build the graph from extracted triples\n",
    "\n",
    "**Open Information Extraction (OIE)** extracts structured triples of the form *(subject, predicate, object)* from text. These triples directly form the edges of a knowledge graph.\n",
    "\n",
    "Knowledge Graphs are used by search engines (Google, Bing), question-answering systems (WolframAlpha, Siri), and social networks (LinkedIn, Facebook). They support better relation extraction, disambiguation, and conversational search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0056",
   "metadata": {},
   "source": [
    "## 6.1 Extracting Triples with Dependency Parsing\n",
    "\n",
    "We can extract simple subject-verb-object triples from text using dependency parsing. Here we demonstrate a simple rule-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple triple extraction using NLTK POS tags\n",
    "# This is a simplified approach - production systems use dependency parsers\n",
    "\n",
    "def extract_triples_simple(text):\n",
    "    \"\"\"Extract simple (subject, relation, object) triples from text using POS patterns.\"\"\"\n",
    "    triples = []\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tokens = word_tokenize(sent)\n",
    "        tagged = pos_tag(tokens)\n",
    "        \n",
    "        # Simple pattern: find NNP sequences separated by verbs\n",
    "        entities = []\n",
    "        current_entity = []\n",
    "        \n",
    "        for word, tag in tagged:\n",
    "            if tag.startswith('NNP'):\n",
    "                current_entity.append(word)\n",
    "            else:\n",
    "                if current_entity:\n",
    "                    entities.append(' '.join(current_entity))\n",
    "                    current_entity = []\n",
    "        if current_entity:\n",
    "            entities.append(' '.join(current_entity))\n",
    "        \n",
    "        # Create co-occurrence relations between entities in the same sentence\n",
    "        for i in range(len(entities)):\n",
    "            for j in range(i + 1, len(entities)):\n",
    "                triples.append((entities[i], \"co-occurs with\", entities[j]))\n",
    "    \n",
    "    return triples\n",
    "\n",
    "# Extract triples from our article\n",
    "triples = extract_triples_simple(article)\n",
    "print(f\"Extracted {len(triples)} triples:\\n\")\n",
    "for s, p, o in triples[:15]:\n",
    "    print(f\"  ({s}, {p}, {o})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0058",
   "metadata": {},
   "source": [
    "## 6.2 Building and Visualizing a Knowledge Graph\n",
    "\n",
    "We can use **NetworkX** to build a graph from extracted triples and visualize the entity co-occurrence network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a knowledge graph from the extracted triples\n",
    "G = nx.Graph()\n",
    "\n",
    "for subject, predicate, obj in triples:\n",
    "    G.add_edge(subject, obj, relation=predicate)\n",
    "\n",
    "print(f\"Knowledge Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "print(f\"\\nNodes: {list(G.nodes())[:15]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the knowledge graph\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Use spring layout for positioning\n",
    "pos = nx.spring_layout(G, k=2, seed=42)\n",
    "\n",
    "# Node sizes based on degree (connectivity)\n",
    "node_sizes = [300 + 200 * G.degree(node) for node in G.nodes()]\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='lightblue', edgecolors='navy', linewidths=1.5)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.4, edge_color='gray')\n",
    "nx.draw_networkx_labels(G, pos, font_size=8, font_weight='bold')\n",
    "\n",
    "plt.title(\"Entity Co-occurrence Knowledge Graph\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0061",
   "metadata": {},
   "source": [
    "## 6.3 Using BERT NER for Better Knowledge Graphs\n",
    "\n",
    "We can improve our knowledge graph by using BERT-based NER to identify entities more accurately, and then build the graph from those entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BERT NER results to build a typed knowledge graph\n",
    "G_bert = nx.Graph()\n",
    "\n",
    "# Group NER results by sentence\n",
    "sentences = sent_tokenize(article)\n",
    "for sent in sentences:\n",
    "    sent_ner = ner_pipeline(sent)\n",
    "    entities_in_sent = [(r['word'], r['entity_group']) for r in sent_ner]\n",
    "    \n",
    "    # Add typed nodes\n",
    "    for word, etype in entities_in_sent:\n",
    "        G_bert.add_node(word, entity_type=etype)\n",
    "    \n",
    "    # Connect entities within the same sentence\n",
    "    for i in range(len(entities_in_sent)):\n",
    "        for j in range(i + 1, len(entities_in_sent)):\n",
    "            G_bert.add_edge(entities_in_sent[i][0], entities_in_sent[j][0])\n",
    "\n",
    "print(f\"BERT Knowledge Graph: {G_bert.number_of_nodes()} nodes, {G_bert.number_of_edges()} edges\")\n",
    "\n",
    "# Color nodes by entity type\n",
    "color_map = {'PER': '#ff9999', 'ORG': '#99ccff', 'LOC': '#99ff99', 'MISC': '#ffcc99'}\n",
    "node_colors = [color_map.get(G_bert.nodes[n].get('entity_type', ''), '#dddddd') for n in G_bert.nodes()]\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "pos = nx.spring_layout(G_bert, k=2.5, seed=42)\n",
    "node_sizes = [400 + 200 * G_bert.degree(node) for node in G_bert.nodes()]\n",
    "\n",
    "nx.draw_networkx_nodes(G_bert, pos, node_size=node_sizes, node_color=node_colors, edgecolors='navy', linewidths=1.5)\n",
    "nx.draw_networkx_edges(G_bert, pos, alpha=0.4, edge_color='gray')\n",
    "nx.draw_networkx_labels(G_bert, pos, font_size=8, font_weight='bold')\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, edgecolor='navy', label=t) for t, c in color_map.items()]\n",
    "plt.legend(handles=legend_elements, loc='upper left', fontsize=10)\n",
    "\n",
    "plt.title(\"BERT-based Entity Knowledge Graph (colored by entity type)\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0063",
   "metadata": {},
   "source": [
    "## 6.4 Searching Knowledge Graphs\n",
    "\n",
    "Once built, knowledge graphs can be queried using:\n",
    "- **Cypher** (Neo4j) — for high-performance pattern matching and traversals in property graphs\n",
    "- **SPARQL** (RDF) — for semantic queries over triple stores following web standards\n",
    "\n",
    "For example, in Cypher:\n",
    "```\n",
    "MATCH (p:Person)-[:WORKS_FOR]->(o:Organization)\n",
    "RETURN p.name, o.name\n",
    "```\n",
    "\n",
    "And in SPARQL:\n",
    "```sparql\n",
    "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "SELECT ?name ?email\n",
    "WHERE {\n",
    "    ?person a foaf:Person .\n",
    "    ?person foaf:name ?name .\n",
    "    ?person foaf:mbox ?email .\n",
    "}\n",
    "```\n",
    "\n",
    "**Key difference:** Neo4j/Cypher excels at deep multi-hop traversals, while RDF/SPARQL is better for linking knowledge across diverse data sources.\n",
    "\n",
    "In the upcoming lectures on **RAG (Retrieval-Augmented Generation)**, we will see how knowledge graphs combined with vector search can significantly improve conversational search and reduce LLM hallucinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287edca",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Applied Pipeline: Knowledge Graph from Your Corpus\n",
    "\n",
    "In Tutorial 03, you built a search engine over a text corpus (the default was *The Adventures of Sherlock Holmes* from Project Gutenberg). The text was chunked into paragraphs and saved as `.txt` files in `custom_corpus/chunks/`.\n",
    "\n",
    "Now we apply the NER and KG techniques from Sections 1–6 to that **real corpus** to build a Knowledge Graph that captures entities and their relationships across the entire text.\n",
    "\n",
    "**Pipeline:**\n",
    "\n",
    "```\n",
    "Sherlock Holmes chunks (from Tutorial 03)\n",
    "        ↓\n",
    "   BERT NER → extract entities per chunk\n",
    "        ↓\n",
    "   Co-occurrence → extract (entity₁, co-occurs_with, entity₂) triples\n",
    "        ↓\n",
    "   NetworkX KG → visualize entity network\n",
    "        ↓\n",
    "   Save KG to disk → reuse in Tutorial 11\n",
    "```\n",
    "\n",
    "## 7.1 Load the Corpus Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c141900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the corpus chunks created in Tutorial 03 (Section 10)\n",
    "CHUNKS_DIR = os.path.join(\"custom_corpus\", \"chunks\")\n",
    "\n",
    "if not os.path.isdir(CHUNKS_DIR):\n",
    "    raise FileNotFoundError(\n",
    "        f\"'{CHUNKS_DIR}' not found. Please run Tutorial 03 Section 10 first \"\n",
    "        \"to download and chunk the Sherlock Holmes text.\"\n",
    "    )\n",
    "\n",
    "chunk_files = sorted(glob.glob(os.path.join(CHUNKS_DIR, \"*.txt\")))\n",
    "corpus_chunks = []\n",
    "for fpath in chunk_files:\n",
    "    with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read().strip()\n",
    "        if len(text) > 20:  # Skip tiny fragments\n",
    "            corpus_chunks.append(text)\n",
    "\n",
    "print(f\"Loaded {len(corpus_chunks)} chunks from {CHUNKS_DIR}\")\n",
    "print(f\"Average chunk length: {np.mean([len(c.split()) for c in corpus_chunks]):.0f} words\")\n",
    "print(f\"\\nSample chunk (first one):\\n{corpus_chunks[0][:300]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b24b3",
   "metadata": {},
   "source": [
    "## 7.2 Extract Entities with BERT NER\n",
    "\n",
    "We use the `dslim/bert-base-NER` pipeline (from Section 3) to extract named entities from every chunk. We then aggregate entity counts to see who and what appears most often in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BERT NER on all corpus chunks\n",
    "# We reuse the ner_pipeline from Section 3 (dslim/bert-base-NER)\n",
    "# If it hasn't been loaded yet, load it:\n",
    "try:\n",
    "    _ = ner_pipeline\n",
    "except NameError:\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "\n",
    "def extract_entities_from_chunks(chunks, ner_pipe, max_length=512, sample_size=None):\n",
    "    \"\"\"Extract named entities from corpus chunks using BERT NER.\n",
    "    \n",
    "    Returns:\n",
    "        entities_per_chunk: list of lists — entities found in each chunk\n",
    "        entity_counts: Counter — global entity frequency\n",
    "    \"\"\"\n",
    "    if sample_size and sample_size < len(chunks):\n",
    "        indices = random.sample(range(len(chunks)), sample_size)\n",
    "        selected = [(i, chunks[i]) for i in sorted(indices)]\n",
    "    else:\n",
    "        selected = list(enumerate(chunks))\n",
    "    \n",
    "    entities_per_chunk = {}\n",
    "    entity_counts = Counter()\n",
    "    \n",
    "    for idx, chunk in selected:\n",
    "        # Truncate to avoid BERT 512-token limit\n",
    "        truncated = \" \".join(chunk.split()[:max_length])\n",
    "        try:\n",
    "            ner_results = ner_pipe(truncated)\n",
    "        except Exception:\n",
    "            ner_results = []\n",
    "        \n",
    "        chunk_entities = []\n",
    "        for ent in ner_results:\n",
    "            name = ent[\"word\"].strip().replace(\" ##\", \"\")\n",
    "            label = ent[\"entity_group\"]\n",
    "            if len(name) > 1:  # Skip single-char entities\n",
    "                chunk_entities.append((name, label))\n",
    "                entity_counts[name] += 1\n",
    "        \n",
    "        entities_per_chunk[idx] = chunk_entities\n",
    "    \n",
    "    return entities_per_chunk, entity_counts\n",
    "\n",
    "# Process a sample of chunks (full corpus may take a few minutes)\n",
    "SAMPLE_SIZE = 300  # Increase for more complete KG; set to None for all chunks\n",
    "entities_per_chunk, entity_counts = extract_entities_from_chunks(\n",
    "    corpus_chunks, ner_pipeline, sample_size=SAMPLE_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Processed {len(entities_per_chunk)} chunks\")\n",
    "print(f\"Unique entities found: {len(entity_counts)}\")\n",
    "print(f\"\\nTop-20 most frequent entities:\")\n",
    "for name, count in entity_counts.most_common(20):\n",
    "    print(f\"  {name:30s} — {count:3d} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7cbf9c",
   "metadata": {},
   "source": [
    "## 7.3 Build Co-occurrence Knowledge Graph\n",
    "\n",
    "We build a KG based on **entity co-occurrence**: if two entities appear in the same chunk, they are likely related. We weight edges by how often they co-occur.\n",
    "\n",
    "This is a simple but effective approach that captures the social and geographic network within a narrative text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4197f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a co-occurrence KG from entity-per-chunk data\n",
    "from itertools import combinations\n",
    "\n",
    "def build_cooccurrence_kg(entities_per_chunk, min_entity_freq=2, min_edge_weight=2):\n",
    "    \"\"\"Build a KG where entities that co-occur in a chunk share an edge.\n",
    "    \n",
    "    Args:\n",
    "        entities_per_chunk: dict mapping chunk_idx → [(name, label), ...]\n",
    "        min_entity_freq: minimum number of occurrences to include an entity\n",
    "        min_edge_weight: minimum co-occurrence count for an edge\n",
    "    \n",
    "    Returns:\n",
    "        G: NetworkX graph with entities as nodes and co-occurrences as edges\n",
    "    \"\"\"\n",
    "    # Count global entity frequencies\n",
    "    global_counts = Counter()\n",
    "    for ents in entities_per_chunk.values():\n",
    "        for name, _ in ents:\n",
    "            global_counts[name] += 1\n",
    "    \n",
    "    # Filter to frequent entities\n",
    "    frequent = {name for name, count in global_counts.items() if count >= min_entity_freq}\n",
    "    \n",
    "    # Count co-occurrences\n",
    "    edge_counts = Counter()\n",
    "    for ents in entities_per_chunk.values():\n",
    "        # Unique entity names in this chunk\n",
    "        names = list(set(name for name, _ in ents if name in frequent))\n",
    "        for a, b in combinations(sorted(names), 2):\n",
    "            edge_counts[(a, b)] += 1\n",
    "    \n",
    "    # Build graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with entity type info\n",
    "    entity_labels = {}\n",
    "    for ents in entities_per_chunk.values():\n",
    "        for name, label in ents:\n",
    "            if name in frequent:\n",
    "                entity_labels[name] = label\n",
    "    \n",
    "    for name, label in entity_labels.items():\n",
    "        G.add_node(name, entity_type=label, frequency=global_counts[name])\n",
    "    \n",
    "    # Add edges\n",
    "    for (a, b), weight in edge_counts.items():\n",
    "        if weight >= min_edge_weight:\n",
    "            G.add_edge(a, b, weight=weight)\n",
    "    \n",
    "    return G\n",
    "\n",
    "G_corpus = build_cooccurrence_kg(entities_per_chunk, min_entity_freq=3, min_edge_weight=2)\n",
    "\n",
    "print(f\"Knowledge Graph built:\")\n",
    "print(f\"  Nodes (entities): {G_corpus.number_of_nodes()}\")\n",
    "print(f\"  Edges (co-occurrences): {G_corpus.number_of_edges()}\")\n",
    "\n",
    "# Show top connected entities\n",
    "degree_sorted = sorted(G_corpus.degree(), key=lambda x: x[1], reverse=True)[:15]\n",
    "print(f\"\\nMost connected entities:\")\n",
    "for name, degree in degree_sorted:\n",
    "    etype = G_corpus.nodes[name].get(\"entity_type\", \"?\")\n",
    "    freq = G_corpus.nodes[name].get(\"frequency\", 0)\n",
    "    print(f\"  {name:25s} type={etype:5s}  connections={degree:3d}  mentions={freq:3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3257e83",
   "metadata": {},
   "source": [
    "## 7.4 Visualize the Corpus Knowledge Graph\n",
    "\n",
    "Let's visualize the character and location network from the Sherlock Holmes stories. Node size reflects mention frequency, edge thickness reflects co-occurrence strength, and node colour indicates entity type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897bed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the corpus KG\n",
    "color_map = {\"PER\": \"#1f77b4\", \"LOC\": \"#2ca02c\", \"ORG\": \"#ff7f0e\", \"MISC\": \"#d62728\"}\n",
    "\n",
    "# Take the largest connected component for a cleaner visualization\n",
    "if G_corpus.number_of_nodes() > 0:\n",
    "    components = sorted(nx.connected_components(G_corpus), key=len, reverse=True)\n",
    "    G_vis = G_corpus.subgraph(components[0]).copy() if components else G_corpus\n",
    "else:\n",
    "    G_vis = G_corpus\n",
    "\n",
    "# Limit to top-40 nodes by degree for readability\n",
    "if G_vis.number_of_nodes() > 40:\n",
    "    top_nodes = [n for n, _ in sorted(G_vis.degree(), key=lambda x: x[1], reverse=True)[:40]]\n",
    "    G_vis = G_vis.subgraph(top_nodes).copy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "# Node properties\n",
    "node_colors = [color_map.get(G_vis.nodes[n].get(\"entity_type\", \"MISC\"), \"#999999\") for n in G_vis.nodes()]\n",
    "node_sizes = [G_vis.nodes[n].get(\"frequency\", 1) * 40 + 100 for n in G_vis.nodes()]\n",
    "\n",
    "# Edge properties\n",
    "edge_weights = [G_vis[u][v].get(\"weight\", 1) for u, v in G_vis.edges()]\n",
    "max_w = max(edge_weights) if edge_weights else 1\n",
    "edge_widths = [1 + 3 * (w / max_w) for w in edge_weights]\n",
    "\n",
    "pos = nx.spring_layout(G_vis, k=2.5, iterations=50, seed=SEED)\n",
    "\n",
    "nx.draw_networkx_edges(G_vis, pos, ax=ax, width=edge_widths, alpha=0.3, edge_color=\"#888888\")\n",
    "nx.draw_networkx_nodes(G_vis, pos, ax=ax, node_color=node_colors, node_size=node_sizes, alpha=0.8)\n",
    "nx.draw_networkx_labels(G_vis, pos, ax=ax, font_size=8, font_weight=\"bold\")\n",
    "\n",
    "# Legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_handles = [Line2D([0], [0], marker='o', color='w', markerfacecolor=c, markersize=10, label=t)\n",
    "                  for t, c in color_map.items()]\n",
    "ax.legend(handles=legend_handles, loc=\"upper left\", fontsize=10, title=\"Entity Type\")\n",
    "\n",
    "ax.set_title(\"Knowledge Graph: Sherlock Holmes Entity Co-occurrence Network\", fontsize=14)\n",
    "ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualized {G_vis.number_of_nodes()} nodes and {G_vis.number_of_edges()} edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27893e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the KG to disk so Tutorial 11 can load it\n",
    "KG_DIR = os.path.join(\"custom_corpus\", \"kg\")\n",
    "os.makedirs(KG_DIR, exist_ok=True)\n",
    "\n",
    "# Save as edge list with attributes\n",
    "kg_data = {\n",
    "    \"nodes\": [\n",
    "        {\"name\": n, \"entity_type\": G_corpus.nodes[n].get(\"entity_type\", \"UNK\"),\n",
    "         \"frequency\": G_corpus.nodes[n].get(\"frequency\", 0)}\n",
    "        for n in G_corpus.nodes()\n",
    "    ],\n",
    "    \"edges\": [\n",
    "        {\"source\": u, \"target\": v, \"weight\": G_corpus[u][v].get(\"weight\", 1)}\n",
    "        for u, v in G_corpus.edges()\n",
    "    ],\n",
    "}\n",
    "\n",
    "kg_path = os.path.join(KG_DIR, \"sherlock_kg.json\")\n",
    "with open(kg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(kg_data, f, indent=2)\n",
    "\n",
    "print(f\"Knowledge Graph saved to {kg_path}\")\n",
    "print(f\"  {len(kg_data['nodes'])} nodes, {len(kg_data['edges'])} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6dc5a8",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. RAG Retrieval Foundation: FAISS Vector Store\n",
    "\n",
    "To build a RAG chatbot in Tutorial 11, we need a **dense retrieval index** over our corpus. Here we:\n",
    "\n",
    "1. **Embed** every chunk using a sentence-transformer bi-encoder (`all-MiniLM-L6-v2`)\n",
    "2. **Index** all embeddings in a FAISS vector store for fast nearest-neighbour search\n",
    "3. **Test** the index with sample queries\n",
    "4. **Save** the index and embeddings to disk\n",
    "\n",
    "This is the same approach used in production RAG systems (e.g., LangChain, LlamaIndex).\n",
    "\n",
    "$$\\text{similarity}(q, d) = \\frac{\\vec{q} \\cdot \\vec{d}}{\\|\\vec{q}\\| \\, \\|\\vec{d}\\|}$$\n",
    "\n",
    "## 8.1 Embed the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd0b8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the bi-encoder model\n",
    "bi_encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "print(f\"Model: all-MiniLM-L6-v2  (embedding dim: {bi_encoder.get_sentence_embedding_dimension()})\")\n",
    "\n",
    "# Embed all corpus chunks\n",
    "print(f\"Embedding {len(corpus_chunks)} chunks...\")\n",
    "chunk_embeddings = bi_encoder.encode(\n",
    "    corpus_chunks,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    batch_size=64,\n",
    ")\n",
    "print(f\"Embeddings shape: {chunk_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e0a024",
   "metadata": {},
   "source": [
    "## 8.2 Build FAISS Index\n",
    "\n",
    "FAISS (Facebook AI Similarity Search) provides highly optimized vector search. We use `IndexFlatIP` (inner product on normalized vectors = cosine similarity) which is exact but fast enough for our corpus size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ccadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the FAISS index\n",
    "dim = chunk_embeddings.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(dim)\n",
    "\n",
    "# Normalize embeddings for cosine similarity via inner product\n",
    "chunk_embeddings_norm = chunk_embeddings.copy()\n",
    "faiss.normalize_L2(chunk_embeddings_norm)\n",
    "faiss_index.add(chunk_embeddings_norm)\n",
    "\n",
    "print(f\"FAISS index built: {faiss_index.ntotal} vectors, dimension={dim}\")\n",
    "\n",
    "# Test with sample queries\n",
    "test_queries = [\n",
    "    \"Who is Sherlock Holmes?\",\n",
    "    \"What happened at the crime scene?\",\n",
    "    \"Where does Watson live?\",\n",
    "    \"Tell me about Irene Adler\",\n",
    "    \"mysterious letter delivered at night\",\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Dense retrieval test (top-3 per query):\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    q_emb = bi_encoder.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, indices = faiss_index.search(q_emb, 3)\n",
    "    \n",
    "    print(f\"Query: \\\"{query}\\\"\")\n",
    "    for rank, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "        preview = corpus_chunks[idx][:100].replace(\"\\n\", \" \")\n",
    "        print(f\"  [{rank+1}] score={score:.4f}  \\\"{preview}...\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483af959",
   "metadata": {},
   "source": [
    "## 8.3 Save Index & Embeddings to Disk\n",
    "\n",
    "We save the FAISS index, the raw embeddings, and the chunk texts so Tutorial 11 can load them directly without re-computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index, embeddings, and chunk texts for reuse in Tutorial 11\n",
    "VECTOR_DIR = os.path.join(\"custom_corpus\", \"vector_store\")\n",
    "os.makedirs(VECTOR_DIR, exist_ok=True)\n",
    "\n",
    "# Save FAISS index\n",
    "faiss.write_index(faiss_index, os.path.join(VECTOR_DIR, \"faiss_index.bin\"))\n",
    "\n",
    "# Save raw embeddings (unnormalized, so they can be used flexibly)\n",
    "np.save(os.path.join(VECTOR_DIR, \"chunk_embeddings.npy\"), chunk_embeddings)\n",
    "\n",
    "# Save chunk texts as JSON for easy loading\n",
    "with open(os.path.join(VECTOR_DIR, \"chunks.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(corpus_chunks, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved to {VECTOR_DIR}:\")\n",
    "print(f\"  faiss_index.bin         — FAISS index ({faiss_index.ntotal} vectors)\")\n",
    "print(f\"  chunk_embeddings.npy    — embeddings array {chunk_embeddings.shape}\")\n",
    "print(f\"  chunks.json             — {len(corpus_chunks)} chunk texts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a701ee3e",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Atomic Facts & QA Test Set for RAGAS Evaluation\n",
    "\n",
    "To evaluate a RAG system, we need a **test set** of questions, expected answers, and **atomic facts** — individual verifiable claims decomposed from text passages. The RAGAS framework (Tutorial 11) uses these to measure **faithfulness** and **hallucination**.\n",
    "\n",
    "## What are Atomic Facts?\n",
    "\n",
    "An **atomic fact** (or atomic claim) is the smallest independently verifiable statement extracted from a passage:\n",
    "\n",
    "| Original Passage | Atomic Facts |\n",
    "|---|---|\n",
    "| \"Sherlock Holmes lives at 221B Baker Street in London and works as a consulting detective.\" | 1. Sherlock Holmes lives at 221B Baker Street. |\n",
    "| | 2. 221B Baker Street is in London. |\n",
    "| | 3. Sherlock Holmes works as a consulting detective. |\n",
    "\n",
    "These decomposed claims become the ground truth for measuring whether a RAG-generated answer is faithful to the source material.\n",
    "\n",
    "## 9.1 Extract Atomic Facts from Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90094dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract atomic facts from corpus chunks using sentence splitting + NER-based filtering\n",
    "# We keep sentences that contain at least one named entity — these are verifiable claims.\n",
    "\n",
    "def extract_atomic_facts(chunks, ner_pipe, max_facts_per_chunk=5, max_chunks=200):\n",
    "    \"\"\"Extract atomic facts (verifiable claims) from corpus chunks.\n",
    "    \n",
    "    Strategy: Split chunks into sentences, keep those with named entities.\n",
    "    This gives us factual/narrative statements rather than background description.\n",
    "    \"\"\"\n",
    "    all_facts = []\n",
    "    \n",
    "    sample = chunks[:max_chunks]  # Use first N chunks for determinism\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(sample):\n",
    "        sentences = sent_tokenize(chunk)\n",
    "        \n",
    "        for sent in sentences:\n",
    "            sent = sent.strip()\n",
    "            if len(sent.split()) < 5 or len(sent.split()) > 60:\n",
    "                continue  # Skip very short or very long sentences\n",
    "            \n",
    "            # Check if sentence contains named entities\n",
    "            try:\n",
    "                ents = ner_pipe(sent[:512])\n",
    "            except Exception:\n",
    "                ents = []\n",
    "            \n",
    "            entity_names = [e[\"word\"].strip() for e in ents if len(e[\"word\"].strip()) > 1]\n",
    "            \n",
    "            if entity_names:\n",
    "                all_facts.append({\n",
    "                    \"fact\": sent,\n",
    "                    \"chunk_idx\": chunk_idx,\n",
    "                    \"entities\": entity_names,\n",
    "                })\n",
    "            \n",
    "            if len(all_facts) >= max_chunks * max_facts_per_chunk:\n",
    "                break\n",
    "    \n",
    "    return all_facts\n",
    "\n",
    "print(\"Extracting atomic facts from corpus (this may take a minute)...\")\n",
    "atomic_facts = extract_atomic_facts(corpus_chunks, ner_pipeline, max_facts_per_chunk=5, max_chunks=150)\n",
    "\n",
    "print(f\"\\nExtracted {len(atomic_facts)} atomic facts from the corpus\")\n",
    "print(f\"\\nSample atomic facts:\")\n",
    "for fact in atomic_facts[:10]:\n",
    "    ents = \", \".join(fact[\"entities\"][:3])\n",
    "    print(f\"  [{fact['chunk_idx']:3d}] ({ents})\")\n",
    "    print(f\"        \\\"{fact['fact'][:120]}...\\\"\" if len(fact['fact']) > 120 else f\"        \\\"{fact['fact']}\\\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9911fbe6",
   "metadata": {},
   "source": [
    "## 9.2 Generate QA Test Set\n",
    "\n",
    "We generate question-answer pairs from our atomic facts using **template-based question generation**. Each fact becomes an expected answer, and we create a question that should retrieve the corresponding chunk.\n",
    "\n",
    "These QA pairs will be used in Tutorial 11 to evaluate the RAG chatbot with RAGAS metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16745115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate QA pairs from atomic facts using template-based question generation\n",
    "\n",
    "QUESTION_TEMPLATES = [\n",
    "    \"What do we know about {entity}?\",\n",
    "    \"What happened involving {entity}?\",\n",
    "    \"Tell me about {entity} in the story.\",\n",
    "    \"What is the role of {entity}?\",\n",
    "    \"What did {entity} do?\",\n",
    "]\n",
    "\n",
    "def generate_qa_pairs(atomic_facts, max_pairs=50):\n",
    "    \"\"\"Generate question-answer pairs from atomic facts.\n",
    "    \n",
    "    For each fact, create a question about one of its entities.\n",
    "    The expected answer is the original fact sentence.\n",
    "    The ground-truth chunk is the source chunk.\n",
    "    \"\"\"\n",
    "    qa_pairs = []\n",
    "    used_entities = set()\n",
    "    \n",
    "    for fact_data in atomic_facts:\n",
    "        if len(qa_pairs) >= max_pairs:\n",
    "            break\n",
    "        \n",
    "        fact = fact_data[\"fact\"]\n",
    "        chunk_idx = fact_data[\"chunk_idx\"]\n",
    "        entities = fact_data[\"entities\"]\n",
    "        \n",
    "        # Pick an entity we haven't used much\n",
    "        for entity in entities:\n",
    "            if entity not in used_entities or len(used_entities) > 20:\n",
    "                template = random.choice(QUESTION_TEMPLATES)\n",
    "                question = template.format(entity=entity)\n",
    "                \n",
    "                qa_pairs.append({\n",
    "                    \"question\": question,\n",
    "                    \"ground_truth_answer\": fact,\n",
    "                    \"ground_truth_chunk_idx\": chunk_idx,\n",
    "                    \"ground_truth_context\": corpus_chunks[chunk_idx],\n",
    "                    \"entity\": entity,\n",
    "                })\n",
    "                used_entities.add(entity)\n",
    "                break\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "qa_test_set = generate_qa_pairs(atomic_facts, max_pairs=50)\n",
    "\n",
    "print(f\"Generated {len(qa_test_set)} QA test pairs\\n\")\n",
    "print(\"Sample QA pairs:\")\n",
    "for qa in qa_test_set[:8]:\n",
    "    print(f\"  Q: {qa['question']}\")\n",
    "    print(f\"  A: \\\"{qa['ground_truth_answer'][:100]}...\\\"\" if len(qa['ground_truth_answer']) > 100 \n",
    "          else f\"  A: \\\"{qa['ground_truth_answer']}\\\"\")\n",
    "    print(f\"  Source chunk: {qa['ground_truth_chunk_idx']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be15231",
   "metadata": {},
   "source": [
    "## 9.3 Save Atomic Facts & QA Set to Disk\n",
    "\n",
    "We save everything to `custom_corpus/evaluation/` so Tutorial 11 can load these artifacts for RAGAS evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92909a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save atomic facts and QA test set for Tutorial 11\n",
    "EVAL_DIR = os.path.join(\"custom_corpus\", \"evaluation\")\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)\n",
    "\n",
    "# Save atomic facts\n",
    "atomic_facts_path = os.path.join(EVAL_DIR, \"atomic_facts.json\")\n",
    "with open(atomic_facts_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(atomic_facts, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save QA test set\n",
    "qa_path = os.path.join(EVAL_DIR, \"qa_test_set.json\")\n",
    "with open(qa_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_test_set, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved to {EVAL_DIR}:\")\n",
    "print(f\"  atomic_facts.json  — {len(atomic_facts)} verifiable claims\")\n",
    "print(f\"  qa_test_set.json   — {len(qa_test_set)} question-answer pairs\")\n",
    "print(f\"\\nThese files will be loaded in Tutorial 11 for RAGAS evaluation.\")\n",
    "\n",
    "# Summary of all saved artifacts across tutorials\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Cross-tutorial data pipeline summary:\")\n",
    "print(f\"  Tutorial 03 → custom_corpus/chunks/           ({len(corpus_chunks)} text chunks)\")\n",
    "print(f\"  Tutorial 07 → custom_corpus/kg/               (Knowledge Graph: {G_corpus.number_of_nodes()} nodes, {G_corpus.number_of_edges()} edges)\")\n",
    "print(f\"  Tutorial 07 → custom_corpus/vector_store/     (FAISS index: {faiss_index.ntotal} vectors)\")\n",
    "print(f\"  Tutorial 07 → custom_corpus/evaluation/       ({len(atomic_facts)} facts, {len(qa_test_set)} QA pairs)\")\n",
    "print(f\"  Tutorial 11 → will load all of the above to build and evaluate a RAG chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0064",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercises\n",
    "\n",
    "The following exercises are graded. Please provide your answers in the designated cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0065",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f4adb0696772fc2a7c5a8d9ba37ef06",
     "grade": true,
     "grade_id": "exercise_1_question",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 1 — NER: Traditional vs. BERT (5 points)\n",
    "\n",
    "Compare traditional Named Entity Recognition approaches (HMM, CRF) with BERT-based approaches. In your answer, address the following:\n",
    "\n",
    "1. What are the main limitations of traditional approaches (HMM, CRF) for NER?\n",
    "2. How does BERT's architecture (bidirectional self-attention, WordPiece tokenization) address these limitations?\n",
    "3. Why might traditional approaches still be preferred in certain production scenarios?\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0066",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "03d6e747df7b9d361e55b75173f3f8a0",
     "grade": true,
     "grade_id": "exercise_1_answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0067",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "59485b18f73a75ff8a57a7aa4995baf8",
     "grade": true,
     "grade_id": "exercise_2_question",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 2 — Knowledge Graphs from Text (5 points)\n",
    "\n",
    "Describe the 4-step pipeline for building Knowledge Graphs from unstructured text. In your answer, address:\n",
    "\n",
    "1. Explain each of the four steps: Co-reference Resolution, NER & Normalization, Relationship Extraction, and KG Construction.\n",
    "2. What role does entity normalization play? Give an example of how failing to normalize entities can lead to a fragmented knowledge graph.\n",
    "3. How can pre-trained BERT models improve each step of this pipeline?\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0068",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f88ffa849e16aea131982d07d577f3ec",
     "grade": true,
     "grade_id": "exercise_2_answer",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-0069",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba8eddacaa0e39545a5595e4bc27e452",
     "grade": true,
     "grade_id": "exercise_3_question",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 3 — Fine-tune BERT for POS Tagging (10 points)\n",
    "\n",
    "Adapt the NER fine-tuning code from Section 4 to fine-tune `bert-base-uncased` on **POS tagging** instead of NER. The CoNLL-2003 dataset contains POS tags in addition to NER tags.\n",
    "\n",
    "Your task:\n",
    "1. Load the CoNLL-2003 dataset and use the `pos_tags` column instead of `ner_tags`\n",
    "2. Create the label alignment function for POS tags\n",
    "3. Fine-tune the model for 2 epochs\n",
    "4. Evaluate and report precision, recall, F1-score, and accuracy\n",
    "\n",
    "Write your code in the cell below.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0070",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c0984154000bfba9e23800959e9b8eb",
     "grade": false,
     "grade_id": "exercise_3_code",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-0071",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed6fd5a12501298be3414adbf1436ece",
     "grade": true,
     "grade_id": "exercise_3_test",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder test cell — do not modify\n",
    "# This cell checks that your model has been trained and produces predictions\n",
    "assert 'model_pos' in dir() or 'trainer_pos' in dir(), \"You need to create a model or trainer for POS tagging\"\n",
    "print(\"Basic check passed. Your solution will be manually reviewed for correctness.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcad108",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4238ba17ca3fba5f1a50d9ecc62cb1a",
     "grade": true,
     "grade_id": "solution_4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 4 — Build a KG & RAG Foundation from Your Own Corpus (15 points)\n",
    "\n",
    "In Tutorial 03 (Exercise 4), you built a search engine over your own text corpus. Now, replicate the pipeline from Sections 7–9 on **your own data**.\n",
    "\n",
    "Your task:\n",
    "1. **Load your corpus chunks** from `custom_corpus/chunks/` (or wherever Tutorial 03 saved them). If you used the default Sherlock Holmes text, choose a different text for this exercise. *(2 pts)*\n",
    "2. **Run BERT NER** on at least 100 chunks and build a **co-occurrence Knowledge Graph**. Print the top-10 entities and the number of nodes/edges. *(4 pts)*\n",
    "3. **Embed your chunks** with `all-MiniLM-L6-v2` and build a **FAISS index**. Test it with 3 domain-specific queries and print the top-3 results for each. *(4 pts)*\n",
    "4. **Extract at least 30 atomic facts** and generate at least 20 **QA test pairs**. Save everything to `custom_corpus/evaluation/`. *(3 pts)*\n",
    "5. **Briefly describe** (in a markdown cell, minimum 100 words) how the quality of your KG and atomic facts depends on the quality of the NER model and the text domain. What works well? What fails? *(2 pts)*\n",
    "\n",
    "Store your final KG as `my_kg` (NetworkX graph), your FAISS index as `my_faiss_index`, and your QA pairs as `my_qa_pairs` (list of dicts).\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b65afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE — Exercise 4\n",
    "# Follow the steps outlined above. You can reuse all functions from Sections 7–9.\n",
    "# Store your results in: my_kg, my_faiss_index, my_qa_pairs\n",
    "\n",
    "raise NotImplementedError(\"Replace this line with your solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b5a10f",
   "metadata": {},
   "source": [
    "YOUR REFLECTION HERE (minimum 100 words on NER quality, KG quality, domain effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d090e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell — do not modify\n",
    "assert 'my_kg' in dir(), \"You need to create 'my_kg' (a NetworkX graph)\"\n",
    "assert 'my_faiss_index' in dir(), \"You need to create 'my_faiss_index' (a FAISS index)\"\n",
    "assert 'my_qa_pairs' in dir(), \"You need to create 'my_qa_pairs' (a list of QA dicts)\"\n",
    "assert isinstance(my_kg, nx.Graph), \"my_kg should be a NetworkX Graph\"\n",
    "assert my_kg.number_of_nodes() >= 5, \"my_kg should have at least 5 nodes\"\n",
    "assert my_faiss_index.ntotal >= 10, \"my_faiss_index should contain at least 10 vectors\"\n",
    "assert len(my_qa_pairs) >= 20, \"my_qa_pairs should contain at least 20 QA pairs\"\n",
    "assert all('question' in q and 'ground_truth_answer' in q for q in my_qa_pairs), \\\n",
    "    \"Each QA pair must have 'question' and 'ground_truth_answer' keys\"\n",
    "print(f\"KG: {my_kg.number_of_nodes()} nodes, {my_kg.number_of_edges()} edges\")\n",
    "print(f\"FAISS index: {my_faiss_index.ntotal} vectors\")\n",
    "print(f\"QA pairs: {len(my_qa_pairs)}\")\n",
    "print(\"All auto-graded tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.0"
  },
  "nbgrader": {
   "grade": false,
   "grade_id": "",
   "locked": false,
   "schema_version": 3,
   "solution": false,
   "task": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
