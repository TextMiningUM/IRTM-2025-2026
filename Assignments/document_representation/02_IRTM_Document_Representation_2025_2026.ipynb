{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230611e8",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f301d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c63ecf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32f3576",
   "metadata": {
    "id": "UIBbWrF5_DXC"
   },
   "source": [
    "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
    "\n",
    "# Information Retrieval and Text Mining Course - Tutorial Document Representation\n",
    "Author: Gijs Wijngaard and Jan Scholtes\n",
    "\n",
    "Version: 2025-2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd44056",
   "metadata": {
    "id": "14xe-BeO_G34"
   },
   "source": [
    "Welcome to the tutorial about **document representation**. A fundamental challenge in Information Retrieval and Text Mining is converting unstructured text into numerical representations that algorithms can process. In this notebook you will explore a progression of methods — from simple counting-based approaches to modern neural embeddings:\n",
    "\n",
    "1. **One-Hot Encoding** — the simplest binary representation\n",
    "2. **N-grams & Bag-of-Words** — counting word occurrences\n",
    "3. **TF-IDF** — weighting terms by importance\n",
    "4. **Cosine Similarity** — measuring document similarity\n",
    "5. **Word2Vec** — learning dense word embeddings\n",
    "6. **Sentence Transformers** — contextual sentence embeddings\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c8ff05",
   "metadata": {},
   "source": [
    "## 1. One-Hot Encoding\n",
    "\n",
    "The simplest way to represent words as numbers is **one-hot encoding**. Given a vocabulary $V = \\{w_1, w_2, \\ldots, w_{|V|}\\}$, each word $w_i$ is represented as a binary vector $\\mathbf{e}_i \\in \\{0, 1\\}^{|V|}$ where:\n",
    "\n",
    "$$\n",
    "\\mathbf{e}_i[j] = \\begin{cases} 1 & \\text{if } j = i \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "For example, with vocabulary $V = \\{\\text{cat}, \\text{dog}, \\text{fox}\\}$:\n",
    "- cat $\\rightarrow [1, 0, 0]$\n",
    "- dog $\\rightarrow [0, 1, 0]$\n",
    "- fox $\\rightarrow [0, 0, 1]$\n",
    "\n",
    "**Limitations:**\n",
    "- Vectors are very high-dimensional (size of vocabulary, typically 10,000+)\n",
    "- All word vectors are **orthogonal** — there is no notion of similarity ($\\text{cosine}(\\mathbf{e}_i, \\mathbf{e}_j) = 0$ for $i \\neq j$)\n",
    "- No semantic information is captured (\"king\" and \"queen\" are as different as \"king\" and \"banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935da78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentence = \"the quick brown fox jumps over the lazy dog\"\n",
    "words = sentence.split()\n",
    "vocabulary = sorted(set(words))\n",
    "\n",
    "print(f\"Vocabulary ({len(vocabulary)} unique words): {vocabulary}\\n\")\n",
    "\n",
    "# Create one-hot vectors for each unique word\n",
    "one_hot = {word: np.eye(len(vocabulary))[i].astype(int) for i, word in enumerate(vocabulary)}\n",
    "\n",
    "for word, vector in one_hot.items():\n",
    "    print(f\"  {word:6s} → {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35a559",
   "metadata": {
    "id": "f1Ih_CLWBDAe"
   },
   "source": [
    "## 2. N-grams and Bag-of-Words\n",
    "\n",
    "Instead of representing individual words, we can capture **sequences** of words. An **n-gram** is a contiguous sequence of $n$ items from a text. Given a sequence of tokens $w_1, w_2, \\ldots, w_m$, the set of n-grams is:\n",
    "\n",
    "$$\n",
    "\\text{ngrams}(n) = \\{(w_i, w_{i+1}, \\ldots, w_{i+n-1}) \\mid 1 \\leq i \\leq m - n + 1\\}\n",
    "$$\n",
    "\n",
    "Let's see this in practice with our example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d1851",
   "metadata": {
    "id": "eOx7S5c5BCr-"
   },
   "outputs": [],
   "source": [
    "sentence = \"the quick brown fox jumps over the lazy dog\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416f6596",
   "metadata": {
    "id": "pcKShOniC-gK"
   },
   "source": [
    "A **bigram** ($n=2$) groups two consecutive words together. Bigrams capture local word co-occurrence patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1250b",
   "metadata": {
    "id": "R1D1tNh-CXaG"
   },
   "outputs": [],
   "source": [
    "splitted = sentence.split(\" \")\n",
    "[bigram for bigram in zip(splitted, splitted[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1dc21b",
   "metadata": {
    "id": "6H_ZcDytEF3o"
   },
   "source": [
    "With the grouping of 3 words together, we call it a trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fcbf9c",
   "metadata": {
    "id": "Edis7nAnD_J7"
   },
   "outputs": [],
   "source": [
    "[trigram for trigram in zip(splitted, splitted[1:], splitted[2:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da8e7a8",
   "metadata": {
    "id": "hXF2g4anFN0L"
   },
   "source": [
    "### Bag-of-Words (BoW)\n",
    "\n",
    "A **Bag-of-Words** representation ignores word order entirely and represents a document as a vector of word counts. Given vocabulary $V = \\{w_1, \\ldots, w_{|V|}\\}$, a document $d$ is represented as:\n",
    "\n",
    "$$\n",
    "\\mathbf{d} = [\\text{tf}(w_1, d), \\text{tf}(w_2, d), \\ldots, \\text{tf}(w_{|V|}, d)]\n",
    "$$\n",
    "\n",
    "where $\\text{tf}(w, d)$ is the **term frequency** — the number of times word $w$ appears in document $d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9d1e4",
   "metadata": {
    "id": "K0mi71kCE6Aq"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "bag_of_words = Counter(splitted)\n",
    "print(\"Bag-of-Words representation:\")\n",
    "for word, count in bag_of_words.items():\n",
    "    print(f\"  {word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7dfbc7",
   "metadata": {
    "id": "6iTvjbKCFYjX"
   },
   "source": [
    "Notice that \"the\" has a count of 2, while all other words appear once. Words like \"the\", \"a\", \"and\" are called **stop words** — they are extremely common but carry little meaning about the document's topic. We need a weighting scheme that reduces the importance of such words. This is where **TF-IDF** comes in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001fdcd5",
   "metadata": {
    "id": "1_UDIfVi39c1"
   },
   "source": [
    "<a name=\"dataset\"></a>\n",
    "\n",
    "## 3. Dataset\n",
    "\n",
    "We use a [movie review dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/) from NLTK. This dataset contains **1000 positive** and **1000 negative** movie reviews, making it suitable for **sentiment analysis** — classifying reviews as positive or negative based on their word content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb4184",
   "metadata": {
    "id": "fz8sw8uN38x8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words, movie_reviews as mr\n",
    "nltk_words = set(words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c6825a",
   "metadata": {
    "id": "hlGANEGI8d_O"
   },
   "source": [
    "We first remove the punctuation from all the words, and afterwards we count the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f766b72",
   "metadata": {
    "id": "fazKk8kh5Peg"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "def remove_punct(word):\n",
    "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "    return word if word in nltk_words else ''\n",
    "all_words = Counter(filter(remove_punct, mr.words()))\n",
    "all_words.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7ad65",
   "metadata": {
    "id": "IvMOSCRS81nm"
   },
   "source": [
    "The same problem we have here. Words such as *the* and *a* are the most common amongst the movie reviews of our dataset. However, to do something with the movie review, such as classifying it, we should give a lower probability to these words, as they do not say much about the content itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e978a5",
   "metadata": {
    "id": "Ty641CpU_1xk"
   },
   "outputs": [],
   "source": [
    "documents = [(list(filter(remove_punct, mr.words(f))), mr.categories(f)) for f in mr.fileids()]\n",
    "print(\"Total number of documents:\", len(documents))\n",
    "print(\"Total number of words in first document:\", len(documents[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e10c2c",
   "metadata": {
    "id": "U6DBjrwDGoow"
   },
   "source": [
    "## 4. TF-IDF (Term Frequency–Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18529ce",
   "metadata": {
    "id": "qXl-jPkoGsZW"
   },
   "source": [
    "TF-IDF assigns a **weight** to each term in a document that reflects how important that term is relative to the collection. The score **increases** with the number of occurrences in a document and **increases** with the rarity of the term across all documents.\n",
    "\n",
    "The TF-IDF weight of term $t$ in document $d$ is:\n",
    "\n",
    "$$\n",
    "w_{t,d} = \\text{tf-idf}(t, d) = \\log(1 + \\text{tf}_{t,d}) \\times \\log_{10}\\!\\left(\\frac{N}{\\text{df}_t}\\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\text{tf}_{t,d}$ = **term frequency**: number of times term $t$ appears in document $d$\n",
    "- $\\text{df}_t$ = **document frequency**: number of documents in the collection containing term $t$\n",
    "- $N$ = total number of documents in the collection\n",
    "- $\\log(1 + \\text{tf}_{t,d})$ applies **sublinear** scaling — a word appearing 10× is not 10× as important\n",
    "- $\\log_{10}(N / \\text{df}_t)$ is the **inverse document frequency (IDF)** — rare terms get higher weight\n",
    "\n",
    "**Key insight**: A term gets a high TF-IDF score when it appears frequently in a specific document (high TF) but rarely across the collection (high IDF). Common words like \"the\" will have $\\text{df}_t \\approx N$, giving $\\text{IDF} \\approx 0$.\n",
    "\n",
    "> **Note:** There are many TF-IDF variants. A popular alternative is **BM25** (Best Matching 25), which adds document length normalization and a saturation parameter. BM25 is used by search engines like Elasticsearch and Solr."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ca4fc",
   "metadata": {
    "id": "27SyhyooCggX"
   },
   "source": [
    "Lets start with calculating the term frequency (tf). Now, we calculated the number of words for all documents. However, to calculate the tf-idf score we need to calculate the term-frequency for each term per document. Thus, we need to loop over the documents and count the occurrences of the terms per document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050e366a",
   "metadata": {
    "id": "61lfkNqbkSU8"
   },
   "outputs": [],
   "source": [
    "tf = [Counter(words) for words, category in documents]\n",
    "tf[0].most_common(10) # Most common terms for the first document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc6c566",
   "metadata": {
    "id": "CV9FnDB2oJxH"
   },
   "source": [
    "Now let's calculate the **document frequency** ($\\text{df}$). For each word in our vocabulary, we count how many documents contain that word. We convert each document to a `set` (unique words) so that membership lookup is $O(1)$ instead of $O(n)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40daf192",
   "metadata": {
    "id": "d2WbKAjXhs5a"
   },
   "outputs": [],
   "source": [
    "setted_docs = [set(doc) for doc, category in documents]\n",
    "df = {word: sum([1 for doc in setted_docs if word in doc]) for word in all_words.keys()}\n",
    "list(df.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a060dd",
   "metadata": {
    "id": "RSkSlby1JVEV"
   },
   "source": [
    "### Computing TF-IDF from Scratch\n",
    "\n",
    "Now let's implement the TF-IDF formula ourselves using `numpy`. For each document, we loop over its words and compute the TF-IDF weight. Notice how we use `np.log` for the sublinear TF scaling and `np.log10` for the IDF:\n",
    "\n",
    "> **Observe**: After running this, look at which words get the highest TF-IDF scores. Are they the common words like \"the\" and \"a\", or more distinctive words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5122a8d",
   "metadata": {
    "id": "CdIeXFByuDd6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = len(documents)\n",
    "\n",
    "# Compute TF-IDF for each word in each document\n",
    "tfidf = []\n",
    "for i in range(N):\n",
    "    tfidf_doc = {}\n",
    "    for word, count in tf[i].items():\n",
    "        tfidf_doc[word] = np.log(1 + count) * np.log10(N / df[word])\n",
    "    tfidf.append(tfidf_doc)\n",
    "\n",
    "# Check: print top 5 TF-IDF terms for the first document\n",
    "sorted_first = sorted(tfidf[0].items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top 5 TF-IDF terms for document 1:\")\n",
    "for word, score in sorted_first:\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "# Compare with a common word\n",
    "if \"the\" in tfidf[0]:\n",
    "    print(f\"\\n  'the' TF-IDF: {tfidf[0]['the']:.4f}  ← very low because it appears in nearly every document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc888d5",
   "metadata": {
    "id": "HsgtPmwkaN9f"
   },
   "source": [
    "### Comparing TF-IDF across Positive and Negative Reviews\n",
    "\n",
    "Let's see whether positive and negative movie reviews use distinctive words. We compute the **average TF-IDF** per word separately for positive and negative reviews:\n",
    "\n",
    "> **Observe**: Look at the top words for each sentiment. Do you see sentiment-related words (\"great\", \"bad\") or mostly movie-related words (\"film\", \"story\")? What does this tell us about using TF-IDF for classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81870dfc",
   "metadata": {
    "id": "H4aqUOZ0pZZ0"
   },
   "outputs": [],
   "source": [
    "# Separate positive and negative documents\n",
    "pos_indices = [i for i, (_, cat) in enumerate(documents) if 'pos' in cat]\n",
    "neg_indices = [i for i, (_, cat) in enumerate(documents) if 'neg' in cat]\n",
    "\n",
    "# Average TF-IDF per word for positive reviews\n",
    "pos_avg = {}\n",
    "for i in pos_indices:\n",
    "    for word, score in tfidf[i].items():\n",
    "        pos_avg[word] = pos_avg.get(word, 0) + score\n",
    "pos_avg = {w: s / len(pos_indices) for w, s in pos_avg.items()}\n",
    "\n",
    "# Average TF-IDF per word for negative reviews\n",
    "neg_avg = {}\n",
    "for i in neg_indices:\n",
    "    for word, score in tfidf[i].items():\n",
    "        neg_avg[word] = neg_avg.get(word, 0) + score\n",
    "neg_avg = {w: s / len(neg_indices) for w, s in neg_avg.items()}\n",
    "\n",
    "top_positive = sorted(pos_avg.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "top_negative = sorted(neg_avg.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "print(\"Top 20 words in POSITIVE reviews:\")\n",
    "for word, score in top_positive:\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 20 words in NEGATIVE reviews:\")\n",
    "for word, score in top_negative:\n",
    "    print(f\"  {word}: {score:.4f}\")\n",
    "\n",
    "# Find words unique to each list\n",
    "pos_words = set(w for w, _ in top_positive)\n",
    "neg_words = set(w for w, _ in top_negative)\n",
    "print(f\"\\nWords only in positive top-20: {pos_words - neg_words}\")\n",
    "print(f\"Words only in negative top-20: {neg_words - pos_words}\")\n",
    "print(f\"Shared words: {pos_words & neg_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0724f56d",
   "metadata": {},
   "source": [
    "### Visualizing TF-IDF Weights\n",
    "\n",
    "Let's visualize how TF-IDF assigns different weights to words. We use scikit-learn's `TfidfVectorizer` to compute TF-IDF on a small set of example documents and display the result as a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f71a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Small example corpus\n",
    "example_docs = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"the fox jumped over the lazy dog\",\n",
    "    \"a quick brown fox\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(example_docs)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names,\n",
    "                        index=[f\"Doc {i+1}\" for i in range(len(example_docs))])\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(tfidf_df, annot=True, fmt=\".2f\", cmap=\"YlOrRd\", linewidths=0.5)\n",
    "plt.title(\"TF-IDF Weights per Document\")\n",
    "plt.ylabel(\"Documents\")\n",
    "plt.xlabel(\"Terms\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how common words like 'the' get lower weights,\")\n",
    "print(\"while distinctive words like 'mat', 'jumped', 'quick' get higher weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f01a1",
   "metadata": {},
   "source": [
    "## 5. Cosine Similarity\n",
    "\n",
    "Once we have vector representations of documents (whether BoW, TF-IDF, or embeddings), we need a way to measure **how similar** two documents are. The most commonly used measure in IR is **cosine similarity**.\n",
    "\n",
    "For two vectors $\\mathbf{a}$ and $\\mathbf{b}$, cosine similarity is defined as:\n",
    "\n",
    "$$\n",
    "\\text{cos}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\|} = \\frac{\\sum_{i=1}^{n} a_i \\, b_i}{\\sqrt{\\sum_{i=1}^{n} a_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} b_i^2}}\n",
    "$$\n",
    "\n",
    "- The result ranges from $-1$ (opposite) to $1$ (identical direction), with $0$ meaning orthogonal (unrelated).\n",
    "- For non-negative vectors (like TF-IDF), the range is $[0, 1]$.\n",
    "- Cosine similarity measures **angle**, not magnitude — a long document and a short document with the same word proportions will have high similarity.\n",
    "\n",
    "Let's compute the cosine similarity between our example TF-IDF documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61953a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute pairwise cosine similarity on TF-IDF vectors\n",
    "cos_sim = cosine_similarity(tfidf_matrix)\n",
    "cos_df = pd.DataFrame(cos_sim, \n",
    "                       index=[f\"Doc {i+1}\" for i in range(len(example_docs))],\n",
    "                       columns=[f\"Doc {i+1}\" for i in range(len(example_docs))])\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cos_df, annot=True, fmt=\".3f\", cmap=\"Blues\", vmin=0, vmax=1, linewidths=0.5)\n",
    "plt.title(\"Cosine Similarity between Documents (TF-IDF)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Documents:\")\n",
    "for i, doc in enumerate(example_docs):\n",
    "    print(f\"  Doc {i+1}: \\\"{doc}\\\"\")\n",
    "print(\"\\nDoc 1 and Doc 2 share 'the' and 'cat' → moderate similarity\")\n",
    "print(\"Doc 3 and Doc 4 share 'fox' → some similarity\")\n",
    "print(\"Doc 1 and Doc 4 share nothing meaningful → low similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ded6f",
   "metadata": {
    "id": "yams1sh5wljY"
   },
   "source": [
    "## 6. Word2Vec — Dense Word Embeddings\n",
    "\n",
    "So far, our representations have been **sparse** and **high-dimensional** (one dimension per vocabulary word). Word2Vec (Mikolov et al., 2013) learns **dense, low-dimensional** vectors (typically 100-300 dimensions) where **semantically similar words are close together** in vector space.\n",
    "\n",
    "The core idea is the **distributional hypothesis**: *\"A word is characterized by the company it keeps.\"* (Firth, 1957)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a679626",
   "metadata": {
    "id": "Br2FrWAyzZtG"
   },
   "source": [
    "Word2Vec has two training architectures:\n",
    "\n",
    "1. **Skip-gram**: Given a center word $w_t$, predict context words $w_{t+j}$ within a window of size $c$. The objective maximizes:\n",
    "\n",
    "$$\n",
    "\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, \\, j \\neq 0} \\log P(w_{t+j} \\mid w_t)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "P(w_O \\mid w_I) = \\frac{\\exp(\\mathbf{v}'_{w_O} \\cdot \\mathbf{v}_{w_I})}{\\sum_{w=1}^{|V|} \\exp(\\mathbf{v}'_w \\cdot \\mathbf{v}_{w_I})}\n",
    "$$\n",
    "\n",
    "2. **CBOW (Continuous Bag of Words)**: Given context words, predict the center word. Faster to train but less effective on rare words.\n",
    "\n",
    "In practice, **negative sampling** is used instead of the full softmax to make training tractable.\n",
    "\n",
    "The most common implementation for Word2Vec in Python is [gensim](https://radimrehurek.com/gensim/models/word2vec.html). Let's train Word2Vec on our movie reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aac702",
   "metadata": {
    "id": "zWcCTa7Vx28V"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=[doc for doc, cat in documents])\n",
    "word_vectors = model.wv\n",
    "word_vectors['the']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8462f4bc",
   "metadata": {
    "id": "qUWGDYUc0qFQ"
   },
   "source": [
    "We can find the most similar vector nearby a word using `most_similar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62db582e",
   "metadata": {
    "id": "Gbk1ZsaUzrGe"
   },
   "outputs": [],
   "source": [
    "word_vectors.most_similar('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494dc4a1",
   "metadata": {
    "id": "vu_9uBVx3Mfu"
   },
   "source": [
    "And we can even do arithmetic with it. The most famous example of this is the `king + man - woman = queen` analogy. By adding the vector of king and man to each other, and subtracting the vector of woman, we should get the queen vector. Lets try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aa2828",
   "metadata": {
    "id": "SrD-Erlzzuvb"
   },
   "outputs": [],
   "source": [
    "word_vectors.most_similar(positive=['king','woman'],negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44b3d35",
   "metadata": {
    "id": "vfQD-O4P3199"
   },
   "source": [
    "We get queen as the second most similar vector. We only trained our word2vec model on our reviews dataset which is a small dataset for word2vec standards, so that makes sense.\n",
    "\n",
    "Lastly, lets plot the data. For this, we need to represent our vectors as a 2-d space. For this, we need a dimensionality reduction technique, such as PCA or t-SNE. We use t-SNE (invented by someone who did the same master as you are doing!). It might take a while to compute the vectors below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc986a5",
   "metadata": {
    "id": "Xm5VC75O4l8U"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "vectors = tsne.fit_transform(np.asarray(model.wv.vectors))\n",
    "x, y = zip(*vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7760e",
   "metadata": {
    "id": "GMg0YUpX8_0R"
   },
   "outputs": [],
   "source": [
    "len(x), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaddbe1f",
   "metadata": {
    "id": "_K__i6RQ7Jzf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe96f3",
   "metadata": {
    "id": "QdtYf5ehuhM6"
   },
   "source": [
    "## 7. Pretrained Word Embeddings (GloVe)\n",
    "\n",
    "Word2Vec works best with **pretrained embeddings** — vectors trained on massive corpora by researchers and made publicly available.\n",
    "\n",
    "**GloVe** (Global Vectors, Pennington et al., 2014) is a popular alternative to Word2Vec. While Word2Vec uses local context windows, GloVe combines:\n",
    "- **Global co-occurrence statistics** (how often words appear together across the entire corpus)  \n",
    "- **Local context** (word-word co-occurrence within windows)\n",
    "\n",
    "GloVe optimizes the objective:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i,j=1}^{|V|} f(X_{ij}) \\left( \\mathbf{w}_i^T \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
    "$$\n",
    "\n",
    "where $X_{ij}$ is the co-occurrence count and $f$ is a weighting function that prevents very common co-occurrences from dominating.\n",
    "\n",
    "Let's download pretrained GloVe vectors (trained on Wikipedia + Gigaword, 6B tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a484c",
   "metadata": {
    "id": "cO1sUsQOu2x6"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "glove = gensim.downloader.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c66731",
   "metadata": {
    "id": "PTBHPYXQvmTI"
   },
   "outputs": [],
   "source": [
    "glove[\"king\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385ba08",
   "metadata": {
    "id": "e0DHPtgXpXf_"
   },
   "source": [
    "### Sentiment Classification with GloVe\n",
    "\n",
    "Let's see how well GloVe embeddings can be used for sentiment classification. The approach:\n",
    "\n",
    "1. For each document, get the GloVe vector for every word (skip words not in the vocabulary)\n",
    "2. **Average** all word vectors to create a single document embedding\n",
    "3. Split into 80/20 train/test\n",
    "4. Train a `LogisticRegression` classifier\n",
    "\n",
    "> **Observe**: Note the libraries we use (`train_test_split`, `LogisticRegression`, `accuracy_score`) and how we handle words missing from the GloVe vocabulary. Also note how we average word vectors with `np.mean(vectors, axis=0)` — this is a common technique to get a document-level representation from word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bac8ed",
   "metadata": {
    "id": "Z-v99KUopn_s"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Create averaged GloVe vectors for each document\n",
    "X_glove = []\n",
    "y_labels = []\n",
    "for words, categories in documents:\n",
    "    word_vectors = [glove[w] for w in words if w in glove]   # skip unknown words\n",
    "    if word_vectors:\n",
    "        X_glove.append(np.mean(word_vectors, axis=0))        # average all word vectors\n",
    "    else:\n",
    "        X_glove.append(np.zeros(50))  # fallback: zero vector for GloVe-50d\n",
    "    y_labels.append(1 if 'pos' in categories else 0)\n",
    "\n",
    "X_glove = np.array(X_glove)\n",
    "y_labels = np.array(y_labels)\n",
    "\n",
    "print(f\"Document matrix shape: {X_glove.shape}  (2000 docs × 50 dimensions)\")\n",
    "\n",
    "# Train/test split (80/20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_glove, y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a Logistic Regression classifier\n",
    "clf_glove = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_glove.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = clf_glove.predict(X_test)\n",
    "glove_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"GloVe + LogisticRegression — Accuracy: {glove_accuracy:.4f}\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1cad6",
   "metadata": {},
   "source": [
    "> **Why not just accuracy?** Accuracy tells you the overall fraction of correct predictions, but it can be misleading — especially with **imbalanced classes**. For example, if 95% of documents are positive, a model that always predicts \"positive\" achieves 95% accuracy but is completely useless for detecting negative documents. **Precision** (of your positive predictions, how many are actually positive?), **Recall** (of all actual positives, how many did you find?), and **F1-score** (harmonic mean of precision and recall) give a per-class breakdown that reveals whether the model truly performs well on *all* classes, not just the majority. Always report these alongside accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801fb26a",
   "metadata": {
    "id": "7Rj4cz1zMCeL"
   },
   "source": [
    "## Bias in Word2Vec\n",
    "One of the problems with Word2Vec (and with machine learning in general) is that there is lots of biases assumed by the model. Examples of biases that can be harmful when using these algorithms include gender bias and ethnicity bias. Lets check for example what happens if we take the female equivalent of `doctor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4940e",
   "metadata": {
    "id": "mAjcNaqcMF_T"
   },
   "outputs": [],
   "source": [
    "glove.most_similar(positive=['doctor','woman'],negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f496d6a",
   "metadata": {
    "id": "jDdSAOEuT0b5"
   },
   "source": [
    "### Exploring Bias in Word Embeddings\n",
    "\n",
    "Word embeddings learn associations from training data — including societal biases present in that data. Let's explore **gender bias** and **professional stereotypes** using analogy queries.\n",
    "\n",
    "> **Observe**: Look at the analogy results below. What patterns do you notice? Are these associations factual, or do they reflect harmful stereotypes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece02931",
   "metadata": {
    "id": "SXAt62XxURRM"
   },
   "source": [
    "**Why do these biases exist?**\n",
    "\n",
    "Word embeddings are trained on large corpora of web text, news articles, and books that reflect historical and societal prejudices. When these embeddings are used in downstream applications — such as search engines, chatbots, hiring tools, or criminal justice risk assessment — they can **perpetuate and amplify discrimination**.\n",
    "\n",
    "For example, if a resume-screening system uses GloVe embeddings that associate \"programmer\" more closely with \"man\" than \"woman\", it may systematically rank male candidates higher. This is an active area of research in **AI fairness** and **debiasing** (Bolukbasi et al., 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ca6b3",
   "metadata": {
    "id": "TIVT4MwNUQ86"
   },
   "outputs": [],
   "source": [
    "# Example 1: Gender bias in professions\n",
    "print(\"man:programmer :: woman:?\")\n",
    "print(glove.most_similar(positive=['programmer', 'woman'], negative=['man'])[:5])\n",
    "\n",
    "print(\"\\nman:doctor :: woman:?\")\n",
    "print(glove.most_similar(positive=['doctor', 'woman'], negative=['man'])[:5])\n",
    "\n",
    "# Example 2: Professional stereotypes\n",
    "print(\"\\nfather:doctor :: mother:?\")\n",
    "print(glove.most_similar(positive=['doctor', 'mother'], negative=['father'])[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e366fb2",
   "metadata": {
    "id": "qGeOKjbPmc46"
   },
   "source": [
    "## 8. Sentence Transformers — Contextual Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acad68ce",
   "metadata": {
    "id": "LmZj5iBvmhOX"
   },
   "source": [
    "A key limitation of Word2Vec and GloVe is that they produce **static embeddings** — each word has exactly one vector regardless of context. The word \"bank\" gets the same vector whether it means \"river bank\" or \"financial bank\".\n",
    "\n",
    "**Transformer-based models** (like BERT) solve this by producing **contextual embeddings** — the vector for each word depends on its surrounding context. These models use the **self-attention mechanism**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "where $Q$, $K$, $V$ are query, key, and value matrices derived from input embeddings, and $d_k$ is the dimension of the keys.\n",
    "\n",
    "**Sentence Transformers** (Reimers & Gurevych, 2019) extend BERT by applying **mean pooling** over all token embeddings to produce a single fixed-size vector for an entire sentence. This is efficient and well-suited for tasks like:\n",
    "- Semantic search\n",
    "- Sentence similarity\n",
    "- Clustering\n",
    "- Sentiment classification\n",
    "\n",
    "| Feature | Word2Vec / GloVe | Sentence Transformers |\n",
    "|:--------|:-----------------|:---------------------|\n",
    "| Type | Static | Contextual |\n",
    "| Granularity | Word-level | Sentence-level |\n",
    "| Polysemy | One vector per word | Context-dependent |\n",
    "| Training data | Co-occurrence | Masked language model |\n",
    "| Typical dims | 50–300 | 384–1024 |\n",
    "\n",
    "Let's load a sentence transformer model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5801d7",
   "metadata": {
    "id": "XvSjvFzQm-g-"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3f836",
   "metadata": {
    "id": "DRRDncajnI0R"
   },
   "source": [
    "We can encode any sentence like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f130749",
   "metadata": {
    "id": "PGubLNl4nBwc"
   },
   "outputs": [],
   "source": [
    "sentence_embedding = sentence_model.encode(\"the quick brown fox jumps over the lazy dog\")\n",
    "sentence_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e29826",
   "metadata": {
    "id": "NE9xT8UJnR5s"
   },
   "source": [
    "We now get a vector of 384 instead of a matrix of 11 by 768. This makes it much easier to deal with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5686e2",
   "metadata": {
    "id": "jT8Xg5I6eW_W"
   },
   "source": [
    "### Sentiment Classification with Sentence Transformers\n",
    "\n",
    "Let's repeat the same sentiment classification task using **Sentence Transformers** instead of GloVe. The key differences are:\n",
    "\n",
    "1. We join each word list into a full sentence string\n",
    "2. `sentence_model.encode()` produces a single 384-d vector per document (no manual averaging needed)\n",
    "3. We use the same Logistic Regression setup for a fair comparison\n",
    "\n",
    "> **Observe**: Compare the Precision, Recall, and F1-score with the GloVe classifier above. Why do we look at these metrics instead of only accuracy? Which representation performs better and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c6139",
   "metadata": {
    "id": "gu12iOsop8qE"
   },
   "outputs": [],
   "source": [
    "# Convert documents to sentence strings\n",
    "doc_strings = [\" \".join(words) for words, cat in documents]\n",
    "\n",
    "# Encode with sentence transformers (produces 384-d vectors directly)\n",
    "X_st = sentence_model.encode(doc_strings, show_progress_bar=True)\n",
    "\n",
    "# Same labels as before\n",
    "X_train_st, X_test_st, y_train_st, y_test_st = train_test_split(\n",
    "    X_st, y_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train classifier\n",
    "clf_st = LogisticRegression(max_iter=1000, random_state=42)\n",
    "clf_st.fit(X_train_st, y_train_st)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_st = clf_st.predict(X_test_st)\n",
    "st_accuracy = accuracy_score(y_test_st, y_pred_st)\n",
    "print(f\"Sentence Transformers + LogisticRegression — Accuracy: {st_accuracy:.4f}\\n\")\n",
    "print(classification_report(y_test_st, y_pred_st, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19be292",
   "metadata": {
    "id": "jkF-yx-Yp2op"
   },
   "source": [
    "**Why does Sentence Transformers outperform GloVe?**\n",
    "\n",
    "Sentence Transformers typically achieve higher accuracy than GloVe for several reasons:\n",
    "1. **Contextual embeddings** capture word meaning in context, while GloVe uses static vectors\n",
    "2. **Sentence-level representations** from mean pooling capture the overall meaning, rather than simply averaging word vectors\n",
    "3. **Pre-training on NLI tasks** makes sentence transformers better at understanding semantic relationships\n",
    "\n",
    "To further improve accuracy, we could fine-tune the sentence transformer on our specific dataset, use a larger model (e.g., `all-mpnet-base-v2`), or fine-tune BERT end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c969956",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Now that you have seen all the document representation methods and their library implementations, answer the following exercises to check your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac099a",
   "metadata": {},
   "source": [
    "### Exercise 1 — Understanding Document Representations (3 points)\n",
    "\n",
    "Answer the following questions in your own words:\n",
    "\n",
    "1. **TF-IDF vs. Bag-of-Words**: Why does TF-IDF give a lower weight to the word \"the\" compared to a rare domain-specific word like \"cinematography\"? Explain using the formulas you saw in Section 4.\n",
    "\n",
    "2. **Static vs. contextual embeddings**: The word \"bank\" can mean a *river bank* or a *financial institution*. Explain how Word2Vec/GloVe and Sentence Transformers handle this differently, and what the practical consequence is for downstream tasks like search or classification.\n",
    "\n",
    "3. **Document averaging**: In the GloVe classifier demo, we averaged all word vectors in a document to get a single vector. What information is lost by this averaging, and how do Sentence Transformers address this limitation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05166893",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "99732bf4494654e26c61736244dbf1bc",
     "grade": true,
     "grade_id": "ex1_solution_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5968fc",
   "metadata": {},
   "source": [
    "### Exercise 2 — Bias and Fairness in Embeddings (3 points)\n",
    "\n",
    "You saw in Section 7 that GloVe encodes societal biases. Answer the following:\n",
    "\n",
    "1. **Come up with 2 new bias examples**: Using `glove.most_similar(positive=[...], negative=[...])`, demonstrate at least **two new bias analogies** that we did **not** show in the demo (e.g., racial bias, age bias, nationality stereotypes). Show your code and results.\n",
    "\n",
    "2. **Static vs. contextual bias**: Do you think Sentence Transformers (contextual models) also suffer from bias? Why or why not? What is fundamentally different about how they learn word associations?\n",
    "\n",
    "3. **Mitigation**: Suggest one concrete technique that could reduce bias in word embeddings used for a job-matching application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823af86",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1f530653ce8ac6a27b00f347943f2f5",
     "grade": false,
     "grade_id": "ex2_solution_2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3fa31",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8a29b45e860d92a34c654e3c9502d25",
     "grade": true,
     "grade_id": "ex2_solution_3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489735a1",
   "metadata": {},
   "source": [
    "### Exercise 3 — TF-IDF Sentiment Classifier (4 points)\n",
    "\n",
    "In the demos above, you saw classifiers built with **GloVe** and **Sentence Transformers**. Now build a sentiment classifier using **TF-IDF** representations:\n",
    "\n",
    "1. Use `TfidfVectorizer` from scikit-learn to transform the movie review documents into TF-IDF vectors\n",
    "2. Split the data 80/20 with `random_state=42`\n",
    "3. Train a `LogisticRegression(max_iter=1000, random_state=42)` classifier\n",
    "4. Store predictions as `y_pred_tfidf`\n",
    "5. Print both the **accuracy** and the full `classification_report` (Precision, Recall, F1) — compare these with the GloVe and Sentence Transformer results\n",
    "\n",
    "> **Hint**: You'll need to convert each document (list of words) to a string first, similar to what you saw in the Sentence Transformers demo. Look at how `TfidfVectorizer` was used in Section 4 for the heatmap. Use `accuracy_score` and `classification_report` from `sklearn.metrics` as shown in the demos above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab0af7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1f111eeca673dc76a69f798fa31c038",
     "grade": false,
     "grade_id": "ex3_solution_4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "tfidf_accuracy = accuracy_score(y_test_tfidf, y_pred_tfidf)\n",
    "print(f\"TF-IDF + LogisticRegression — Accuracy: {tfidf_accuracy:.4f}\\n\")\n",
    "print(classification_report(y_test_tfidf, y_pred_tfidf, target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831c81b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e30f19143feb3132284c796d0506635",
     "grade": true,
     "grade_id": "ex3_tests",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "360b85ae",
   "metadata": {},
   "source": [
    "### Exercise 4 — Why Does TF-IDF Beat Dense Embeddings Here? (3 points)\n",
    "\n",
    "Look at the results from the three classifiers above (TF-IDF, GloVe, Sentence Transformers). You may notice that the \"simplest\" representation — TF-IDF — actually achieves the **highest** accuracy and F1-score. This is counter-intuitive: shouldn't more sophisticated models perform better?\n",
    "\n",
    "Answer the following questions:\n",
    "\n",
    "1. **TF-IDF advantage**: Why might TF-IDF + Logistic Regression outperform GloVe averaging for sentiment classification specifically? Think about how each representation handles individual sentiment-bearing words like \"terrible\", \"masterpiece\", or \"boring\".\n",
    "\n",
    "2. **GloVe averaging weakness**: When we average all word vectors in a document into a single 50-d vector, what information is lost? Consider what happens when a review has 300 neutral words and 5 strongly negative words.\n",
    "\n",
    "3. **Sentence Transformer limitations**: The model `all-MiniLM-L6-v2` has a maximum input length of ~256 tokens. Movie reviews are often much longer. How does this affect performance compared to TF-IDF, which sees *all* words? Also, this model was pre-trained on sentence similarity tasks — not sentiment. How would fine-tuning change the result?\n",
    "\n",
    "4. **General lesson**: What does this tell you about the relationship between model complexity and task performance? When should you use simple baselines vs. deep learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc8d08",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "354690bc27176b09ec82560b3275c7f2",
     "grade": true,
     "grade_id": "ex4_solution_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c42d1ca",
   "metadata": {},
   "source": [
    "## 9. Summary — Comparison of Document Representation Methods\n",
    "\n",
    "| Method | Type | Dimensions | Captures Semantics? | Handles Polysemy? | Key Advantage | Key Limitation |\n",
    "|:-------|:-----|:-----------|:--------------------|:-----------------|:-------------|:--------------|\n",
    "| **One-Hot** | Sparse, binary | size of vacabulary (10K+) | No | No | Simple, interpretable | No similarity between words |\n",
    "| **Bag-of-Words** | Sparse, count | size of vacabulary | No | No | Captures word frequency | Ignores word order and importance |\n",
    "| **TF-IDF** | Sparse, weighted | size of vacabulary | Partially | No | Weights by importance | Still high-dimensional, no semantics |\n",
    "| **Word2Vec** | Dense, static | 100–300 | Yes | No | Captures analogies and similarity | One vector per word |\n",
    "| **GloVe** | Dense, static | 50–300 | Yes | No | Combines global + local statistics | One vector per word |\n",
    "| **Sentence Transformers** | Dense, contextual | 384–1024 | Yes | Yes | Context-aware, sentence-level | Computationally expensive |\n",
    "\n",
    "**The evolution**: From sparse, high-dimensional, context-free representations → dense, low-dimensional, context-aware embeddings. Each step captures more linguistic information but requires more computational resources."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
