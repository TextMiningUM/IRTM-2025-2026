{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230611e8",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f301d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c63ecf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c54e46",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6edaecad233b23bf479630530a3ca103",
     "grade": false,
     "grade_id": "cell-1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "![maastricht-university-logo.jpeg](data:image/jpeg;base64,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)\n",
    "#Faculty of Science and Engineering - Department of Advanced Computer Sciences\n",
    "# Course Information Retrieval and Text Mining - Tutorial Building Structured Representations from Text - Part 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ef742",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0522e2171b8a375703a1410beaf04547",
     "grade": false,
     "grade_id": "cell-2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "By Jan Scholtes\n",
    "\n",
    "**Version 2025-2026**\n",
    "\n",
    "Welcome to the tutorial for the lectures on *Building Structured Representations from Text*. In this notebook you will learn how various NLP approaches let us deal with syntactic structures, semantics, co-reference & pronoun resolution, and negation handling — all fundamental steps on the road to building Knowledge Graphs from text.\n",
    "\n",
    "**Learning Objectives:**\n",
    "1. Understand and apply sentence detection, tokenization, stemming, and lemmatization\n",
    "2. Understand POS tagging: rule-based, statistical (HMM), and discriminative (CRF) approaches\n",
    "3. Know the mathematical foundations of HMMs and CRFs for sequence labelling\n",
    "4. Understand phrase detection (chunking) and dependency parsing\n",
    "5. Understand the principles of co-reference resolution and negation scope detection\n",
    "\n",
    "These methods have been used since the 1970s. In many aspects, they are limited and often slow. The goal of this tutorial is to let you experience hands-on the challenges in NLP (especially dealing with ambiguity), but also the limitations of grammar-based approaches (e.g. not being able to deal with ambiguity, wrong spelling, or unexpected grammatical use). This will help you understand why statistical and deep-learning methods are so much better for many NLP tasks.\n",
    "\n",
    "In Part 2 of this tutorial we will build on these foundations to perform Named Entity Recognition, Relation Extraction, and Knowledge Graph construction.\n",
    "\n",
    "In this notebook, we use the NLTK library throughout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553ee70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccc2d40d554cee77045d24800609f3fe",
     "grade": false,
     "grade_id": "cell-3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Library Installation\n",
    "\n",
    "We install all required packages in a single cell. Run this cell once at the beginning of your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2339eec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db9525acca9c410dd452dd9815c93ebf",
     "grade": false,
     "grade_id": "cell-4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\"python-crfsuite\", \"sklearn-crfsuite\", \"fastcoref\"]\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b9c54d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "630c08b500fc2489c105b9d446337e65",
     "grade": false,
     "grade_id": "cell-5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('book')\n",
    "nltk.download('tagsets_json')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('treebank')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db4b273",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5b0af31927f40847fa75e8e98facfd5",
     "grade": false,
     "grade_id": "cell-6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "NLTK comes with many corpora, toy grammars, trained models, etc. A complete list is posted at: https://www.nltk.org/nltk_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41720ae3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a3df710c0a50ac99b63433b0f3d86a00",
     "grade": false,
     "grade_id": "cell-7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2. Load Data\n",
    "\n",
    "We will use the Moby Dick corpus from NLTK for our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e982f998",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1464784d9df8740ccda5d596b2af7834",
     "grade": false,
     "grade_id": "cell-8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44f302",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db60ebdb1b7cabf0f78883d4fc04f163",
     "grade": false,
     "grade_id": "cell-9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "list_of_text = text1\n",
    "print(list_of_text[0:1000:1])  # print first 1000 tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e9934",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5182c10424520d8a3d6fe6a224d0c74",
     "grade": false,
     "grade_id": "cell-10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's convert this list to a long string, as this is what NLTK requires as input and what you would normally get from any preprocessing text-extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e615b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77da01406f6a3881aac90a036fcc8784",
     "grade": false,
     "grade_id": "cell-11",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "RawTextMobyDick = \" \".join(list_of_text)\n",
    "print(RawTextMobyDick[:2000])  # print first 2000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21709b54",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3aab010e58fcfa5bdc937607155459c",
     "grade": false,
     "grade_id": "cell-12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 3. Sentence Detection\n",
    "\n",
    "Sentence detection (or sentence segmentation) is the task of splitting a text into individual sentences. This is harder than it looks — periods can appear in abbreviations (\"Dr.\"), numbers (\"3.14\"), and URLs.\n",
    "\n",
    "NLTK's Punkt tokenizer uses an unsupervised algorithm trained on large corpora to detect sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45932dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03bd40bbff284f14256bada01cc335bf",
     "grade": false,
     "grade_id": "cell-13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "sentences = tokenizer.tokenize(RawTextMobyDick)\n",
    "\n",
    "for i, s in enumerate(sentences[:30]):\n",
    "    print(f\"[{i}] {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b526c721",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9a365cc74d13b53b0647dc64bca4f98",
     "grade": false,
     "grade_id": "cell-14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 4. Tokenization\n",
    "\n",
    "We tokenize the text into individual words (tokens). We leave punctuation in, as we need it later for linguistic operations like POS tagging and parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f96e4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "22062a879d7c3141fe7afd47a43a20b3",
     "grade": false,
     "grade_id": "cell-15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "words = nltk.tokenize.word_tokenize(RawTextMobyDick)\n",
    "\n",
    "for i, w in enumerate(words[:50]):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd73d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "296dd01b5dde69efdf6ac93785bd8909",
     "grade": false,
     "grade_id": "cell-16",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 5. Stemming\n",
    "\n",
    "Stemming reduces words to their root form by stripping suffixes using simple rules. The most common stemmer for English is the **Porter Stemmer** (1980).\n",
    "\n",
    "While fast, stemming often produces non-words (e.g. \"studies\" → \"studi\", \"university\" → \"univers\"). It has no linguistic knowledge — it merely applies character-level transformation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdb7d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9889e52e1003f3a251cc83f869ec0f88",
     "grade": false,
     "grade_id": "cell-17",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i, w in enumerate(words[:100]):\n",
    "    print(f\"{w:20s} → {ps.stem(w)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41390515",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ac66e4b24564fab57711f9e5f7614b3",
     "grade": false,
     "grade_id": "cell-18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 6. POS Tagging (Rule-Based)\n",
    "\n",
    "**Part-of-Speech (POS) tagging** is the process of assigning a grammatical tag (noun, verb, adjective, etc.) to each word in a text based on its definition and context.\n",
    "\n",
    "Schools teach 9 parts of speech in English: noun, verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection. However, annotation schemes like the **Penn Treebank** tag set use ~45 fine-grained tags.\n",
    "\n",
    "## The Penn Treebank\n",
    "\n",
    "The Penn Treebank (PTB) project annotated 2,499 Wall Street Journal stories with syntactic structure. Its POS tag set has become the de facto standard. Below is a summary of the most important tags:\n",
    "\n",
    "| Tag | Description | Example |\n",
    "|-----|------------|---------|\n",
    "| NN  | Noun, singular | dog, city |\n",
    "| NNS | Noun, plural | dogs, cities |\n",
    "| NNP | Proper noun, singular | London, Jan |\n",
    "| VB  | Verb, base form | run, eat |\n",
    "| VBD | Verb, past tense | ran, ate |\n",
    "| VBG | Verb, gerund | running, eating |\n",
    "| VBN | Verb, past participle | run, eaten |\n",
    "| JJ  | Adjective | big, fast |\n",
    "| RB  | Adverb | quickly, very |\n",
    "| DT  | Determiner | the, a, an |\n",
    "| IN  | Preposition | in, of, by |\n",
    "| PRP | Personal pronoun | he, she, it |\n",
    "| CC  | Coordinating conjunction | and, but, or |\n",
    "\n",
    "The full list can be viewed with `nltk.help.upenn_tagset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7952c3e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff4bafc7c32d02258ab5c49bcdb6d223",
     "grade": false,
     "grade_id": "cell-19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c0797",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f73b5df531897b346b70336e93806f29",
     "grade": false,
     "grade_id": "cell-20",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's apply POS tagging to our text using NLTK's default tagger (a trained Averaged Perceptron model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1771a87",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7bd0527ec6cdc90d419bce6681245721",
     "grade": false,
     "grade_id": "cell-21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "words_with_pos = nltk.pos_tag(words)\n",
    "\n",
    "for i, w in enumerate(words_with_pos[:100]):\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea753212",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1924184a5ad6b7405c10540f7cd04af9",
     "grade": false,
     "grade_id": "cell-22",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 7. Lemmatization with POS Tags\n",
    "\n",
    "Stemming algorithms simply remove suffixes using rules, often producing non-linguistic outputs. **Lemmatization** instead performs morphological analysis — it returns the dictionary form (**lemma**) of a word by considering its grammatical role.\n",
    "\n",
    "For example:\n",
    "- \"better\" → \"good\" (adjective lemma)\n",
    "- \"ran\" → \"run\" (verb lemma)\n",
    "- \"mice\" → \"mouse\" (noun lemma)\n",
    "\n",
    "NLTK's `WordNetLemmatizer` requires the POS tag to work correctly. It accepts simplified tags: `'n'` (noun), `'v'` (verb), `'a'` (adjective), `'r'` (adverb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8065de0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "37b41bb9704b40348aa85a43d9b2d09f",
     "grade": false,
     "grade_id": "cell-23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Map Penn Treebank tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # default to noun\n",
    "\n",
    "# Demonstrate lemmatization vs stemming\n",
    "demo_words = [\"better\", \"worse\", \"running\", \"ran\", \"mice\", \"studies\", \"universities\", \"went\"]\n",
    "demo_tagged = nltk.pos_tag(demo_words)\n",
    "\n",
    "print(f\"{'Word':15s} {'POS':5s} {'Stem':15s} {'Lemma':15s}\")\n",
    "print(\"-\" * 55)\n",
    "for word, tag in demo_tagged:\n",
    "    stem = ps.stem(word)\n",
    "    lemma = lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "    print(f\"{word:15s} {tag:5s} {stem:15s} {lemma:15s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318b4e78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9da36bd8c622ec21665febc155a8cd93",
     "grade": false,
     "grade_id": "cell-24",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice how lemmatization produces actual English words (\"better\" → \"good\", \"ran\" → \"run\"), while stemming produces non-words (\"better\" → \"better\", \"ran\" → \"ran\" or truncated forms)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74015dec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "255f378b89db59c04ecad14f6a11ba8a",
     "grade": false,
     "grade_id": "cell-25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 8. Phrase Detection (Chunking)\n",
    "\n",
    "Above POS tags, sentences have higher-level syntactic structure. The most common phrases are **Noun Phrases (NP)** and **Verb Phrases (VP)**. Detecting these is useful for:\n",
    "- Understanding where a noun begins and ends (e.g. \"the big red dog\")\n",
    "- Identifying subjects and objects\n",
    "- Extracting candidate entities for Knowledge Graphs\n",
    "\n",
    "We define phrase patterns using **regular expressions over POS tags** and parse them with NLTK's `RegexpParser`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075974f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bc9addbeee380c33ebf7fe9b9603907a",
     "grade": false,
     "grade_id": "cell-26",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Noun Phrase Chunking\n",
    "\n",
    "A simple NP grammar: an optional determiner, followed by zero or more adjectives, followed by a noun:\n",
    "\n",
    "```\n",
    "NP: {<DT>?<JJ>*<NN.*>+}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f8a95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "482570b22c710b8b52ed00180792e98c",
     "grade": false,
     "grade_id": "cell-27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import RegexpParser\n",
    "\n",
    "sentence = \"The big red dog chased the small white cat across the green field\"\n",
    "sentence_words = nltk.tokenize.word_tokenize(sentence)\n",
    "pos_sentence = nltk.pos_tag(sentence_words)\n",
    "print(\"POS tags:\", pos_sentence)\n",
    "\n",
    "# Define NP grammar\n",
    "grammar = 'NP: {<DT>?<JJ>*<NN.*>+}'\n",
    "cp = RegexpParser(grammar)\n",
    "result = cp.parse(pos_sentence)\n",
    "print(\"\\nChunked result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881bc2a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24dcd414a32cdd8d0e2e511622a2470b",
     "grade": false,
     "grade_id": "cell-28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Verb Phrase Chunking\n",
    "\n",
    "We can similarly define a VP pattern — e.g. a modal or auxiliary verb followed by a main verb:\n",
    "\n",
    "```\n",
    "VP: {<MD>?<VB.*>+}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d351d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "511cdda0863f448742d0b4b2ec09c769",
     "grade": false,
     "grade_id": "cell-29",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "grammar_vp = \"\"\"\n",
    "    NP: {<DT>?<JJ>*<NN.*>+}\n",
    "    VP: {<MD>?<VB.*>+}\n",
    "\"\"\"\n",
    "cp_vp = RegexpParser(grammar_vp)\n",
    "\n",
    "sentence2 = \"The student should have completed the assignment before the deadline\"\n",
    "words2 = nltk.tokenize.word_tokenize(sentence2)\n",
    "pos2 = nltk.pos_tag(words2)\n",
    "result2 = cp_vp.parse(pos2)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45232996",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2ef8abe01c7c684c1471aadb74ee990b",
     "grade": false,
     "grade_id": "cell-30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 9. Statistical POS Tagging — Hidden Markov Models (HMMs)\n",
    "\n",
    "The rule-based NLTK tagger works reasonably well, but to understand *why*, we need to look at the statistical models behind sequence labelling.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "A **Hidden Markov Model** is a generative probabilistic model for sequences. It models the **joint probability** of a tag sequence $T = t_1, t_2, \\ldots, t_n$ and a word sequence $W = w_1, w_2, \\ldots, w_n$.\n",
    "\n",
    "An HMM is defined by:\n",
    "\n",
    "**1. States:** The set of POS tags $\\{t_1, t_2, \\ldots, t_K\\}$\n",
    "\n",
    "**2. Transition probabilities** — the probability of moving from tag $t_i$ to tag $t_j$:\n",
    "\n",
    "$$a_{ij} = P(t_n = j \\mid t_{n-1} = i)$$\n",
    "\n",
    "**3. Emission probabilities** — the probability of observing word $w$ in state (tag) $t$:\n",
    "\n",
    "$$b_t(w) = P(w \\mid t)$$\n",
    "\n",
    "**4. Initial state distribution** $\\pi_i = P(t_1 = i)$\n",
    "\n",
    "## The Tagging Problem\n",
    "\n",
    "Given a word sequence $W$, find the most likely tag sequence $\\hat{T}$:\n",
    "\n",
    "$$\\hat{T} = \\arg\\max_{T} P(T \\mid W)$$\n",
    "\n",
    "Using Bayes' theorem:\n",
    "\n",
    "$$P(T \\mid W) \\propto P(W \\mid T) \\cdot P(T)$$\n",
    "\n",
    "With the Markov and independence assumptions, this becomes:\n",
    "\n",
    "$$P(T \\mid W) \\propto \\prod_{i=1}^{n} \\underbrace{P(w_i \\mid t_i)}_{\\text{emission}} \\cdot \\underbrace{P(t_i \\mid t_{i-1})}_{\\text{transition}}$$\n",
    "\n",
    "## The Viterbi Algorithm\n",
    "\n",
    "Finding the optimal tag sequence via brute force is exponential ($K^n$ possible sequences). The **Viterbi algorithm** uses dynamic programming to find the optimal path in $O(n \\cdot K^2)$ time.\n",
    "\n",
    "At each position $i$ and for each tag $t$, it computes:\n",
    "\n",
    "$$V_i(t) = \\max_{t'} \\left[ V_{i-1}(t') \\cdot a_{t',t} \\cdot b_t(w_i) \\right]$$\n",
    "\n",
    "Then backtracks from the end to recover the optimal tag sequence.\n",
    "\n",
    "## Why HMMs Struggle with Unknown Words\n",
    "\n",
    "Since HMMs model $P(w \\mid t)$, if a word $w$ was never seen during training, the emission probability $b_t(w) = 0$ for all tags. This makes the entire sequence probability zero — the model completely fails on out-of-vocabulary words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d55e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "952f7ef37bc9a470fa362abed5ff823d",
     "grade": false,
     "grade_id": "cell-31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training an HMM Tagger\n",
    "\n",
    "We use the Penn Treebank corpus in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c455a84",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19582e31bdbe4b4d623e3e2803e7867c",
     "grade": false,
     "grade_id": "cell-32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc8ca5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47c8f3831950ac51bb1346cc6ece3432",
     "grade": false,
     "grade_id": "cell-33",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Inspect the tag distribution\n",
    "fd = FreqDist()\n",
    "for word, tag in treebank.tagged_words():\n",
    "    fd[tag] += 1\n",
    "\n",
    "print(f\"Total unique tags: {len(fd)}\")\n",
    "print(f\"\\nTop 15 tags:\")\n",
    "for tag, count in fd.most_common(15):\n",
    "    print(f\"  {tag:6s} : {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096e7773",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6962d09333708980fe68209dda695f3",
     "grade": false,
     "grade_id": "cell-34",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train HMM tagger (supervised / MLE)\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "tagger = trainer.train_supervised(treebank.tagged_sents())\n",
    "\n",
    "print(f\"Total tagged sentences used for training: {len(treebank.tagged_sents())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e88ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "64788af17c1cf3ad773e42c055d09e3b",
     "grade": false,
     "grade_id": "cell-35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "# Test on a normal sentence\n",
    "result = tagger.tag(word_tokenize(\"Today is a good day. Yesterday was also a great day\"))\n",
    "print(\"Tags:\", result)\n",
    "print(f\"\\nLog probability: {tagger.log_probability(result):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02abcd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "656531f9ef8c15e9cbe63ca4c39c950f",
     "grade": false,
     "grade_id": "cell-36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test on a sentence with out-of-vocabulary words\n",
    "result_oov = tagger.tag(word_tokenize(\n",
    "    \"Jan Scholtes is a name that does not occur in the corpus. What do you observe?\"\n",
    "))\n",
    "print(\"Tags:\", result_oov)\n",
    "print(f\"\\nLog probability: {tagger.log_probability(result_oov):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c8afa0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88b70ac196ad0a19ccb180a0012e76bc",
     "grade": false,
     "grade_id": "cell-37",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice how the HMM assigns a very low (or $-\\infty$) probability to sentences containing unknown words like \"Jan\" and \"Scholtes\". The emission probability for unseen words is essentially zero, which causes the entire sequence probability to collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb53180",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e7ac6bc1618ef48fcea00769948672e9",
     "grade": false,
     "grade_id": "cell-38",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 10. Discriminative POS Tagging — Conditional Random Fields (CRFs)\n",
    "\n",
    "## From Generative to Discriminative\n",
    "\n",
    "HMMs are **generative** models — they model the joint probability $P(W, T)$ and need to estimate $P(w \\mid t)$ for every word-tag pair. This is their Achilles' heel for unknown words.\n",
    "\n",
    "**Conditional Random Fields (CRFs)** are **discriminative** models — they directly model $P(T \\mid W)$ without ever needing to model how words are generated. This fundamental difference allows CRFs to use arbitrary **feature functions** that can look at the spelling, capitalization, suffixes, and context of words.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "A linear-chain CRF defines:\n",
    "\n",
    "$$P(T \\mid W) = \\frac{1}{Z(W)} \\exp\\left(\\sum_{i=1}^{n} \\sum_{k} \\lambda_k \\cdot f_k(t_{i-1}, t_i, W, i)\\right)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $f_k(t_{i-1}, t_i, W, i)$ are **feature functions** that can examine the entire input $W$, the current position $i$, and the current and previous tags\n",
    "- $\\lambda_k$ are learned **weights** for each feature function\n",
    "- $Z(W) = \\sum_{T'} \\exp\\left(\\sum_{i} \\sum_{k} \\lambda_k \\cdot f_k(t'_{i-1}, t'_i, W, i)\\right)$ is the **partition function** (normalisation constant)\n",
    "\n",
    "## Feature Functions — The Key Advantage\n",
    "\n",
    "Feature functions can capture rich patterns. Examples:\n",
    "\n",
    "| Feature function | Fires when... | Helps with... |\n",
    "|---|---|---|\n",
    "| $f(t_i, w_i)$: word is capitalised and $t_i$ = NNP | Current word starts with uppercase | Proper noun detection |\n",
    "| $f(t_i, w_i)$: word ends in \"-ing\" and $t_i$ = VBG | Word has gerund suffix | Verb form recognition |\n",
    "| $f(t_i, w_i)$: word ends in \"-tion\" and $t_i$ = NN | Word has nominal suffix | Noun detection |\n",
    "| $f(t_{i-1}, t_i)$: previous tag is DT, current is NN | Determiner → Noun pattern | Syntactic context |\n",
    "| $f(t_i, w_i)$: word contains digits and $t_i$ = CD | Word looks like a number | Cardinal number detection |\n",
    "\n",
    "These features allow CRFs to handle **unknown words** — even if \"Scholtes\" was never seen in training, the capitalisation feature fires, suggesting NNP (proper noun).\n",
    "\n",
    "## HMM vs CRF — Comparison\n",
    "\n",
    "| Aspect | HMM | CRF |\n",
    "|--------|-----|-----|\n",
    "| Model type | Generative: $P(W, T)$ | Discriminative: $P(T \\mid W)$ |\n",
    "| Unknown words | Fails (zero emission probability) | Handles via features (capitalisation, suffixes) |\n",
    "| Features | Only word identity | Arbitrary: spelling, context, suffixes, prefixes |\n",
    "| Training | Fast (counting) | Slower (iterative optimisation) |\n",
    "| Independence | $w_i$ depends only on $t_i$ | Can look at entire input $W$ |\n",
    "| Optimal when | Large training data, closed vocabulary | Open vocabulary, rich features needed |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3db86",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e1782426443f103b1d35298906c8fe9",
     "grade": false,
     "grade_id": "cell-39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Training a CRF Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ffdcb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5a987074a5d242701be32f97e3f1c24",
     "grade": false,
     "grade_id": "cell-40",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pycrfsuite\n",
    "from nltk.tag import CRFTagger\n",
    "\n",
    "train_data = treebank.tagged_sents()\n",
    "print(f\"Total tagged sentences: {len(train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8a199",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0285be78d63aa9e28a175e997937c68",
     "grade": false,
     "grade_id": "cell-41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Train the CRF tagger (this may take a few minutes)\n",
    "taggerCRF = CRFTagger(verbose=True)\n",
    "taggerCRF.train(train_data, 'model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f789abb9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5cb74ad57727376d5e29db504032428",
     "grade": false,
     "grade_id": "cell-42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test on the same sentences as the HMM\n",
    "print(\"Normal sentence:\")\n",
    "print(taggerCRF.tag(word_tokenize(\"Today is a good day. Yesterday was also a great day\")))\n",
    "\n",
    "print(\"\\nSentence with unknown words:\")\n",
    "print(taggerCRF.tag(word_tokenize(\n",
    "    \"Jan Scholtes is a name that does not occur in the corpus. What do you observe?\"\n",
    ")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38da436",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "be892f7e8968fa7d778f8251da582982",
     "grade": false,
     "grade_id": "cell-43",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice how the CRF correctly tags \"Jan\" and \"Scholtes\" as NNP (proper nouns), even though they never appeared in the training data. The capitalisation and context features allow the model to generalise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328683e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d176e3a64aacb2feb0f5594cbff4b6b",
     "grade": false,
     "grade_id": "cell-44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Inspecting CRF Feature Functions\n",
    "\n",
    "We can look at what features the CRF extracts for each word position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd9c49",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3276ff42e4d05958479a30afa50aa0ec",
     "grade": false,
     "grade_id": "cell-45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Show features for a sample sentence\n",
    "sample_tokens = word_tokenize(\"Jan Scholtes works at Maastricht University\")\n",
    "for i, token in enumerate(sample_tokens):\n",
    "    features = taggerCRF._get_features(sample_tokens, i)\n",
    "    print(f\"\\nToken '{token}' — Features:\")\n",
    "    for f in features:\n",
    "        print(f\"  {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5496a4ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20ad0113ee6345f344ae492704fd44c5",
     "grade": false,
     "grade_id": "cell-46",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 11. Dependency Parsing\n",
    "\n",
    "## From Phrase Structure to Dependencies\n",
    "\n",
    "So far we have detected **phrases** (NPs, VPs) using chunking grammars. But these flat chunks don't tell us *how* words relate to each other across the sentence.\n",
    "\n",
    "**Dependency parsing** identifies binary **head-dependent** relationships between words. Each word in a sentence depends on exactly one other word (its *head*), except the *root* of the sentence.\n",
    "\n",
    "A dependency relation is a triple: **(head, relation, dependent)**\n",
    "\n",
    "For example, in \"The cat sat on the mat\":\n",
    "- (sat, **nsubj**, cat) — \"cat\" is the nominal subject of \"sat\"\n",
    "- (cat, **det**, The) — \"The\" is the determiner of \"cat\"\n",
    "- (sat, **obl**, mat) — \"mat\" is an oblique dependent of \"sat\"\n",
    "- (mat, **case**, on) — \"on\" marks the case of \"mat\"\n",
    "- (mat, **det**, the) — \"the\" determines \"mat\"\n",
    "\n",
    "## Universal Dependencies\n",
    "\n",
    "The [Universal Dependencies](https://universaldependencies.org/) (UD) project defines a cross-lingual annotation scheme with ~37 universal dependency relations, including:\n",
    "\n",
    "| Relation | Meaning | Example |\n",
    "|----------|---------|---------|\n",
    "| nsubj | Nominal subject | *John* runs |\n",
    "| obj | Direct object | reads *books* |\n",
    "| det | Determiner | *the* dog |\n",
    "| amod | Adjectival modifier | *big* house |\n",
    "| nmod | Nominal modifier | house *of cards* |\n",
    "| neg / advmod | Negation | does *not* run |\n",
    "| conj | Conjunction | cats *and* dogs |\n",
    "\n",
    "Dependency parsing is essential for:\n",
    "- **Relation extraction** (who did what to whom)\n",
    "- **Co-reference resolution** (linking pronouns to their antecedents)\n",
    "- **Negation scope detection** (what is being negated)\n",
    "- **Knowledge Graph construction** (Part 2 of this tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e42d520",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a631fcadb861404de2fe8429e93701a0",
     "grade": false,
     "grade_id": "cell-47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Dependency Parsing in NLTK\n",
    "\n",
    "NLTK can parse dependency structures from annotated data in CoNLL format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bde1f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ba83aaa8b0c07a51f802f9eeaa83c66f",
     "grade": false,
     "grade_id": "cell-48",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.parse import DependencyGraph\n",
    "\n",
    "treebank_data = \"\"\"Pierre  NNP     2       NMOD\n",
    "Vinken  NNP     8       SUB\n",
    ",       ,       2       P\n",
    "61      CD      5       NMOD\n",
    "years   NNS     6       AMOD\n",
    "old     JJ      2       NMOD\n",
    ",       ,       2       P\n",
    "will    MD      0       ROOT\n",
    "join    VB      8       VC\n",
    "the     DT      11      NMOD\n",
    "board   NN      9       OBJ\n",
    "as      IN      9       VMOD\n",
    "a       DT      15      NMOD\n",
    "nonexecutive    JJ      15      NMOD\n",
    "director        NN      12      PMOD\n",
    "Nov.    NNP     9       VMOD\n",
    "29      CD      16      NMOD\n",
    ".       .       9       VMOD\n",
    "\"\"\"\n",
    "\n",
    "dg = DependencyGraph(treebank_data)\n",
    "\n",
    "# Print the dependency tree\n",
    "print(\"Dependency tree:\")\n",
    "dg.tree().pprint()\n",
    "\n",
    "# Print the triples (head, relation, dependent)\n",
    "print(\"\\nDependency triples:\")\n",
    "for head, rel, dep in dg.triples():\n",
    "    print(f\"  ({head[0]}, {rel}, {dep[0]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106eb8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c49c4fbb89492b5f2341139c6b68f517",
     "grade": false,
     "grade_id": "cell-49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Parse the CoNLL-format dependency data\n",
    "dg = DependencyGraph(treebank_data)\n",
    "\n",
    "# Print the dependency tree\n",
    "print(\"Dependency Tree:\")\n",
    "dg.tree().pretty_print()\n",
    "\n",
    "# Print all dependency triples (head, relation, dependent)\n",
    "print(\"\\nDependency Triples:\")\n",
    "for triple in dg.triples():\n",
    "    print(f\"  {triple[0][0]:15s} --{triple[1]:6s}--> {triple[2][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371b931b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88c34828997cae0b2c81c9d78ede5a54",
     "grade": false,
     "grade_id": "cell-50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 12. Co-reference and Pronoun Resolution\n",
    "\n",
    "## What is Co-reference?\n",
    "\n",
    "**Co-reference resolution** is the task of determining which expressions in a text refer to the same real-world entity.\n",
    "\n",
    "Consider:\n",
    "\n",
    "> *\"**Barack Obama** was born in Hawaii. **He** graduated from Harvard Law School. **The former president** later moved to Chicago.\"*\n",
    "\n",
    "Here, \"Barack Obama\", \"He\", and \"The former president\" all refer to the **same entity**. A co-reference system groups them into a **mention cluster**.\n",
    "\n",
    "## Types of Referring Expressions\n",
    "\n",
    "| Type | Example | Challenge |\n",
    "|------|---------|----------|\n",
    "| **Pronouns** | he, she, it, they | Require antecedent identification |\n",
    "| **Definite NPs** | the president, the company | Require world knowledge |\n",
    "| **Proper nouns** | Obama, Mr. Obama | Aliases are tricky |\n",
    "| **Demonstratives** | this, that, these | Often refer to events |\n",
    "\n",
    "## How Dependency Grammar Helps\n",
    "\n",
    "Dependency parsing provides structural clues for pronoun resolution:\n",
    "\n",
    "1. **Subject preference** — pronouns tend to refer to the **subject** of the previous clause (`nsubj`)\n",
    "2. **Syntactic constraints** — Binding Theory: a pronoun cannot co-refer with a noun it c-commands in the same clause\n",
    "3. **Parallelism** — in coordinated clauses, pronouns map to the same grammatical role\n",
    "\n",
    "Below we build a simple co-reference pipeline step by step, using only the libraries you already know from this tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38044e8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b7e2e4cd3d157b17a79547b6451e52f",
     "grade": false,
     "grade_id": "cell-51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 1 — Finding Pronouns and Candidate Antecedents\n",
    "\n",
    "We use **POS tagging** (Section 6) to locate pronouns (`PRP`, `PRP$`) and **NP chunking** (Section 8) to find candidate antecedent noun phrases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252df16e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4050069ddc2d9196f8733dfa4180e301",
     "grade": false,
     "grade_id": "cell-52",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, RegexpParser\n",
    "\n",
    "# --- sample multi-sentence text ---\n",
    "text = (\"Barack Obama was born in Hawaii. \"\n",
    "        \"He graduated from Harvard Law School. \"\n",
    "        \"The former president later moved to Chicago.\")\n",
    "\n",
    "# Split into sentences (re-use the tokenizer from Section 3)\n",
    "sents = tokenizer.tokenize(text)\n",
    "\n",
    "# NP chunker (same grammar as Section 8)\n",
    "np_grammar = 'NP: {<DT>?<JJ>*<NN.*>+}'\n",
    "np_chunker = RegexpParser(np_grammar)\n",
    "\n",
    "# --- collect mentions across all sentences ---\n",
    "mentions = []  # list of dicts\n",
    "\n",
    "for sent_idx, sent in enumerate(sents):\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree   = np_chunker.parse(tagged)\n",
    "\n",
    "    # 1) Noun-phrase chunks  →  candidate antecedents\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        np_text = ' '.join(w for w, t in subtree.leaves())\n",
    "        mentions.append({\n",
    "            'sentence': sent_idx, 'text': np_text,\n",
    "            'type': 'NP', 'tags': [t for _, t in subtree.leaves()]\n",
    "        })\n",
    "\n",
    "    # 2) Pronouns\n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        if tag in ('PRP', 'PRP$'):\n",
    "            mentions.append({\n",
    "                'sentence': sent_idx, 'text': word,\n",
    "                'type': 'pronoun', 'tags': [tag]\n",
    "            })\n",
    "\n",
    "# --- display ---\n",
    "print(f\"{'Type':10s} {'Sent':4s}  Text\")\n",
    "print('-' * 50)\n",
    "for m in mentions:\n",
    "    print(f\"{m['type']:10s} {m['sentence']:<4d}  {m['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc055fb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "57c6d4f2d6267269c4d26922def8bc5e",
     "grade": false,
     "grade_id": "cell-53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Notice how POS tagging and NP chunking — tools you already used in Sections 6 and 8 — let us automatically extract every pronoun **and** every candidate antecedent noun phrase from the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455bf519",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4f1d74d1c34fe8f72e47767086c9648",
     "grade": false,
     "grade_id": "cell-54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 2 — A Simple Rule-Based Pronoun Resolver\n",
    "\n",
    "We now link each pronoun to its most likely antecedent using two simple rules:\n",
    "\n",
    "1. **Recency** — prefer the closest preceding NP\n",
    "2. **Proper-noun preference** — prefer NPs that contain a proper noun (`NNP`), because pronouns most often refer to named entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a246d2e3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a8f98006b2f176b971f42b0cfb37837",
     "grade": false,
     "grade_id": "cell-55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def resolve_pronouns(mentions):\n",
    "    \"\"\"Link each pronoun to its best preceding NP antecedent.\"\"\"\n",
    "    antecedents = []   # running list of NPs seen so far\n",
    "    resolutions = []\n",
    "\n",
    "    for m in mentions:\n",
    "        if m['type'] == 'NP':\n",
    "            antecedents.append(m)\n",
    "        elif m['type'] == 'pronoun' and antecedents:\n",
    "            # Score candidates: prefer NPs with proper nouns, then recency\n",
    "            def score(np):\n",
    "                has_nnp = any(t.startswith('NNP') for t in np['tags'])\n",
    "                return (has_nnp, -abs(m['sentence'] - np['sentence']))\n",
    "\n",
    "            best = max(antecedents, key=score)\n",
    "            resolutions.append((m, best))\n",
    "\n",
    "    return resolutions\n",
    "\n",
    "# --- run resolver ---\n",
    "resolutions = resolve_pronouns(mentions)\n",
    "\n",
    "print(\"Pronoun resolutions:\")\n",
    "for pronoun, antecedent in resolutions:\n",
    "    print(f\"  '{pronoun['text']}' (sent {pronoun['sentence']})  \"\n",
    "          f\"→  '{antecedent['text']}' (sent {antecedent['sentence']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd2374d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "369a57e3dbd1bf4e5a52de186d737e15",
     "grade": false,
     "grade_id": "cell-56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The resolver correctly links *\"He\"* and *\"The former president\"* back to *\"Barack Obama\"* — using only POS tagging and NP chunking. But what happens when we throw a harder text at it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7289b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- A harder text: two named entities + multiple pronouns ---\n",
    "hard_text = (\"Dr. Sarah Chen met Professor James Lee at the annual conference. \"\n",
    "             \"She presented her groundbreaking research on climate change. \"\n",
    "             \"He asked several challenging questions about her methodology. \"\n",
    "             \"Later, she invited him to collaborate on a joint paper.\")\n",
    "\n",
    "# Re-use the same pipeline from Step 1\n",
    "sents_hard = tokenizer.tokenize(hard_text)\n",
    "mentions_hard = []\n",
    "\n",
    "for sent_idx, sent in enumerate(sents_hard):\n",
    "    tokens = word_tokenize(sent)\n",
    "    tagged = pos_tag(tokens)\n",
    "    tree   = np_chunker.parse(tagged)\n",
    "    for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):\n",
    "        np_text = ' '.join(w for w, t in subtree.leaves())\n",
    "        mentions_hard.append({\n",
    "            'sentence': sent_idx, 'text': np_text,\n",
    "            'type': 'NP', 'tags': [t for _, t in subtree.leaves()]\n",
    "        })\n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        if tag in ('PRP', 'PRP$'):\n",
    "            mentions_hard.append({\n",
    "                'sentence': sent_idx, 'text': word,\n",
    "                'type': 'pronoun', 'tags': [tag]\n",
    "            })\n",
    "\n",
    "resolutions_hard = resolve_pronouns(mentions_hard)\n",
    "\n",
    "print(\"Text:\", hard_text)\n",
    "print(\"\\nRule-based pronoun resolutions:\")\n",
    "correct = 0\n",
    "total   = len(resolutions_hard)\n",
    "expected = {\n",
    "    'She': 'Dr. Sarah Chen', 'her': 'Dr. Sarah Chen',\n",
    "    'He': 'Professor James Lee', 'him': 'Professor James Lee',\n",
    "    'she': 'Dr. Sarah Chen'\n",
    "}\n",
    "for pronoun, antecedent in resolutions_hard:\n",
    "    p = pronoun['text']\n",
    "    a = antecedent['text']\n",
    "    # Check correctness (simplified: \"He\"/\"him\" should map to James Lee)\n",
    "    if p.lower() in ('he', 'him'):\n",
    "        is_correct = 'James Lee' in a\n",
    "    else:\n",
    "        is_correct = 'Sarah Chen' in a or 'Dr.' in a\n",
    "    mark = \"correct\" if is_correct else \"WRONG\"\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "    print(f\"  '{p}' (sent {pronoun['sentence']})  -->  '{a}'  [{mark}]\")\n",
    "\n",
    "print(f\"\\nAccuracy: {correct}/{total} — the rule-based resolver has no concept of \"\n",
    "      f\"gender, so 'He' and 'him' are wrongly linked to 'Dr. Sarah Chen'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3362d",
   "metadata": {},
   "source": [
    "### Why the Rule-Based Resolver Fails Here\n",
    "\n",
    "Our resolver uses only **recency** and **proper-noun preference**. Since both \"Dr. Sarah Chen\" and \"Professor James Lee\" contain proper nouns (`NNP`), the tie-breaker is recency — and \"Dr. Sarah Chen\" appears first, so it keeps winning. The resolver has:\n",
    "- **No gender model** — it cannot distinguish *\"He\"* (male) from *\"She\"* (female)\n",
    "- **No number model** — it cannot distinguish *\"They\"* (plural) from *\"It\"* (singular)\n",
    "- **No semantic understanding** — it cannot link *\"the former president\"* to *\"Obama\"* based on meaning\n",
    "\n",
    "State-of-the-art systems (e.g., Lee et al. 2017) use neural networks with contextualised embeddings to handle these cases. Let's see how a **BERT-based** coreference model compares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c85b9e1",
   "metadata": {},
   "source": [
    "## Step 3 — Neural Coreference Resolution with BERT\n",
    "\n",
    "The **F-Coref** model (*Otmazgin et al., 2023*) is a coreference resolution system built on top of a **RoBERTa** encoder (a BERT variant). It was trained on the **OntoNotes 5.0** corpus — the standard benchmark for coreference resolution — and learns to identify which spans of text refer to the same real-world entity.\n",
    "\n",
    "Key advantages over our rule-based approach:\n",
    "- **Semantic understanding** — the model \"knows\" that *\"the former president\"* refers to *\"Barack Obama\"* because of meaning, not just proximity\n",
    "- **Gender and number agreement** — can distinguish *\"She\"* → *\"Mary\"* from *\"he\"* → *\"John\"* using learned representations\n",
    "- **Event coreference** — can link *\"It\"* to *\"The merger\"* when the pronoun refers to an event rather than a person\n",
    "\n",
    "We use the `fastcoref` library, which wraps this model in a simple API. The model outputs **coreference clusters** — groups of text spans that refer to the same entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6973db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, logging, time, torch\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "# --- Fix for transformers 5.x compatibility ---\n",
    "from fastcoref.modeling import FCorefModel\n",
    "if not hasattr(FCorefModel, 'all_tied_weights_keys'):\n",
    "    FCorefModel.all_tied_weights_keys = {}\n",
    "\n",
    "from fastcoref import FCoref\n",
    "\n",
    "# Load F-Coref model (automatically uses GPU if available)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Loading F-Coref model on {device}...\")\n",
    "coref_model = FCoref(device=device)\n",
    "print(\"Model loaded.\\n\")\n",
    "\n",
    "# ── 1. Same text as our rule-based approach ────────────────────────\n",
    "easy_text = (\"Barack Obama was born in Hawaii. \"\n",
    "             \"He graduated from Harvard Law School. \"\n",
    "             \"The former president later moved to Chicago.\")\n",
    "\n",
    "preds = coref_model.predict(texts=[easy_text])\n",
    "clusters = preds[0].get_clusters(as_strings=True)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Easy text (same as Steps 1-2)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Text: {easy_text}\\n\")\n",
    "print(\"Coreference clusters found by F-Coref:\")\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"  Cluster {i+1}: {cluster}\")\n",
    "\n",
    "# ── 2. Same HARD text from Step 2b (rule-based got 4/6 wrong!) ──\n",
    "hard_text = (\"Dr. Sarah Chen met Professor James Lee at the annual conference. \"\n",
    "             \"She presented her groundbreaking research on climate change. \"\n",
    "             \"He asked several challenging questions about her methodology. \"\n",
    "             \"Later, she invited him to collaborate on a joint paper.\")\n",
    "\n",
    "preds = coref_model.predict(texts=[hard_text])\n",
    "clusters = preds[0].get_clusters(as_strings=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Hard text (rule-based resolver failed on gender)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Text: {hard_text}\\n\")\n",
    "print(\"Coreference clusters found by F-Coref:\")\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(f\"  Cluster {i+1}: {cluster}\")\n",
    "print(\"\\nF-Coref correctly separates the two people using gender\"\n",
    "      \" — something our rule-based resolver could not do.\")\n",
    "\n",
    "# ── 3. Additional hard cases ──────────────────────────────────────\n",
    "more_texts = [\n",
    "    # Event coreference — \"It\" refers to an event, not a person\n",
    "    \"The company announced a merger with its rival. \"\n",
    "    \"It was expected to create 500 new jobs.\",\n",
    "\n",
    "    # Unambiguous gender in \"told X that she...\"\n",
    "    \"Alice told Bob that she had passed the exam.\",\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"More hard cases\")\n",
    "print(\"=\" * 60)\n",
    "for text in more_texts:\n",
    "    preds = coref_model.predict(texts=[text])\n",
    "    clusters = preds[0].get_clusters(as_strings=True)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    if clusters:\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            print(f\"  Cluster {i+1}: {cluster}\")\n",
    "    else:\n",
    "        print(\"  No coreference found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b51ba",
   "metadata": {},
   "source": [
    "### Rule-Based vs BERT: Key Observations\n",
    "\n",
    "| Feature | Rule-Based (Step 2) | BERT — F-Coref (Step 3) |\n",
    "|---------|-------------------|------------------------|\n",
    "| **Easy case** (Obama / He) | ✓ Correct | ✓ Correct |\n",
    "| **Gender matching** (Mary / She) | ✗ Picks wrong antecedent | ✓ Correct |\n",
    "| **Event coreference** (merger / It) | ✗ Cannot resolve | ✓ Correct |\n",
    "| **Speed** | Instant | ~0.3 s per text (GPU) |\n",
    "| **Training data required** | None | OntoNotes 5.0 (1.7M words) |\n",
    "| **Interpretability** | High — two explicit rules | Low — learned representations |\n",
    "| **Dependencies** | NLTK only | PyTorch + Transformers (362 MB model) |\n",
    "\n",
    "**Takeaway:** The BERT-based model handles the hard cases that broke our rule-based resolver — gender agreement, event coreference, and semantic similarity — but it trades **interpretability** and **simplicity** for **accuracy**. We can clearly explain *why* the rule-based system made each decision (recency + proper-noun preference), while the neural model is essentially a black box.\n",
    "\n",
    "This **interpretability vs. performance** trade-off is a recurring theme in NLP and information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbda9b78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e6ab385457dd19e68abdbf215720a8d",
     "grade": false,
     "grade_id": "cell-57",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 13. Negation Handling\n",
    "\n",
    "## Why Negation Matters\n",
    "\n",
    "Consider medical text mining:\n",
    "\n",
    "> *\"The patient does **not** have diabetes.\"*\n",
    "\n",
    "A keyword system would extract *\"patient — has — diabetes\"*, the **opposite** of the truth. Detecting negation and its **scope** is critical for:\n",
    "- **Sentiment analysis** — \"not good\" vs. \"good\"\n",
    "- **Medical NLP** — negated vs. confirmed symptoms\n",
    "- **Knowledge Graphs** — avoiding false-positive relations\n",
    "\n",
    "## Negation Cues and Scope\n",
    "\n",
    "| Component | Definition | Examples |\n",
    "|-----------|-----------|----------|\n",
    "| **Cue** | Word or morpheme that signals negation | *not, no, never, un-, -less* |\n",
    "| **Scope** | Part of the sentence affected by the cue | *have diabetes* |\n",
    "\n",
    "| Sentence | Cue | Scope |\n",
    "|----------|-----|-------|\n",
    "| The patient does **not** have diabetes | not | have diabetes |\n",
    "| There is **no** evidence of cancer | no | evidence of cancer |\n",
    "| The test was **un**successful | un- | successful |\n",
    "| He **never** returned to work | never | returned to work |\n",
    "\n",
    "## How Dependency Grammar Helps\n",
    "\n",
    "In a dependency tree, *\"not\"* attaches to the verb it modifies (`advmod` / `neg`). The **scope** of negation is approximated by the **subtree** rooted at that verb:\n",
    "\n",
    "- *\"John does **not** like chocolate\"*\n",
    "  - not → `advmod` → like → subtree = {like, chocolate}\n",
    "  - Scope = *\"like chocolate\"*\n",
    "\n",
    "Below we build a negation detector and scope analyser step by step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0651f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "488850ea08c31c346e8065f0df323f4d",
     "grade": false,
     "grade_id": "cell-58",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 1 — Detecting Negation Cues\n",
    "\n",
    "We scan POS-tagged sentences for two kinds of negation:\n",
    "1. **Word-level** — tokens like *not*, *no*, *never*\n",
    "2. **Prefix-level** — morphemes like *un-*, *in-*, *im-*, *dis-*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae469d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b9f629dbc96f9afc0e71322766a54c0e",
     "grade": false,
     "grade_id": "cell-59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# --- negation cue lexicon ---\n",
    "NEGATION_WORDS    = {'not', \"n't\", 'no', 'never', 'neither', 'nor',\n",
    "                     'nobody', 'nothing', 'nowhere', 'none'}\n",
    "NEGATION_PREFIXES = ['un', 'in', 'im', 'ir', 'il', 'non', 'dis']\n",
    "\n",
    "# Words that START with a negation prefix but are NOT negated\n",
    "FALSE_POSITIVES   = {'under', 'until', 'unless', 'into', 'increase',\n",
    "                     'include', 'indeed', 'indicate', 'individual',\n",
    "                     'industry', 'information', 'interest', 'inside',\n",
    "                     'important', 'improve', 'imagine', 'image',\n",
    "                     'discuss', 'discover', 'display', 'distance',\n",
    "                     'district', 'dinner', 'direct', 'different',\n",
    "                     'none', 'normal', 'note', 'notice', 'novel'}\n",
    "\n",
    "def find_negation_cues(sentence):\n",
    "    \"\"\"Return a list of (position, word, cue_type) tuples.\"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tagged = pos_tag(tokens)\n",
    "    cues = []\n",
    "\n",
    "    for i, (word, tag) in enumerate(tagged):\n",
    "        low = word.lower()\n",
    "\n",
    "        # 1) word-level negation\n",
    "        if low in NEGATION_WORDS:\n",
    "            cues.append((i, word, 'word'))\n",
    "            continue\n",
    "\n",
    "        # 2) prefix-level negation (only adjectives/adverbs/nouns)\n",
    "        if tag in ('JJ', 'RB', 'NN', 'VBN', 'VBD') and low not in FALSE_POSITIVES:\n",
    "            for pfx in NEGATION_PREFIXES:\n",
    "                if low.startswith(pfx) and len(low) > len(pfx) + 2:\n",
    "                    cues.append((i, word, f'prefix ({pfx}-)'))\n",
    "                    break\n",
    "\n",
    "    return tokens, tagged, cues\n",
    "\n",
    "# --- test sentences ---\n",
    "test_sentences = [\n",
    "    \"The patient does not have diabetes.\",\n",
    "    \"No significant abnormalities were detected.\",\n",
    "    \"He never returned to work.\",\n",
    "    \"The treatment was unsuccessful.\",\n",
    "    \"The result is not insignificant.\",   # double negation!\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    tokens, tagged, cues = find_negation_cues(sent)\n",
    "    print(f\"Sentence: {sent}\")\n",
    "    if cues:\n",
    "        for pos, word, ctype in cues:\n",
    "            print(f\"  → cue '{word}' at position {pos}  (type: {ctype})\")\n",
    "    else:\n",
    "        print(\"  → no negation detected\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8238f83",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "380d86348117761ec7fe732c3875aef1",
     "grade": false,
     "grade_id": "cell-60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The detector finds word-level cues (*not, no, never*) and prefix-level cues (*un-successful*, *in-significant*). Note the false-positive filter: words like *under* or *increase* start with negation prefixes but are not negated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2b9d2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "adeaaa1bcf9bc0e8b050043dec2a7613",
     "grade": false,
     "grade_id": "cell-61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Step 2 — Determining Negation Scope with Chunking\n",
    "\n",
    "We use **VP chunking** (Section 8) to determine what the negation applies to. The basic rule:\n",
    "\n",
    "> The scope of a negation cue = the **verb phrase and its complements** that follow the cue, up to the next clause boundary (punctuation or conjunction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49169e96",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d69e54d25dbb67a5ec55d3633873a282",
     "grade": false,
     "grade_id": "cell-62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def detect_negation_scope(sentence):\n",
    "    \"\"\"Detect negation cues and determine their scope.\"\"\"\n",
    "    tokens, tagged, cues = find_negation_cues(sentence)\n",
    "\n",
    "    results = []\n",
    "    for cue_pos, cue_word, cue_type in cues:\n",
    "        if 'prefix' in cue_type:\n",
    "            # Prefix negation: scope is just the word itself\n",
    "            results.append({\n",
    "                'cue': cue_word, 'type': cue_type,\n",
    "                'scope': cue_word, 'negated_meaning': f'not {cue_word}'\n",
    "            })\n",
    "        else:\n",
    "            # Word negation: scope extends to next clause boundary\n",
    "            # Clause boundaries: . ! ? , ; : and coordinating conjunctions\n",
    "            boundary_tags = {'.', ',', ':', 'CC'}  # CC = and, but, or\n",
    "            scope_end = len(tokens)\n",
    "            for j in range(cue_pos + 1, len(tokens)):\n",
    "                if (tagged[j][1] in boundary_tags or\n",
    "                    tokens[j] in '.!?;'):\n",
    "                    scope_end = j\n",
    "                    break\n",
    "            scope_tokens = tokens[cue_pos + 1 : scope_end]\n",
    "            results.append({\n",
    "                'cue': cue_word, 'type': cue_type,\n",
    "                'scope': ' '.join(scope_tokens),\n",
    "                'negated_meaning': f'NOT [{\" \".join(scope_tokens)}]'\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- test ---\n",
    "scope_tests = [\n",
    "    \"The patient does not have a history of heart disease.\",\n",
    "    \"No evidence of malignancy was found in the biopsy.\",\n",
    "    \"The CEO did not approve the merger but supported the partnership.\",\n",
    "    \"The treatment was unsuccessful.\",\n",
    "    \"The result is not insignificant.\",\n",
    "]\n",
    "\n",
    "for sent in scope_tests:\n",
    "    print(f\"Sentence:  {sent}\")\n",
    "    negs = detect_negation_scope(sent)\n",
    "    for n in negs:\n",
    "        print(f\"  Cue:     '{n['cue']}' ({n['type']})\")\n",
    "        print(f\"  Scope:   '{n['scope']}'\")\n",
    "        print(f\"  Meaning: {n['negated_meaning']}\")\n",
    "    if not negs:\n",
    "        print(\"  No negation.\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49424e3e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43c6753e0fc942251044ceeb9c1bfb51",
     "grade": false,
     "grade_id": "cell-63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### What the Scope Detector Does Well\n",
    "\n",
    "- *\"does not have a history of heart disease\"* --> scope = *\"have a history of heart disease\"* correct\n",
    "- *\"did not approve the merger **but** supported the partnership\"* --> scope stops at *\"but\"*, correctly excluding the positive second clause\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Double negation** (*\"not insignificant\"*) — the detector finds two separate cues but does not combine them into the positive meaning \"significant\"\n",
    "- **Long-range scope** — in *\"I do not think he is coming\"*, the scope arguably extends into the embedded clause, but our heuristic would stop at the clause boundary\n",
    "- These hard cases are why clinical NLP tools like **NegEx** and **NegBio** use carefully crafted rule sets over dependency trees, and modern systems use neural models trained on annotated negation corpora (SEM-2012 Shared Task).\n",
    "\n",
    "But have modern BERT-based models actually **solved** the negation problem? Let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdbdc92",
   "metadata": {},
   "source": [
    "## Step 3 — Can BERT Handle Negation?\n",
    "\n",
    "Back in 2019, several studies showed that BERT-era models struggled with negation — treating *\"this movie is not good\"* as positive because the word *\"good\"* dominated the representation. The SEM-2012 Shared Task specifically targeted this problem.\n",
    "\n",
    "But transformer models have improved significantly since then. Let's test a modern **DistilBERT sentiment classifier** (trained on SST-2) on a progression of increasingly difficult negation patterns:\n",
    "\n",
    "1. **Simple negation** — *\"not good\"*, *\"not bad\"*\n",
    "2. **Negated negatives** — *\"not bad\"*, *\"not terrible\"* (should flip to positive)\n",
    "3. **Double negation with prefixes** — *\"not unhappy\"*, *\"not insignificant\"* (should be positive)\n",
    "4. **Complex multi-clause negation** — *\"I would not say it was not enjoyable\"* (the hardest case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7497a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Step 3: Test BERT sentiment analysis on negation ──\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment = pipeline(\"sentiment-analysis\", device=device)\n",
    "\n",
    "# Group 1: Simple negation (solved since ~2020)\n",
    "simple_tests = [\n",
    "    (\"The movie is good\",           \"POSITIVE\"),\n",
    "    (\"The movie is not good\",       \"NEGATIVE\"),\n",
    "    (\"The movie is bad\",            \"NEGATIVE\"),\n",
    "    (\"The service was terrible\",    \"NEGATIVE\"),\n",
    "    (\"The service was not terrible\",\"POSITIVE\"),\n",
    "]\n",
    "\n",
    "# Group 2: Negated negatives / double negation with prefixes\n",
    "double_neg_tests = [\n",
    "    (\"The movie was not bad\",          \"POSITIVE\"),\n",
    "    (\"I am not unhappy with the result\",\"POSITIVE\"),\n",
    "    (\"The effect was not insignificant\",\"POSITIVE\"),\n",
    "    (\"The food was not unenjoyable\",   \"POSITIVE\"),\n",
    "]\n",
    "\n",
    "# Group 3: Complex multi-clause negation (still hard!)\n",
    "hard_tests = [\n",
    "    (\"I would not say the movie was not enjoyable\",  \"POSITIVE\"),  # double neg → positive\n",
    "    (\"I don't think this isn't a bad idea\",          \"NEGATIVE\"),  # triple neg → negative\n",
    "]\n",
    "\n",
    "all_groups = [\n",
    "    (\"SIMPLE NEGATION\", simple_tests),\n",
    "    (\"DOUBLE NEGATION (negated negatives)\", double_neg_tests),\n",
    "    (\"COMPLEX MULTI-CLAUSE NEGATION\", hard_tests),\n",
    "]\n",
    "\n",
    "print(\"=\" * 85)\n",
    "print(\"BERT Sentiment Analysis on Negation Patterns\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "total_correct = 0\n",
    "total_count = 0\n",
    "\n",
    "for group_name, tests in all_groups:\n",
    "    print(f\"\\n--- {group_name} ---\")\n",
    "    for text, expected in tests:\n",
    "        result = sentiment(text)[0]\n",
    "        predicted = result[\"label\"]\n",
    "        score = result[\"score\"]\n",
    "        correct = predicted == expected\n",
    "        total_count += 1\n",
    "        if correct:\n",
    "            total_correct += 1\n",
    "        status = \"OK\" if correct else \"WRONG\"\n",
    "        print(f\"  [{status:5s}] \\\"{text}\\\"\")\n",
    "        print(f\"          Expected: {expected:8s}  |  Got: {predicted:8s} ({score:.3f})\")\n",
    "\n",
    "print(f\"\\n{'=' * 85}\")\n",
    "print(f\"Overall accuracy: {total_correct}/{total_count} ({100*total_correct/total_count:.0f}%)\")\n",
    "print(f\"{'=' * 85}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6fdc0",
   "metadata": {},
   "source": [
    "### What We Learned\n",
    "\n",
    "| Negation Type | Rule-Based Detector | BERT Sentiment |\n",
    "|---|---|---|\n",
    "| Simple negation (*\"not good\"*) | Finds cue + scope, but no sentiment | **Correct** — handles simple negation well |\n",
    "| Negated negatives (*\"not bad\"*) | Finds \"not\" but misses the flip | **Correct** — understands \"not bad\" = positive |\n",
    "| Prefix double negation (*\"not unhappy\"*) | Detects two separate cues | **Correct** — resolves to positive meaning |\n",
    "| Complex multi-clause (*\"would not say...not enjoyable\"*) | Cannot parse nested structure | **Fails** — still trips on deeply nested negation |\n",
    "\n",
    "**Key takeaway:** Modern transformer models have largely **solved** the basic negation problems that plagued NLP systems circa 2019. Simple negation and even double negation with prefixes are handled correctly. However, complex multi-clause negation with nested scopes (e.g., *\"I would not say the movie was not enjoyable\"*) remains a challenge — these constructions are rare in training data and require deep syntactic reasoning that even modern models struggle with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f237080",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "967006829795d1c9eb769d442cd3b001",
     "grade": false,
     "grade_id": "cell-64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 14. Exercises\n",
    "\n",
    "The exercises below test your understanding of co-reference resolution and negation handling. For the programming exercise, look back at the code examples in Sections 12 and 13, as well as the POS tagging (Section 6), chunking (Section 8), and dependency parsing (Section 11) code earlier in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b26e25",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b66593b2a2e3a636b340216206a7956d",
     "grade": false,
     "grade_id": "exercise1_prompt",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Exercise 1 — Co-reference Resolution (Open Question)\n",
    "\n",
    "Consider the following text:\n",
    "\n",
    "> *\"Mary told Susan that **she** had been promoted. **She** was very happy about **it**.\"*\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "1. The pronoun *\"she\"* appears twice. For each occurrence, who does it most likely refer to — Mary or Susan? Explain your reasoning using the concepts of **subject preference** and **recency**.\n",
    "\n",
    "2. Our simple `resolve_pronouns()` function above would resolve both occurrences of *\"she\"* to the same antecedent. Explain **why** it fails and what additional rule or information you would need to handle this correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083e8898",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8eae188a5a27f083a2242ab7ae26dcd5",
     "grade": true,
     "grade_id": "exercise1_answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9257c310",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a39c69f9721db54e4620642f6874639",
     "grade": false,
     "grade_id": "exercise2_prompt",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "## Exercise 2 — Negation Scope (Open Question)\n",
    "\n",
    "Consider the following two sentences:\n",
    "\n",
    "> A: *\"The patient was **not** treated with antibiotics and recovered quickly.\"*\n",
    ">\n",
    "> B: *\"The patient was **not** treated with antibiotics and **not** given any medication.\"*\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "1. In sentence A, does the negation apply only to *\"treated with antibiotics\"*, or does it also negate *\"recovered quickly\"*? How does the coordinating conjunction *\"and\"* create ambiguity here?\n",
    "\n",
    "2. In sentence B, the scope is clearer. Explain why the second *\"not\"* makes the intended meaning unambiguous.\n",
    "\n",
    "3. Our `detect_negation_scope()` function uses clause boundaries (commas, conjunctions) to limit scope. Run sentence A through the function mentally (or in code). Does it produce the correct scope? Why or why not?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84f8734",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44adf833a573d6728476ea66bd0ccb68",
     "grade": true,
     "grade_id": "exercise2_answer",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5310055d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf3380b84d782c4d122490fc23216570",
     "grade": false,
     "grade_id": "exercise3_prompt",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Exercise 3 — Improved Negation-Aware Information Extractor (Programming)\n",
    "\n",
    "Build a function `extract_medical_facts(sentence)` that:\n",
    "\n",
    "1. **POS-tags** the sentence (Section 6)\n",
    "2. **Detects NP chunks** as subjects and objects (Section 8 — use `RegexpParser`)\n",
    "3. **Detects negation cues** using the `find_negation_cues()` function from Section 13\n",
    "4. Returns a list of **(subject, verb, object, is_negated)** tuples\n",
    "\n",
    "**Test sentences** (expected output shown):\n",
    "\n",
    "| Sentence | Expected output |\n",
    "|----------|-----------------|\n",
    "| *\"The patient has diabetes.\"* | `('The patient', 'has', 'diabetes', False)` |\n",
    "| *\"The patient does not have diabetes.\"* | `('The patient', 'have', 'diabetes', True)` |\n",
    "| *\"No evidence of cancer was found.\"* | `('evidence of cancer', 'found', '', True)` |\n",
    "\n",
    "**Hints:**\n",
    "- Re-use `RegexpParser` with the NP grammar `{<DT>?<JJ>*<NN.*>+}` from Section 8\n",
    "- Re-use `find_negation_cues()` from Section 13 to check if a verb falls within a negation scope\n",
    "- You can find the main verb by looking for POS tags starting with `VB`\n",
    "- Don't worry about getting every edge case right — focus on the three test sentences above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71d66e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb8cbe021e071b340f3b77c0efcfd62b",
     "grade": false,
     "grade_id": "exercise3_solution",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e87726e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54da676de13cd86655e1fdf2d94f37c8",
     "grade": true,
     "grade_id": "exercise3_tests",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# AUTO-GRADED TESTS (10 points)\n",
    "# Do not modify this cell\n",
    "\n",
    "# Test 1: function exists and is callable\n",
    "assert callable(extract_medical_facts), \"extract_medical_facts() must be defined as a function\"\n",
    "\n",
    "# Test 2: positive assertion\n",
    "_r1 = extract_medical_facts(\"The patient has diabetes.\")\n",
    "assert _r1 is not None, \"Function must return a result (not None)\"\n",
    "assert isinstance(_r1, (list, tuple)), \"Function must return a list or tuple\"\n",
    "if len(_r1) > 0:\n",
    "    _item = _r1[0] if isinstance(_r1, list) else _r1\n",
    "    assert len(_item) == 4, \"Each result must be a 4-tuple: (subject, verb, object, is_negated)\"\n",
    "    assert _item[3] == False, \"'The patient has diabetes.' should NOT be negated\"\n",
    "\n",
    "# Test 3: negation detection\n",
    "_r2 = extract_medical_facts(\"The patient does not have diabetes.\")\n",
    "assert _r2 is not None, \"Function must return a result for negated sentence\"\n",
    "if len(_r2) > 0:\n",
    "    _item2 = _r2[0] if isinstance(_r2, list) else _r2\n",
    "    assert _item2[3] == True, \"'The patient does not have diabetes.' SHOULD be negated\"\n",
    "\n",
    "# Test 4: another negation pattern\n",
    "_r3 = extract_medical_facts(\"No evidence of cancer was found.\")\n",
    "assert _r3 is not None, \"Function must return a result for 'No' negation\"\n",
    "if len(_r3) > 0:\n",
    "    _item3 = _r3[0] if isinstance(_r3, list) else _r3\n",
    "    assert _item3[3] == True, \"'No evidence of cancer was found.' SHOULD be negated\"\n",
    "\n",
    "print(\"All auto-graded tests passed! ✓\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e11bc50",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9baabae9e1f202b21d3c7f8a33eaf69b",
     "grade": false,
     "grade_id": "cell-71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Submission\n",
    "\n",
    "To submit your work:\n",
    "\n",
    "1. Make sure all cells have been executed and the outputs are visible\n",
    "2. Fill in your answers for **Exercise 1** (co-reference) and **Exercise 2** (negation) in the designated cells\n",
    "3. Complete the code for **Exercise 3** and verify it runs on the test sentences\n",
    "4. Save your notebook: **File → Save** (or Ctrl+S)\n",
    "5. Submit the `.ipynb` file via the course submission system\n",
    "\n",
    "**Reminder**: Part 2 of this tutorial covers Named Entity Recognition, Relation Extraction, and Knowledge Graph construction — building on the foundations from this notebook.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
