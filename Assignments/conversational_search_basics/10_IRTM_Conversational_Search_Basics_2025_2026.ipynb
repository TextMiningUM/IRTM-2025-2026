{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230611e8",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f301d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c63ecf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8179d822",
   "metadata": {},
   "source": [
    "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
    "\n",
    "# Information Retrieval and Text Mining Course\n",
    "## Tutorial 10 — Conversational Search: The Basics\n",
    "\n",
    "**Author:** Jan Scholtes\n",
    "\n",
    "**Edition 2025-2026**\n",
    "\n",
    "Department of Advanced Computer Sciences — Maastricht University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c35c83",
   "metadata": {},
   "source": [
    "Welcome to Tutorial 10 on **Conversational Search: The Basics**. This is the first of three tutorials on Conversational Search:\n",
    "\n",
    "1. **The Basics** (this tutorial) — dialogue structure, query understanding, hybrid search, re-ranking, evaluation metrics\n",
    "2. **RAG — Retrieval-Augmented Generation** (Tutorial 11) — fusion methods, hallucination detection\n",
    "3. **Agentic Approaches** (Tutorial 12) — agents, memory, tools, multi-agent orchestration\n",
    "\n",
    "In this tutorial we explore how conversational AI transforms traditional search from keyword matching into intent-driven, multi-turn dialogue systems. The topics covered are:\n",
    "\n",
    "1. **Properties of Human Conversations** — turns, grounding, adjacency pairs, mixed initiative, sub-dialogues.\n",
    "2. **From Keywords to Conversations** — the evolution from keyword-based → intent-based → conversational search.\n",
    "3. **Query Understanding & Reformulation** — NER, query expansion, coreference resolution, context tracking.\n",
    "4. **BM25: The Classic Retrieval Model** — term frequency, inverse document frequency, document-length normalization.\n",
    "5. **Dense Retrieval with Sentence Transformers** — encoding queries and documents as embeddings.\n",
    "6. **Hybrid Search: BM25 + Dense Retrieval** — combining lexical and semantic search with score fusion.\n",
    "7. **Neural Re-Ranking** — using cross-encoders to re-score retrieved documents.\n",
    "8. **Evaluation Metrics for Conversational AI** — semantic similarity, answer relevancy, context relevancy, faithfulness, RAGAS.\n",
    "9. **Commercial Systems** — how Bing, Google, and Perplexity implement conversational search.\n",
    "\n",
    "At the end you will find the **Exercises** section with graded assignments.\n",
    "\n",
    "> **Note:** This course is about Information Retrieval, Text Mining, and Conversational Search — not about programming skills. The code cells below show you *how* these methods work in practice using Python libraries. Focus on understanding the **concepts** and **results**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b380b5da",
   "metadata": {},
   "source": [
    "## Library Installation\n",
    "\n",
    "We install all required packages in a single cell. Run this cell once at the beginning of your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a32c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    \"sentence-transformers\",\n",
    "    \"rank_bm25\",\n",
    "    \"spacy\",\n",
    "    \"faiss-cpu\",\n",
    "    \"scikit-learn\",\n",
    "]\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "# Download spaCy English model (if not already installed)\n",
    "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\", \"-q\"])\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50af5aa",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "All imports are grouped here so the notebook is easy to set up and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34231629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "\n",
    "# BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Sentence Transformers (dense retrieval)\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# FAISS for vector search\n",
    "import faiss\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223fc918",
   "metadata": {},
   "source": [
    "## Sample Corpus\n",
    "\n",
    "We create a small corpus of text passages about Information Retrieval topics. This corpus will be used throughout the tutorial to demonstrate search techniques. In a real system, these would be passages retrieved from a large document collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4aadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus: short passages about IR topics\n",
    "corpus = [\n",
    "    \"BM25 is a bag-of-words retrieval function that ranks documents based on term frequency and inverse document frequency. It is widely used in search engines like Elasticsearch and Apache Solr.\",\n",
    "    \"Dense retrieval uses neural network embeddings to encode queries and documents into a shared vector space. Models like DPR and ColBERT achieve state-of-the-art performance on passage retrieval benchmarks.\",\n",
    "    \"TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents.\",\n",
    "    \"PageRank is an algorithm used by Google Search to rank web pages. It works by counting the number and quality of links to a page to determine a rough estimate of the website's importance.\",\n",
    "    \"BERT (Bidirectional Encoder Representations from Transformers) is a language model developed by Google. It uses masked language modeling and next sentence prediction for pre-training.\",\n",
    "    \"Retrieval-Augmented Generation (RAG) combines a retrieval system with a language model. The retriever finds relevant documents, and the generator produces an answer grounded in those documents.\",\n",
    "    \"Conversational search systems maintain context across multiple turns of dialogue. They use coreference resolution and query reformulation to understand follow-up questions.\",\n",
    "    \"The RAGAS framework evaluates RAG pipelines using metrics like faithfulness, answer relevancy, and context relevancy. It helps detect hallucinations in generated answers.\",\n",
    "    \"ColBERT uses late interaction between query and document token embeddings for efficient retrieval. Each query token attends to all document tokens via MaxSim operations.\",\n",
    "    \"Query expansion improves recall by adding synonyms or related terms to the original query. Techniques include using word embeddings (Word2Vec, GloVe) and knowledge graphs.\",\n",
    "    \"Hybrid search combines BM25 keyword matching with dense vector retrieval. Score fusion merges both ranking signals for improved precision and recall.\",\n",
    "    \"Cross-encoder re-ranking models take a query-document pair as input and output a relevance score. They are more accurate than bi-encoders but too slow for first-stage retrieval.\",\n",
    "    \"Grounding in conversational AI means establishing common understanding between the user and the system. The system confirms what it understood before proceeding.\",\n",
    "    \"Mixed-initiative dialogue allows both the user and the system to take the lead in a conversation. The system can ask clarifying questions when the query is ambiguous.\",\n",
    "    \"Perplexity AI is a conversational search engine that combines real-time web retrieval with LLM-based answer synthesis. Every claim is linked to a verifiable source.\",\n",
    "]\n",
    "\n",
    "# Labels for reference\n",
    "corpus_labels = [\n",
    "    \"BM25\", \"Dense Retrieval\", \"TF-IDF\", \"PageRank\", \"BERT\",\n",
    "    \"RAG\", \"Conversational Search\", \"RAGAS\", \"ColBERT\", \"Query Expansion\",\n",
    "    \"Hybrid Search\", \"Cross-Encoder Re-ranking\", \"Grounding\", \"Mixed Initiative\", \"Perplexity AI\",\n",
    "]\n",
    "\n",
    "print(f\"Corpus loaded: {len(corpus)} passages\")\n",
    "for i, (label, text) in enumerate(zip(corpus_labels, corpus)):\n",
    "    print(f\"  [{i:2d}] {label}: {text[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90c8ab",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Properties of Human Conversations\n",
    "\n",
    "Before building conversational search systems, we must understand what makes human conversations work. Natural dialogue is remarkably complex:\n",
    "\n",
    "## Key Properties\n",
    "\n",
    "| Property | Description | Example |\n",
    "|---|---|---|\n",
    "| **Turns** | Each contribution is a \"turn\"; participants alternate | User asks → System answers → User follows up |\n",
    "| **Grounding** | Establishing common understanding; confirming what was heard | System: \"Okay, searching for flights to Paris...\" |\n",
    "| **Adjacency Pairs** | Local structure between speech acts (Sacks et al., 1974) | Question → Answer, Proposal → Accept/Reject |\n",
    "| **Sub-dialogues** | Embedded corrections or clarifications within the main dialogue | User: \"No, I meant *New York*, not *New Haven*\" |\n",
    "| **Mixed Initiative** | Both parties can lead the conversation (Walker & Whittaker, 1990) | System: \"Did you mean the city or the state?\" |\n",
    "| **Knowledge Inference** | Inferring implicit constraints from context | \"meeting on Monday\" → implies travel on Sunday |\n",
    "\n",
    "## Why This Matters for Search\n",
    "\n",
    "Traditional search is **single-turn**: the user types a query, gets results, end of interaction. Conversational search must handle:\n",
    "\n",
    "- **Multi-turn context** — remembering what was discussed earlier\n",
    "- **Ambiguity resolution** — asking clarifying questions\n",
    "- **Coreference** — understanding that \"it\", \"they\", \"those\" refer to earlier entities\n",
    "- **Conversational memory** — maintaining both short-term and long-term context\n",
    "\n",
    "## The Grounding Problem\n",
    "\n",
    "Grounding (Clark, 1996) is the **Principle of Closure**: agents need evidence they have succeeded in performing an action. Good conversational systems always acknowledge what they understood:\n",
    "\n",
    "| | System Response |\n",
    "|---|---|\n",
    "| **Bad** ❌ | \"Here are your results.\" (no acknowledgment) |\n",
    "| **Good** ✅ | \"Okay, searching for Italian restaurants in New York near you...\" |\n",
    "\n",
    "Let's demonstrate some of these properties with NLP tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: NER and query understanding with spaCy\n",
    "\n",
    "# Simulated multi-turn conversation\n",
    "turns = [\n",
    "    \"Find me Italian restaurants in New York.\",\n",
    "    \"Which ones have outdoor seating?\",       # Coreference: \"ones\" → Italian restaurants in NY\n",
    "    \"What about their prices?\",               # Coreference: \"their\" → the ones with outdoor seating\n",
    "    \"Book the cheapest one for Friday.\",       # Coreference: \"one\" → cheapest restaurant\n",
    "]\n",
    "\n",
    "print(\"=== Multi-Turn Conversation Analysis ===\\n\")\n",
    "for i, turn in enumerate(turns, 1):\n",
    "    doc = nlp(turn)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    # Detect potential coreference markers\n",
    "    pronouns = [token.text for token in doc if token.pos_ == \"PRON\" or token.text.lower() in (\"ones\", \"one\")]\n",
    "    \n",
    "    print(f\"Turn {i}: \\\"{turn}\\\"\")\n",
    "    print(f\"  Entities: {entities if entities else 'None detected'}\")\n",
    "    print(f\"  Pronouns/References: {pronouns if pronouns else 'None'}\")\n",
    "    if pronouns:\n",
    "        print(f\"  ⚠ Coreference resolution needed — '{', '.join(pronouns)}' refers to context from earlier turns\")\n",
    "    print()\n",
    "\n",
    "print(\"--- Key Insight ---\")\n",
    "print(\"Turns 2-4 contain references ('ones', 'their', 'one') that only make sense\")\n",
    "print(\"when resolved against the context from Turn 1. A single-turn search engine\")\n",
    "print(\"would fail completely on these follow-up queries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cbca9e",
   "metadata": {},
   "source": [
    "**Observation:** Turns 2–4 contain pronouns and references that require **coreference resolution** — linking \"ones\", \"their\", and \"one\" back to the Italian restaurants from Turn 1. Without this, a search engine would not understand the follow-up queries.\n",
    "\n",
    "This demonstrates why conversational search needs more than just keyword matching: it needs **dialogue management**, **context tracking**, and **grounding**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dd09b8",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. From Keyword-Based to Conversational Search\n",
    "\n",
    "The evolution of search can be understood as three stages:\n",
    "\n",
    "## Stage 1: Keyword-Based Search (Traditional)\n",
    "\n",
    "- **Mechanism:** Matching exact words/phrases using TF-IDF, BM25, inverted index\n",
    "- **Ranking:** Term frequency, document frequency, PageRank\n",
    "- **Limitation:** No understanding of intent — \"apple\" could mean the fruit or the company\n",
    "- **Challenges:** Polysemy, synonyms, no context retention\n",
    "\n",
    "## Stage 2: Intent-Based Search\n",
    "\n",
    "- **Mechanism:** Identifying the *underlying intent* behind a query\n",
    "- **Technologies:** NER, dependency parsing, query expansion (Word2Vec, GloVe, BERT), knowledge graphs\n",
    "- **Advantage:** Better synonym/paraphrase handling, semantic understanding\n",
    "- **Example:** Google's BERT update (2019) — understanding queries like \"can you get medicine for someone pharmacy\" (the word \"for\" changes meaning)\n",
    "\n",
    "## Stage 3: Conversational Search\n",
    "\n",
    "- **Mechanism:** Multi-turn dialogue with natural query refinement\n",
    "- **Technologies:** LLM integration, RAG, conversational memory, coreference resolution\n",
    "- **Key difference:** Synthesizes answers instead of returning links\n",
    "- **Challenges:** Hallucination, bias, scalability, privacy\n",
    "\n",
    "| Feature | Keyword Search | Intent Search | Conversational Search |\n",
    "|---|---|---|---|\n",
    "| **Query format** | Keywords, Boolean | Natural language | Multi-turn dialogue |\n",
    "| **Understanding** | Lexical matching | Semantic intent | Full context + history |\n",
    "| **Response** | Ranked links | Relevant results + snippets | Synthesized answer |\n",
    "| **Memory** | None | Session-limited | Multi-turn + long-term |\n",
    "| **Proactivity** | None | Limited suggestions | Clarifying questions |\n",
    "\n",
    "Let's now implement each search strategy to see the differences in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ea59e",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Query Understanding & Reformulation\n",
    "\n",
    "Before a search engine retrieves documents, it must **understand** the query. In conversational search, this involves several steps:\n",
    "\n",
    "## A. NER & POS Tagging\n",
    "Identify entities (people, places, organizations) and parts of speech to understand *what* the user is asking about.\n",
    "\n",
    "## B. Query Clarification\n",
    "Detect ambiguous queries and ask follow-up questions to clarify intent.\n",
    "\n",
    "## C. Query Expansion\n",
    "Improve recall by adding semantically related terms:\n",
    "- **Word Embeddings** (Word2Vec, GloVe, FastText) — find similar words in vector space\n",
    "- **Thesauri** — lexical expansion with synonyms\n",
    "- **Knowledge Graphs** (DBpedia, Google Knowledge Graph) — e.g., \"heart attack\" → \"myocardial infarction\"\n",
    "- **Neural Reformulation** — dense retrieval models (DPR, ColBERT)\n",
    "\n",
    "## D. Context Tracking in Multi-Turn Conversations\n",
    "- **Coreference resolution** — \"Italian ones\" → \"Italian restaurants in New York\"\n",
    "- **Session-based search** — maintaining state across turns\n",
    "- **Conversational memory** — short-term (context window) + long-term (preference vectors in FAISS)\n",
    "\n",
    "Let's demonstrate query expansion using sentence embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Expansion using Sentence Embeddings\n",
    "# We find terms semantically related to the query using cosine similarity\n",
    "\n",
    "# Load a sentence transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Original query\n",
    "query = \"heart attack symptoms\"\n",
    "\n",
    "# Candidate expansion terms (in practice, these would come from a thesaurus or knowledge graph)\n",
    "expansion_candidates = [\n",
    "    \"myocardial infarction\",      # Medical synonym\n",
    "    \"chest pain\",                  # Symptom\n",
    "    \"shortness of breath\",         # Symptom\n",
    "    \"cardiovascular disease\",      # Related broader term\n",
    "    \"aspirin treatment\",           # Related treatment\n",
    "    \"broken bone\",                 # Unrelated\n",
    "    \"software engineering\",        # Completely unrelated\n",
    "    \"cardiac arrest\",              # Related but different\n",
    "    \"stroke symptoms\",             # Related condition\n",
    "    \"high blood pressure\",         # Risk factor\n",
    "]\n",
    "\n",
    "# Encode query and candidates\n",
    "query_emb = model.encode([query])\n",
    "candidate_embs = model.encode(expansion_candidates)\n",
    "\n",
    "# Compute cosine similarities\n",
    "similarities = cosine_similarity(query_emb, candidate_embs)[0]\n",
    "\n",
    "# Rank by similarity\n",
    "ranked = sorted(zip(expansion_candidates, similarities), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "print(f\"{'Expansion Term':<30} {'Similarity':>10}\")\n",
    "print(\"-\" * 42)\n",
    "for term, sim in ranked:\n",
    "    marker = \" ✓ expand\" if sim > 0.4 else \"\"\n",
    "    print(f\"{term:<30} {sim:>10.4f}{marker}\")\n",
    "\n",
    "print(f\"\\n--- Key Insight ---\")\n",
    "print(f\"Terms like 'myocardial infarction' and 'chest pain' score high because\")\n",
    "print(f\"they are semantically related. 'software engineering' scores low.\")\n",
    "print(f\"A threshold (e.g., > 0.4) selects useful expansion terms.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1261b3",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. BM25: The Classic Retrieval Model\n",
    "\n",
    "**BM25** (Best Matching 25) is the most widely used retrieval function in production search engines (Elasticsearch, Apache Solr). It ranks documents based on:\n",
    "\n",
    "$$\\text{BM25}(D, Q) = \\sum_{i=1}^{n} \\text{IDF}(q_i) \\cdot \\frac{f(q_i, D) \\cdot (k_1 + 1)}{f(q_i, D) + k_1 \\cdot \\left(1 - b + b \\cdot \\frac{|D|}{\\text{avgdl}}\\right)}$$\n",
    "\n",
    "Where:\n",
    "- $f(q_i, D)$ = frequency of term $q_i$ in document $D$\n",
    "- $|D|$ = document length\n",
    "- $\\text{avgdl}$ = average document length in the collection\n",
    "- $k_1$ = term frequency saturation parameter (typically 1.2–2.0)\n",
    "- $b$ = document length normalization parameter (typically 0.75)\n",
    "- $\\text{IDF}(q_i) = \\log\\frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}$ where $N$ = total documents, $n(q_i)$ = documents containing $q_i$\n",
    "\n",
    "### Strengths\n",
    "- Fast and efficient (inverted index lookup)\n",
    "- Well-understood and reliable\n",
    "- No training required\n",
    "\n",
    "### Weaknesses\n",
    "- Pure lexical matching — can't handle synonyms\n",
    "- Bag-of-words — ignores word order and semantics\n",
    "- \"apple fruit\" won't match \"pear banana orange\"\n",
    "\n",
    "Let's implement BM25 search on our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cc2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 Search\n",
    "# Tokenize corpus (simple whitespace + lowercase)\n",
    "tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "\n",
    "# Build BM25 index\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# Search queries\n",
    "queries = [\n",
    "    \"How does BM25 ranking work?\",\n",
    "    \"neural network embeddings for search\",\n",
    "    \"what is retrieval augmented generation\",\n",
    "    \"conversational dialogue systems\",        # Tests synonym handling\n",
    "]\n",
    "\n",
    "print(\"=== BM25 Search Results ===\\n\")\n",
    "for query in queries:\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Get top 3\n",
    "    top_indices = np.argsort(scores)[::-1][:3]\n",
    "    \n",
    "    print(f\"Query: \\\"{query}\\\"\")\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        print(f\"  #{rank} [{corpus_labels[idx]}] score={scores[idx]:.3f}\")\n",
    "        print(f\"       {corpus[idx][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073604f1",
   "metadata": {},
   "source": [
    "**Observation:** BM25 works well when query terms directly match document terms (e.g., \"BM25\" or \"retrieval augmented generation\"). But notice that \"conversational dialogue systems\" may not find the best matches because BM25 relies on exact word overlap — it can't understand that \"dialogue\" and \"conversation\" are related."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb3896f",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Dense Retrieval with Sentence Transformers\n",
    "\n",
    "**Dense retrieval** encodes queries and documents into a shared vector space using neural networks. Instead of matching keywords, it measures **semantic similarity** between embeddings.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Encode** all documents into dense vectors using a pre-trained model (e.g., all-MiniLM-L6-v2)\n",
    "2. **Encode** the query into the same vector space\n",
    "3. **Compute** cosine similarity between the query vector and all document vectors\n",
    "4. **Rank** by similarity score\n",
    "\n",
    "### Models\n",
    "\n",
    "| Model | Type | Speed | Accuracy |\n",
    "|---|---|---|---|\n",
    "| **DPR** (Dense Passage Retrieval) | Bi-encoder | Fast | Good |\n",
    "| **ANCE** | Bi-encoder with hard negatives | Fast | Better |\n",
    "| **ColBERT** | Late interaction (token-level) | Medium | Very good |\n",
    "| **Cross-Encoder** | Full attention on pair | Slow | Best |\n",
    "\n",
    "### Bi-Encoder vs Cross-Encoder\n",
    "\n",
    "- **Bi-encoder:** Encodes query and document *separately* → fast (can pre-compute doc embeddings), but less accurate\n",
    "- **Cross-encoder:** Encodes query-document *pair together* → more accurate, but too slow for first-stage retrieval (used for re-ranking)\n",
    "\n",
    "Let's compare dense retrieval to BM25 on the same queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07f427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Retrieval with Sentence Transformers (Bi-Encoder)\n",
    "\n",
    "# Encode all corpus documents (typically done offline)\n",
    "corpus_embeddings = model.encode(corpus, convert_to_numpy=True, show_progress_bar=False)\n",
    "\n",
    "print(f\"Corpus encoded: {corpus_embeddings.shape}  (documents × embedding dimensions)\")\n",
    "\n",
    "# Search same queries as BM25 for comparison\n",
    "print(\"\\n=== Dense Retrieval Results ===\\n\")\n",
    "for query in queries:\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
    "    \n",
    "    # Get top 3\n",
    "    top_indices = np.argsort(similarities)[::-1][:3]\n",
    "    \n",
    "    print(f\"Query: \\\"{query}\\\"\")\n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        print(f\"  #{rank} [{corpus_labels[idx]}] score={similarities[idx]:.4f}\")\n",
    "        print(f\"       {corpus[idx][:100]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94faaa50",
   "metadata": {},
   "source": [
    "**Observation:** Dense retrieval handles semantically related queries much better than BM25. For example, \"conversational dialogue systems\" now correctly retrieves passages about conversational search, even when the exact words don't match. The model understands that \"dialogue\" and \"conversation\" are semantically related.\n",
    "\n",
    "However, dense retrieval can sometimes miss results where specific technical terms matter (where BM25 excels). This is why **hybrid search** combines both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2ea978",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Hybrid Search: BM25 + Dense Retrieval\n",
    "\n",
    "**Hybrid search** combines the strengths of both approaches:\n",
    "\n",
    "| Approach | Strength | Weakness |\n",
    "|---|---|---|\n",
    "| **BM25** | Exact keyword matching, fast | Misses synonyms and paraphrases |\n",
    "| **Dense Retrieval** | Semantic understanding | May miss exact technical terms |\n",
    "| **Hybrid** | Best of both | Slightly more complex |\n",
    "\n",
    "### Score Fusion\n",
    "\n",
    "The key challenge is combining scores from two different systems. Common approaches:\n",
    "\n",
    "1. **Linear combination:** $\\text{score}_{\\text{hybrid}} = \\alpha \\cdot \\text{score}_{\\text{BM25}} + (1 - \\alpha) \\cdot \\text{score}_{\\text{dense}}$\n",
    "2. **Reciprocal Rank Fusion (RRF):** $\\text{RRF}(d) = \\sum_{r \\in R} \\frac{1}{k + \\text{rank}_r(d)}$ where $k$ is a constant (typically 60)\n",
    "\n",
    "Both scores need to be **normalized** to the same scale before fusion.\n",
    "\n",
    "This is the approach used by modern search systems like **Bing**, **Google AI Overview**, and **Perplexity AI**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid Search: BM25 + Dense Retrieval with Score Fusion\n",
    "\n",
    "def hybrid_search(query, corpus, bm25, model, corpus_embeddings, alpha=0.5, top_k=5):\n",
    "    \"\"\"\n",
    "    Combine BM25 and dense retrieval scores using linear interpolation.\n",
    "    \n",
    "    Parameters:\n",
    "        alpha: weight for BM25 (1-alpha for dense). 0.5 = equal weight.\n",
    "    \"\"\"\n",
    "    # BM25 scores\n",
    "    tokenized_query = query.lower().split()\n",
    "    bm25_scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Dense retrieval scores (cosine similarity)\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    dense_scores = cosine_similarity(query_embedding, corpus_embeddings)[0]\n",
    "    \n",
    "    # Normalize both to [0, 1] range\n",
    "    scaler = MinMaxScaler()\n",
    "    bm25_norm = scaler.fit_transform(bm25_scores.reshape(-1, 1)).flatten()\n",
    "    dense_norm = scaler.fit_transform(dense_scores.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Linear fusion\n",
    "    hybrid_scores = alpha * bm25_norm + (1 - alpha) * dense_norm\n",
    "    \n",
    "    # Get top-k\n",
    "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "    \n",
    "    return [(idx, hybrid_scores[idx], bm25_norm[idx], dense_norm[idx]) for idx in top_indices]\n",
    "\n",
    "# Compare all three approaches\n",
    "test_queries = [\n",
    "    \"How does BM25 ranking work?\",\n",
    "    \"neural embeddings for document search\",\n",
    "    \"conversational dialogue with context\",\n",
    "    \"what is ColBERT late interaction\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "    \n",
    "    # BM25 only\n",
    "    bm25_scores = bm25.get_scores(query.lower().split())\n",
    "    bm25_top = np.argsort(bm25_scores)[::-1][0]\n",
    "    \n",
    "    # Dense only\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)\n",
    "    dense_scores = cosine_similarity(q_emb, corpus_embeddings)[0]\n",
    "    dense_top = np.argsort(dense_scores)[::-1][0]\n",
    "    \n",
    "    # Hybrid\n",
    "    hybrid_results = hybrid_search(query, corpus, bm25, model, corpus_embeddings, alpha=0.5)\n",
    "    hybrid_top = hybrid_results[0][0]\n",
    "    \n",
    "    print(f\"  BM25 top-1:   [{corpus_labels[bm25_top]}]\")\n",
    "    print(f\"  Dense top-1:  [{corpus_labels[dense_top]}]\")\n",
    "    print(f\"  Hybrid top-1: [{corpus_labels[hybrid_top]}]\")\n",
    "    \n",
    "    print(f\"\\n  Hybrid Top-3:\")\n",
    "    for idx, h_score, b_score, d_score in hybrid_results[:3]:\n",
    "        print(f\"    [{corpus_labels[idx]:25s}] hybrid={h_score:.3f}  (BM25={b_score:.3f}, Dense={d_score:.3f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753f001c",
   "metadata": {},
   "source": [
    "**Observation:** The hybrid approach leverages both BM25's exact keyword matching and dense retrieval's semantic understanding. When both agree on the top result, we can be more confident. When they disagree, the hybrid score finds a balanced result. This is the foundation of modern search engines like Bing (with its Prometheus model) and Perplexity AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5cb9f0",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Neural Re-Ranking\n",
    "\n",
    "Even after hybrid search, the initial retrieval may not produce the optimal ranking. **Re-ranking** uses a more powerful (but slower) model to re-score the top results:\n",
    "\n",
    "### The Two-Stage Pipeline\n",
    "\n",
    "```\n",
    "Query → [Stage 1: Fast Retrieval (BM25 + Dense)] → Top-K candidates\n",
    "      → [Stage 2: Cross-Encoder Re-Ranking] → Final ranked list\n",
    "```\n",
    "\n",
    "### Why Re-Ranking?\n",
    "\n",
    "- **Bi-encoders** (Stage 1) encode query and document *separately* — fast but lose fine-grained interaction\n",
    "- **Cross-encoders** (Stage 2) process the query-document *pair together* — capturing word-level interactions\n",
    "- Cross-encoders are too slow for the entire corpus, but perfect for re-scoring a small candidate set\n",
    "\n",
    "### Re-Ranking in Practice\n",
    "\n",
    "| System | Re-Ranking Model |\n",
    "|---|---|\n",
    "| **Bing** | Prometheus (proprietary) |\n",
    "| **Google** | DeepRank, LambdaMART |\n",
    "| **Perplexity** | T5, GPT-4 cross-encoders |\n",
    "\n",
    "Let's apply cross-encoder re-ranking to our hybrid search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade572df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Re-Ranking with a Cross-Encoder\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "query = \"How do modern search engines combine keyword and semantic search?\"\n",
    "\n",
    "# Stage 1: Get top-10 candidates from hybrid search\n",
    "hybrid_candidates = hybrid_search(query, corpus, bm25, model, corpus_embeddings, alpha=0.5, top_k=10)\n",
    "\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "print(\"=== Stage 1: Hybrid Search Top-10 ===\")\n",
    "candidate_indices = []\n",
    "for rank, (idx, h_score, b_score, d_score) in enumerate(hybrid_candidates, 1):\n",
    "    candidate_indices.append(idx)\n",
    "    print(f\"  #{rank:2d} [{corpus_labels[idx]:25s}] hybrid={h_score:.3f}\")\n",
    "\n",
    "# Stage 2: Re-rank with cross-encoder\n",
    "print(\"\\n=== Stage 2: Cross-Encoder Re-Ranking ===\")\n",
    "pairs = [(query, corpus[idx]) for idx in candidate_indices]\n",
    "ce_scores = cross_encoder.predict(pairs)\n",
    "\n",
    "# Sort by cross-encoder score\n",
    "reranked = sorted(zip(candidate_indices, ce_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n{'Rank':<6} {'Topic':<28} {'CE Score':>10} {'Movement':>10}\")\n",
    "print(\"-\" * 56)\n",
    "for new_rank, (idx, ce_score) in enumerate(reranked, 1):\n",
    "    old_rank = candidate_indices.index(idx) + 1\n",
    "    movement = old_rank - new_rank\n",
    "    arrow = \"↑\" + str(abs(movement)) if movement > 0 else \"↓\" + str(abs(movement)) if movement < 0 else \"—\"\n",
    "    print(f\"  #{new_rank:<4d} [{corpus_labels[idx]:25s}] {ce_score:>9.4f}  {arrow:>8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7427cb",
   "metadata": {},
   "source": [
    "**Observation:** The cross-encoder re-ranks documents by jointly analyzing each query-document pair with full self-attention. This captures fine-grained relevance that bi-encoders miss. Notice how some documents move up or down — the cross-encoder understands *which* passages actually answer the question, not just which ones are topically related.\n",
    "\n",
    "This two-stage pipeline (fast retrieval → precise re-ranking) is the standard architecture in modern conversational search systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1851634",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Evaluation Metrics for Conversational AI in Search\n",
    "\n",
    "Evaluating conversational search systems requires metrics beyond traditional precision/recall. We need to measure both **retrieval quality** and **generation quality**.\n",
    "\n",
    "## Traditional IR Metrics (Recap from earlier lectures)\n",
    "\n",
    "| Metric | What It Measures |\n",
    "|---|---|\n",
    "| **Precision@K** | Fraction of top-K results that are relevant |\n",
    "| **Recall** | Fraction of all relevant results that are retrieved |\n",
    "| **MRR** (Mean Reciprocal Rank) | Average of $\\frac{1}{\\text{rank of first relevant result}}$ |\n",
    "| **MAP** (Mean Average Precision) | Average precision at each relevant result position |\n",
    "| **NDCG** (Normalized Discounted Cumulative Gain) | Rank-weighted quality measure |\n",
    "\n",
    "## New Metrics for Conversational AI / RAG\n",
    "\n",
    "These metrics are critical for evaluating systems that *generate* answers rather than just returning links:\n",
    "\n",
    "| Metric | Formula / Description |\n",
    "|---|---|\n",
    "| **Semantic Similarity** | $\\text{sim}(a, g) = \\cos(E_a, E_g)$ — cosine similarity between answer and ground-truth embeddings |\n",
    "| **Answer Relevancy** | How relevant is the generated answer to the input question? Penalizes incomplete or off-topic answers |\n",
    "| **Context Relevancy** | $\\frac{\\|S\\|}{\\text{Total sentences in context}}$ where $S$ = relevant sentences in the retrieved context |\n",
    "| **Faithfulness** | $\\frac{\\text{Claims supported by context}}{\\text{Total claims}}$ — does the answer only state things supported by the retrieved documents? |\n",
    "| **Answer Correctness** | 1–5 scale examining accuracy against reference answer |\n",
    "\n",
    "## The RAGAS Framework\n",
    "\n",
    "**RAGAS** (Retrieval-Augmented Generation Assessment) is a framework for evaluating RAG pipelines. It combines all the above metrics and can also generate evaluation datasets. See: [docs.ragas.io](https://docs.ragas.io/)\n",
    "\n",
    "Let's implement some of these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Evaluation Metrics for Conversational AI\n",
    "\n",
    "# --- Semantic Similarity ---\n",
    "def semantic_similarity(answer: str, ground_truth: str, model) -> float:\n",
    "    \"\"\"Cosine similarity between answer and ground truth embeddings.\"\"\"\n",
    "    embs = model.encode([answer, ground_truth])\n",
    "    return float(cosine_similarity([embs[0]], [embs[1]])[0][0])\n",
    "\n",
    "# --- Context Relevancy ---\n",
    "def context_relevancy(question: str, context_passages: list, model, threshold=0.3) -> float:\n",
    "    \"\"\"Fraction of context passages that are relevant to the question.\"\"\"\n",
    "    q_emb = model.encode([question])\n",
    "    c_embs = model.encode(context_passages)\n",
    "    sims = cosine_similarity(q_emb, c_embs)[0]\n",
    "    relevant = sum(1 for s in sims if s > threshold)\n",
    "    return relevant / len(context_passages)\n",
    "\n",
    "# --- Faithfulness (simplified) ---\n",
    "def faithfulness_score(answer_claims: list, context: str, model, threshold=0.5) -> float:\n",
    "    \"\"\"Fraction of answer claims that can be found in the context.\"\"\"\n",
    "    context_emb = model.encode([context])\n",
    "    claim_embs = model.encode(answer_claims)\n",
    "    sims = cosine_similarity(claim_embs, context_emb).flatten()\n",
    "    supported = sum(1 for s in sims if s > threshold)\n",
    "    return supported / len(answer_claims)\n",
    "\n",
    "# --- Demonstration ---\n",
    "question = \"What is BM25 and how does it rank documents?\"\n",
    "ground_truth = \"BM25 is a retrieval function that ranks documents based on term frequency, inverse document frequency, and document length normalization.\"\n",
    "generated_answer = \"BM25 is a bag-of-words retrieval function widely used in search engines. It ranks documents using term frequency and inverse document frequency.\"\n",
    "hallucinated_answer = \"BM25 uses deep learning transformers to understand query semantics and generates answers using GPT-4.\"\n",
    "\n",
    "context = corpus[0]  # The BM25 passage\n",
    "answer_claims_good = [\n",
    "    \"BM25 is a bag-of-words retrieval function\",\n",
    "    \"BM25 is widely used in search engines\",\n",
    "    \"BM25 ranks documents using term frequency\",\n",
    "]\n",
    "answer_claims_bad = [\n",
    "    \"BM25 uses deep learning transformers\",\n",
    "    \"BM25 understands query semantics\",\n",
    "    \"BM25 generates answers using GPT-4\",\n",
    "]\n",
    "\n",
    "print(\"=== Evaluation Metrics Demo ===\\n\")\n",
    "print(f\"Question: \\\"{question}\\\"\\n\")\n",
    "\n",
    "# Semantic Similarity\n",
    "sim_good = semantic_similarity(generated_answer, ground_truth, model)\n",
    "sim_bad = semantic_similarity(hallucinated_answer, ground_truth, model)\n",
    "print(f\"Semantic Similarity:\")\n",
    "print(f\"  Good answer:         {sim_good:.4f}\")\n",
    "print(f\"  Hallucinated answer: {sim_bad:.4f}\")\n",
    "\n",
    "# Faithfulness\n",
    "faith_good = faithfulness_score(answer_claims_good, context, model)\n",
    "faith_bad = faithfulness_score(answer_claims_bad, context, model)\n",
    "print(f\"\\nFaithfulness (claims supported by context):\")\n",
    "print(f\"  Good answer:         {faith_good:.2f} ({int(faith_good * len(answer_claims_good))}/{len(answer_claims_good)} claims)\")\n",
    "print(f\"  Hallucinated answer: {faith_bad:.2f} ({int(faith_bad * len(answer_claims_bad))}/{len(answer_claims_bad)} claims)\")\n",
    "\n",
    "# Context Relevancy\n",
    "retrieved_context = [corpus[0], corpus[2], corpus[3], corpus[11]]  # BM25, TF-IDF, PageRank, Cross-Encoder\n",
    "ctx_rel = context_relevancy(question, retrieved_context, model)\n",
    "print(f\"\\nContext Relevancy (relevant passages / retrieved):\")\n",
    "print(f\"  {ctx_rel:.2f}\")\n",
    "for i, (passage, label) in enumerate(zip(retrieved_context, [\"BM25\", \"TF-IDF\", \"PageRank\", \"Cross-Encoder\"])):\n",
    "    q_emb = model.encode([question])\n",
    "    p_emb = model.encode([passage])\n",
    "    sim = cosine_similarity(q_emb, p_emb)[0][0]\n",
    "    relevant = \"✓ relevant\" if sim > 0.3 else \"✗ irrelevant\"\n",
    "    print(f\"    [{label}] sim={sim:.3f} — {relevant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14704141",
   "metadata": {},
   "source": [
    "**Observation:** The evaluation metrics clearly distinguish between good and hallucinated answers:\n",
    "\n",
    "- **Semantic similarity** scores the factually correct answer much higher than the hallucinated one\n",
    "- **Faithfulness** catches that the hallucinated claims about \"deep learning transformers\" and \"GPT-4\" are NOT supported by the BM25 context passage\n",
    "- **Context relevancy** identifies which retrieved passages actually help answer the question\n",
    "\n",
    "These metrics form the foundation of the **RAGAS framework** used to evaluate modern RAG-based conversational search systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfd0fe",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. Commercial Conversational Search Systems\n",
    "\n",
    "Three major systems illustrate how these techniques combine in production:\n",
    "\n",
    "## Bing + ChatGPT (Microsoft)\n",
    "\n",
    "| Stage | Implementation |\n",
    "|---|---|\n",
    "| **Preprocessing** | NER, POS tagging, intent classification, query expansion via GPT-4, multi-turn context retention |\n",
    "| **Retrieval & Ranking** | **Prometheus model**: BM25 + neural retrieval (ColBERT, DPR) + LLM-based answer generation |\n",
    "| **Post-processing** | Fact verification with citations, confidence scoring, RLHF from user feedback (thumbs up/down) |\n",
    "\n",
    "## Google AI Overview (Gemini)\n",
    "\n",
    "| Stage | Implementation |\n",
    "|---|---|\n",
    "| **Preprocessing** | Query expansion with T5-based models, Knowledge Graph integration |\n",
    "| **Retrieval & Ranking** | ColBERT + DPR, Hybrid BM25 + Neural, RAG with live search, ranking Transformers (LambdaMART, DeepRank) |\n",
    "| **Post-processing** | **AIS** (Attributable to Identified Sources — refuses to answer if unverifiable), citation tracking, real-time RL from engagement |\n",
    "\n",
    "## Perplexity AI\n",
    "\n",
    "| Stage | Implementation |\n",
    "|---|---|\n",
    "| **Preprocessing** | NER, intent classification (T5, GPT), query expansion (Word2Vec, FastText, Transformers), spell checking |\n",
    "| **Retrieval & Ranking** | Real-time web search (not static index!), Hybrid BM25 + DPR/ColBERT, cross-encoders (T5, GPT-4), citation-aware ranking |\n",
    "| **Post-processing** | AIS, TruthfulQA & RAGAS fact-checking, score thresholding (0.85), every claim linked to source, multiple LLMs (Claude, GPT-4, Mistral) + RAG |\n",
    "\n",
    "## Comparison\n",
    "\n",
    "| Feature | Bing | Google | Perplexity |\n",
    "|---|---|---|---|\n",
    "| **Retrieval** | Prometheus (BM25+Neural) | Hybrid BM25+Neural+KG | Real-time web + Hybrid |\n",
    "| **LLM** | GPT-4 | Gemini | Multiple (GPT-4, Claude, Mistral) |\n",
    "| **Fact-checking** | Citations + RLHF | AIS + Fact-Check API | AIS + RAGAS + TruthfulQA |\n",
    "| **Unique feature** | Deep Windows integration | Knowledge Graph + AIS | Real-time sources, multi-LLM |\n",
    "\n",
    "**Key takeaway:** All three systems use the same fundamental building blocks we've explored in this tutorial — hybrid search, neural re-ranking, and evaluation metrics — combined with proprietary models and infrastructure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef77f6ac",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Building a Simple Conversational Search Pipeline\n",
    "\n",
    "Let's tie everything together by building a simple conversational search pipeline that demonstrates the key concepts from this tutorial. This pipeline uses FAISS as a vector store and processes multi-turn queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549e151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Conversational Search Pipeline\n",
    "\n",
    "class SimpleConversationalSearch:\n",
    "    \"\"\"A simple conversational search engine demonstrating the core concepts.\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.corpus = corpus\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "        \n",
    "        # Build BM25 index\n",
    "        self.tokenized_corpus = [doc.lower().split() for doc in corpus]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        \n",
    "        # Build FAISS index for dense retrieval\n",
    "        self.corpus_embeddings = self.model.encode(corpus, convert_to_numpy=True)\n",
    "        dim = self.corpus_embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        faiss.normalize_L2(self.corpus_embeddings)\n",
    "        self.index.add(self.corpus_embeddings)\n",
    "        \n",
    "        # Conversation history for context tracking\n",
    "        self.history = []\n",
    "    \n",
    "    def search(self, query, top_k=5, alpha=0.5, rerank=True):\n",
    "        \"\"\"Execute hybrid search with optional re-ranking.\"\"\"\n",
    "        # Track conversation\n",
    "        self.history.append(query)\n",
    "        \n",
    "        # Context-aware query: append recent history for context\n",
    "        context_query = query\n",
    "        if len(self.history) > 1:\n",
    "            context_query = \" \".join(self.history[-3:])  # Last 3 turns\n",
    "        \n",
    "        # BM25 scores\n",
    "        bm25_scores = self.bm25.get_scores(context_query.lower().split())\n",
    "        \n",
    "        # Dense retrieval scores\n",
    "        q_emb = self.model.encode([context_query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_emb)\n",
    "        dense_scores, dense_ids = self.index.search(q_emb, len(self.corpus))\n",
    "        \n",
    "        # Reconstruct dense scores array (FAISS returns sorted)\n",
    "        dense_score_map = np.zeros(len(self.corpus))\n",
    "        for i, idx in enumerate(dense_ids[0]):\n",
    "            dense_score_map[idx] = dense_scores[0][i]\n",
    "        \n",
    "        # Normalize and fuse\n",
    "        scaler = MinMaxScaler()\n",
    "        bm25_norm = scaler.fit_transform(bm25_scores.reshape(-1, 1)).flatten()\n",
    "        dense_norm = scaler.fit_transform(dense_score_map.reshape(-1, 1)).flatten()\n",
    "        hybrid_scores = alpha * bm25_norm + (1 - alpha) * dense_norm\n",
    "        \n",
    "        # Get top-k candidates\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
    "        \n",
    "        if rerank:\n",
    "            # Cross-encoder re-ranking\n",
    "            pairs = [(query, self.corpus[idx]) for idx in top_indices]\n",
    "            ce_scores = self.cross_encoder.predict(pairs)\n",
    "            reranked = sorted(zip(top_indices, ce_scores), key=lambda x: x[1], reverse=True)\n",
    "            return [(idx, score) for idx, score in reranked]\n",
    "        \n",
    "        return [(idx, hybrid_scores[idx]) for idx in top_indices]\n",
    "\n",
    "# Build the search engine\n",
    "search_engine = SimpleConversationalSearch(corpus)\n",
    "\n",
    "# Simulate a multi-turn conversation\n",
    "conversation = [\n",
    "    \"What retrieval methods are used in modern search engines?\",\n",
    "    \"How do you combine keyword and semantic search?\",\n",
    "    \"What about re-ranking the results?\",\n",
    "    \"How do you evaluate the quality of the answers?\",\n",
    "]\n",
    "\n",
    "print(\"=== Simulated Conversational Search Session ===\\n\")\n",
    "for turn_num, query in enumerate(conversation, 1):\n",
    "    print(f\"User (Turn {turn_num}): \\\"{query}\\\"\")\n",
    "    results = search_engine.search(query, top_k=3)\n",
    "    \n",
    "    print(f\"System: Here are the most relevant results:\")\n",
    "    for rank, (idx, score) in enumerate(results, 1):\n",
    "        print(f\"  #{rank} [{corpus_labels[idx]}] (score: {score:.4f})\")\n",
    "        print(f\"     {corpus[idx][:120]}...\")\n",
    "    \n",
    "    # Grounding: confirm understanding\n",
    "    top_topic = corpus_labels[results[0][0]]\n",
    "    print(f\"  → (Grounding: The system identified '{top_topic}' as most relevant to your question)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f77117",
   "metadata": {},
   "source": [
    "**Observation:** This pipeline demonstrates all the key concepts from the lecture:\n",
    "\n",
    "1. **Hybrid search** (BM25 + dense retrieval with score fusion)\n",
    "2. **Neural re-ranking** (cross-encoder re-scores the top candidates)\n",
    "3. **Context tracking** (conversation history is appended to improve follow-up queries)\n",
    "4. **Grounding** (the system acknowledges what it found)\n",
    "\n",
    "In a production system, this would be extended with:\n",
    "- Full coreference resolution for pronouns in follow-up turns\n",
    "- RAG-based answer generation (covered in Tutorial 11)\n",
    "- Agent-based orchestration with tools and guardrails (covered in Tutorial 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92eb449d",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "| Concept | What We Learned |\n",
    "|---|---|\n",
    "| **Human conversation properties** | Turns, grounding, adjacency pairs, mixed initiative, coreference — search must handle all of these |\n",
    "| **Search evolution** | Keyword (BM25) → Intent (BERT embeddings) → Conversational (multi-turn + RAG) |\n",
    "| **Query understanding** | NER, query expansion (embedding similarity), coreference resolution, context tracking |\n",
    "| **BM25** | Fast, exact keyword matching; foundation of all search engines |\n",
    "| **Dense retrieval** | Semantic understanding via embeddings; handles synonyms and paraphrases |\n",
    "| **Hybrid search** | Combines BM25 + dense retrieval with score fusion for best results |\n",
    "| **Re-ranking** | Cross-encoders re-score top candidates with full query-document attention |\n",
    "| **Evaluation metrics** | Semantic similarity, faithfulness, context relevancy, answer relevancy — RAGAS framework |\n",
    "| **Commercial systems** | Bing, Google, Perplexity all use hybrid search + re-ranking + fact-checking |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- **Tutorial 11:** RAG (Retrieval-Augmented Generation) — fusion methods, hallucination detection with RAGAS\n",
    "- **Tutorial 12:** Agentic Search — agents with memory, tools, multi-agent orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debfb69",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercises\n",
    "\n",
    "The following exercises are graded. Please provide your answers in the designated cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9394e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b5cd5d2a43c7994198d355a864a4a93",
     "grade": true,
     "grade_id": "solution_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 1 — Hybrid Search vs BM25 (5 points)\n",
    "\n",
    "Compare **BM25** and **Hybrid Search** (BM25 + Dense Retrieval) as retrieval strategies for a conversational search system. In your answer, address:\n",
    "\n",
    "1. What are the specific strengths and weaknesses of BM25 alone? Give an example query where BM25 would fail.\n",
    "2. How does dense retrieval complement BM25's weaknesses? What does score fusion achieve?\n",
    "3. In what scenario might a pure BM25 approach actually outperform hybrid search?\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b673463e",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58f4f3e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da940173c5a7ee0b4c7b222f7c118924",
     "grade": true,
     "grade_id": "solution_2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 2 — Evaluation Metrics for Conversational Search (5 points)\n",
    "\n",
    "A conversational search system generates the following answer to the question *\"What is BM25?\"*:\n",
    "\n",
    "> \"BM25 is a deep learning algorithm that uses transformer attention to rank documents. It was developed by Google and is the basis for PageRank.\"\n",
    "\n",
    "Using the metrics from Section 8, evaluate this answer. Address:\n",
    "\n",
    "1. What would the **faithfulness** score be if the context contains the correct BM25 definition? Identify each claim and whether it is supported.\n",
    "2. What would the **semantic similarity** score likely be compared to the ground truth \"BM25 is a bag-of-words retrieval function based on term frequency and inverse document frequency\"? Would it be high or low, and why?\n",
    "3. How would the **RAGAS framework** combine these signals to flag this answer as problematic?\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b950633",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8be051f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12563654498394685dd996e87468da83",
     "grade": true,
     "grade_id": "solution_3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 3 — Implementing Re-Ranking Evaluation (10 points)\n",
    "\n",
    "Write code that evaluates the impact of **cross-encoder re-ranking** on retrieval quality. Your code should:\n",
    "\n",
    "1. Use the `corpus` and `queries` defined earlier in this notebook\n",
    "2. For each query, compute the **top-3 results** using:\n",
    "   a. BM25 only\n",
    "   b. Hybrid search (BM25 + Dense, alpha=0.5)\n",
    "   c. Hybrid search + cross-encoder re-ranking\n",
    "3. For each method, compute the **Mean Reciprocal Rank (MRR)** against the expected best passage for each query\n",
    "4. Store the results in three variables: `mrr_bm25`, `mrr_hybrid`, `mrr_reranked` (each a float)\n",
    "\n",
    "Use these query-answer pairs (the expected best passage index in the corpus):\n",
    "```python\n",
    "eval_queries = {\n",
    "    \"How does BM25 work?\": 0,                        # BM25 passage\n",
    "    \"neural embeddings for retrieval\": 1,             # Dense Retrieval passage\n",
    "    \"combining keyword and semantic search\": 10,      # Hybrid Search passage\n",
    "    \"cross-encoder models for search\": 11,            # Cross-Encoder Re-ranking passage\n",
    "    \"evaluating RAG pipelines\": 7,                    # RAGAS passage\n",
    "}\n",
    "```\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36c6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line with your solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714b68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell — do not modify\n",
    "assert 'mrr_bm25' in dir(), \"You need to define 'mrr_bm25'\"\n",
    "assert 'mrr_hybrid' in dir(), \"You need to define 'mrr_hybrid'\"\n",
    "assert 'mrr_reranked' in dir(), \"You need to define 'mrr_reranked'\"\n",
    "assert isinstance(mrr_bm25, float), \"mrr_bm25 should be a float\"\n",
    "assert isinstance(mrr_hybrid, float), \"mrr_hybrid should be a float\"\n",
    "assert isinstance(mrr_reranked, float), \"mrr_reranked should be a float\"\n",
    "assert 0 <= mrr_bm25 <= 1, \"mrr_bm25 should be between 0 and 1\"\n",
    "assert 0 <= mrr_hybrid <= 1, \"mrr_hybrid should be between 0 and 1\"\n",
    "assert 0 <= mrr_reranked <= 1, \"mrr_reranked should be between 0 and 1\"\n",
    "print(f\"MRR (BM25 only):             {mrr_bm25:.4f}\")\n",
    "print(f\"MRR (Hybrid):                {mrr_hybrid:.4f}\")\n",
    "print(f\"MRR (Hybrid + Re-Ranking):   {mrr_reranked:.4f}\")\n",
    "print(f\"\\nRe-ranking improvement over BM25: {(mrr_reranked - mrr_bm25) / max(mrr_bm25, 0.001) * 100:.1f}%\")\n",
    "print(\"All auto-graded tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
