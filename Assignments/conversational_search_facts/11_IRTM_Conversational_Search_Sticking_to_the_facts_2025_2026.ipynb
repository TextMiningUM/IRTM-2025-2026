{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "230611e8",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f301d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c63ecf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f67d0",
   "metadata": {},
   "source": [
    "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
    "\n",
    "# Information Retrieval and Text Mining Course\n",
    "## Tutorial 11 — Conversational Search: Sticking to the Facts (RAG & Knowledge Graphs)\n",
    "\n",
    "**Author:** Jan Scholtes\n",
    "\n",
    "**Edition 2025-2026**\n",
    "\n",
    "Department of Advanced Computer Sciences — Maastricht University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928fe96d",
   "metadata": {},
   "source": [
    "Welcome to Tutorial 11 on **Conversational Search: Sticking to the Facts**. This is the second of three tutorials on Conversational Search:\n",
    "\n",
    "1. **The Basics** (Tutorial 10) — dialogue structure, query understanding, hybrid search, re-ranking, evaluation metrics\n",
    "2. **Sticking to the Facts — RAG & Knowledge Graphs** (this tutorial) — retrieval-augmented generation, knowledge graph integration, hallucination detection\n",
    "3. **Agentic Approaches** (Tutorial 12) — agents, memory, tools, multi-agent orchestration\n",
    "\n",
    "In this tutorial we explore how to keep LLMs factually grounded using external knowledge. The topics covered are:\n",
    "\n",
    "1. **Challenges in Conversational AI** — bias, hallucination, provenance, \"stochastic parrots\"\n",
    "2. **What is RAG?** — the retrieval-augmented generation pipeline and its components\n",
    "3. **Prompt-Level RAG (Early Fusion)** — injecting retrieved text into prompts\n",
    "4. **Vector-Level RAG (Embedding Fusion)** — merging query and context embeddings\n",
    "5. **Knowledge Graph Integration** — SPARQL, KG-augmented RAG, TransE, GraphSAGE\n",
    "6. **Late Fusion & Memory-Augmented Methods** — cross-attention, kNN-LM, dynamic retrieval\n",
    "7. **Building a Complete RAG Pipeline** — chunking, embedding, retrieval, generation\n",
    "8. **Hallucination Detection with RAGAS** — faithfulness, answer relevancy, context precision\n",
    "9. **RAG Trade-offs** — benefits, limitations, computational complexity\n",
    "10. **Applied RAG Pipeline: Sherlock Holmes Chatbot** — building a full RAG chatbot using the corpus from Tutorials 03 & 07\n",
    "11. **RAGAS Evaluation on Your Corpus** — quantitative evaluation of RAG modes (No-RAG vs BM25 vs Dense)\n",
    "\n",
    "At the end you will find the **Exercises** section with graded assignments.\n",
    "\n",
    "> **Note:** This course is about Information Retrieval, Text Mining, and Conversational Search — not about programming skills. The code cells below show you *how* these methods work in practice using Python libraries. Focus on understanding the **concepts** and **results**.\n",
    "\n",
    "> **Cross-notebook arc:** In Tutorial 03 you built a search engine over your own text. In Tutorial 07 you built a Knowledge Graph, FAISS vector store, and QA test set from that corpus. In Sections 10–11 of this tutorial, you use all of those artifacts to build and evaluate a RAG chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e6175",
   "metadata": {},
   "source": [
    "## Library Installation\n",
    "\n",
    "We install all required packages in a single cell. Run this cell once at the beginning of your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    \"sentence-transformers\",\n",
    "    \"rank_bm25\",\n",
    "    \"faiss-cpu\",\n",
    "    \"scikit-learn\",\n",
    "    \"openai\",\n",
    "    \"tiktoken\",\n",
    "]\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f320e9",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "All imports are grouped here so the notebook is easy to set up and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44e24c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import textwrap\n",
    "import getpass\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# BM25\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Sentence Transformers (dense retrieval + cross-encoder)\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# FAISS for vector search\n",
    "import faiss\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Tiktoken for token counting\n",
    "import tiktoken\n",
    "\n",
    "# OpenAI (for RAG generation)\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1660f85",
   "metadata": {},
   "source": [
    "## API Key Setup\n",
    "\n",
    "We use OpenAI's API for the generation component of our RAG pipeline. Enter your API key below — it will not be stored or displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b534e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OpenAI API key securely\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "client = OpenAI()\n",
    "print(\"OpenAI client configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d49d9",
   "metadata": {},
   "source": [
    "## Load Models\n",
    "\n",
    "We load the embedding model and cross-encoder once, so they can be reused throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6fc9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "# Token counter for context window management\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "print(f\"Embedding model: all-MiniLM-L6-v2 (dim={embedding_model.get_sentence_embedding_dimension()})\")\n",
    "print(f\"Cross-encoder: ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"Models loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d279fa96",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Challenges in Conversational AI\n",
    "\n",
    "Before diving into RAG, we must understand *why* it is needed. Large Language Models face several fundamental challenges:\n",
    "\n",
    "## The \"Stochastic Parrot\" Problem\n",
    "\n",
    "LLMs are statistical next-word predictors trained on massive text corpora. They have:\n",
    "- **No real knowledge** — only patterns learned from training data\n",
    "- **No memory** — each conversation starts fresh (without external memory systems)\n",
    "- **No understanding** — just sophisticated pattern matching\n",
    "- **No built-in fact-checking** — they generate plausible-sounding text regardless of truth\n",
    "\n",
    "> *\"On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?\"* — Bender, Gebru et al. (FAccT 2021)\n",
    "\n",
    "## Key Challenges\n",
    "\n",
    "| Challenge | Description | Impact |\n",
    "|---|---|---|\n",
    "| **Hallucination** | LLMs fabricate facts that sound convincing but are false | Users trust wrong information |\n",
    "| **Bias** | Training data contains societal biases (Wikipedia, Reddit, news) | Unfair or skewed responses |\n",
    "| **Provenance** | No way to trace *where* an answer comes from | Cannot verify claims |\n",
    "| **Stale knowledge** | Training data has a cutoff date | Outdated information |\n",
    "| **No domain expertise** | General models lack specialized knowledge | Poor performance on domain tasks |\n",
    "\n",
    "## Solutions Brief\n",
    "\n",
    "| Solution | Approach | Where in Pipeline |\n",
    "|---|---|---|\n",
    "| **Prompt Engineering** | Background prompts, CoT, context injection | Pre-generation |\n",
    "| **RAG** | Retrieve external documents to ground generation | Pre-generation |\n",
    "| **Knowledge Graphs** | Structured facts via SPARQL queries | Pre-generation |\n",
    "| **RLHF** | Align model outputs with human preferences | Training time |\n",
    "| **Hallucination Detection** | RAGAS, TruthfulQA metrics | Post-generation |\n",
    "| **Agentic Architecture** | Agents with verification tools | Full pipeline (Tutorial 12) |\n",
    "\n",
    "In this tutorial, we focus on **RAG**, **Knowledge Graphs**, and **Hallucination Detection**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75d050",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. What is RAG? (Retrieval-Augmented Generation)\n",
    "\n",
    "**RAG** is a hybrid approach that combines information retrieval with generative models to enhance factual grounding:\n",
    "\n",
    "```\n",
    "Without RAG:  Query ──────────────────────────────→ LLM → Response (may hallucinate)\n",
    "\n",
    "With RAG:     Query → Retriever → Retrieved Docs ─→ LLM → Response (grounded in evidence)\n",
    "                         ↑\n",
    "                   Vector Store / Index\n",
    "```\n",
    "\n",
    "## RAG Pipeline Components\n",
    "\n",
    "| Component | Role | Example Tools |\n",
    "|---|---|---|\n",
    "| **Data Ingestion** | Load documents from various sources | LangChain, LlamaIndex |\n",
    "| **Chunking** | Split documents into manageable pieces | LangChain TextSplitters |\n",
    "| **Embedding Model** | Convert text chunks to vectors | SentenceTransformers, OpenAI Embeddings |\n",
    "| **Vector Store** | Store and search embeddings | FAISS, Pinecone, Weaviate, Chroma |\n",
    "| **Retriever** | Find relevant passages for a query | BM25, DPR, ColBERT |\n",
    "| **Re-Ranker** | Re-score candidates for precision | Cross-Encoders (ms-marco) |\n",
    "| **Generator** | Produce final answer from query + context | GPT-4, FLAN-T5, LLaMA |\n",
    "| **Evaluation** | Measure quality and detect hallucination | RAGAS, TruthfulQA |\n",
    "\n",
    "## Benefits of RAG\n",
    "\n",
    "- **Up-to-date output** — retrieves from current knowledge base, not frozen training data\n",
    "- **Factual grounding** — answers are backed by retrieved evidence\n",
    "- **Smaller models, better results** — retrieval reduces the LLM's knowledge burden\n",
    "- **Domain adaptability** — swap the retrieval corpus for any domain\n",
    "- **Explainability** — sources can be cited alongside the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8401f45",
   "metadata": {},
   "source": [
    "## Sample Knowledge Base\n",
    "\n",
    "We create a knowledge base of text passages about AI and Information Retrieval. This serves as our document collection for the RAG pipeline throughout this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge base: passages about AI and IR topics\n",
    "knowledge_base = [\n",
    "    {\n",
    "        \"id\": \"doc_001\",\n",
    "        \"title\": \"BM25 Retrieval\",\n",
    "        \"text\": \"BM25 (Best Matching 25) is a probabilistic retrieval function that ranks documents based on term frequency, inverse document frequency, and document length normalization. It is the default ranking function in Elasticsearch and Apache Solr. The formula uses parameters k1 (typically 1.2-2.0) for term frequency saturation and b (typically 0.75) for document length normalization.\",\n",
    "        \"source\": \"IR Textbook, Chapter 5\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_002\",\n",
    "        \"title\": \"Dense Passage Retrieval (DPR)\",\n",
    "        \"text\": \"Dense Passage Retrieval uses dual BERT encoders to independently encode questions and passages into dense vectors. The encoders are trained on question-passage pairs from Natural Questions and TriviaQA. DPR achieves significant improvements over BM25 on open-domain question answering by capturing semantic similarity beyond lexical matching.\",\n",
    "        \"source\": \"Karpukhin et al., 2020\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_003\",\n",
    "        \"title\": \"RAG Architecture\",\n",
    "        \"text\": \"Retrieval-Augmented Generation (RAG) combines a pre-trained retriever (e.g., DPR) with a pre-trained sequence-to-sequence generator (e.g., BART). The retriever finds relevant documents from a knowledge source, and the generator conditions on both the query and retrieved documents to produce the final output. RAG can be used in two modes: RAG-Sequence (retrieves for entire sequence) and RAG-Token (retrieves per output token).\",\n",
    "        \"source\": \"Lewis et al., 2020\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_004\",\n",
    "        \"title\": \"Knowledge Graphs in AI\",\n",
    "        \"text\": \"Knowledge Graphs store structured information as triples: (subject, predicate, object). Major public KGs include Wikidata, DBpedia, and YAGO. They can be queried using SPARQL, a graph query language similar to SQL. Knowledge Graphs provide factual grounding for AI systems by offering verified, structured relationships between entities.\",\n",
    "        \"source\": \"Hogan et al., 2021\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_005\",\n",
    "        \"title\": \"TransE Embedding Model\",\n",
    "        \"text\": \"TransE (Translating Embeddings) is a knowledge graph embedding method where relationships are modeled as translations in embedding space: h + r approximately equals t, where h is the head entity, r is the relation, and t is the tail entity. TransE is simple and efficient but struggles with one-to-many and many-to-many relations. It produces embeddings that can be used in vector-level RAG.\",\n",
    "        \"source\": \"Bordes et al., 2013\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_006\",\n",
    "        \"title\": \"RAGAS Evaluation Framework\",\n",
    "        \"text\": \"RAGAS (Retrieval-Augmented Generation Assessment) evaluates RAG pipelines using four metrics: Faithfulness (fraction of answer claims supported by context), Answer Relevancy (relevance of answer to question), Context Precision (proportion of relevant items in retrieved context), and Context Recall (how much of the ground truth is covered by context). Faithfulness is the primary metric for hallucination detection.\",\n",
    "        \"source\": \"Es et al., 2023\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_007\",\n",
    "        \"title\": \"Chain-of-Thought Prompting\",\n",
    "        \"text\": \"Chain-of-Thought (CoT) prompting enables LLMs to solve complex reasoning tasks by including intermediate reasoning steps in the prompt. By providing worked examples with step-by-step reasoning, LLMs produce more accurate and verifiable outputs. CoT can be combined with RAG to improve both retrieval queries and answer generation.\",\n",
    "        \"source\": \"Wei et al., 2023\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_008\",\n",
    "        \"title\": \"FAISS Vector Database\",\n",
    "        \"text\": \"FAISS (Facebook AI Similarity Search) is an open-source library for efficient similarity search and clustering of dense vectors. It supports various index types including flat (exact), IVF (inverted file), and HNSW (hierarchical navigable small world) for approximate nearest neighbor search. FAISS reduces retrieval complexity from O(N) to O(log N) and is essential for production RAG systems.\",\n",
    "        \"source\": \"Johnson et al., 2019\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_009\",\n",
    "        \"title\": \"Hallucination in LLMs\",\n",
    "        \"text\": \"Hallucination occurs when an LLM generates text that is fluent and plausible but factually incorrect or unsupported by the provided context. Types include intrinsic hallucination (contradicting source material) and extrinsic hallucination (adding unverifiable information). Mitigation strategies include RAG, knowledge graph grounding, RLHF, and post-generation fact-checking with frameworks like RAGAS and TruthfulQA.\",\n",
    "        \"source\": \"Ji et al., 2023\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_010\",\n",
    "        \"title\": \"Fusion-in-Decoder (FiD)\",\n",
    "        \"text\": \"Fusion-in-Decoder is a RAG variant where each retrieved passage is independently encoded by the encoder, and the decoder cross-attends to all encoded passages. Unlike simple prompt concatenation, FiD can handle a large number of retrieved passages efficiently because the encoder processes each passage separately, while the decoder fuses information during generation.\",\n",
    "        \"source\": \"Izacard & Grave, 2020\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_011\",\n",
    "        \"title\": \"kNN-LM (Memory-Augmented Generation)\",\n",
    "        \"text\": \"The k-Nearest Neighbor Language Model (kNN-LM) augments a pre-trained language model by interpolating its output distribution with a distribution derived from nearest neighbor lookups in a datastore. At each generation step, the model queries an external memory of (context, target) pairs, and the final probability is a weighted combination: p = lambda * p_kNN + (1-lambda) * p_LM. This enables domain adaptation without retraining.\",\n",
    "        \"source\": \"Khandelwal et al., 2020\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc_012\",\n",
    "        \"title\": \"GraphSAGE for Node Embeddings\",\n",
    "        \"text\": \"GraphSAGE (SAmple and aggreGatE) is a graph neural network that generates node embeddings by sampling and aggregating features from local neighborhoods. For each node, it samples neighbors, aggregates their features (using mean, LSTM, or pooling), and concatenates with the node's own features. GraphSAGE is inductive: it can generate embeddings for unseen nodes, making it suitable for dynamic knowledge graphs in RAG.\",\n",
    "        \"source\": \"Hamilton et al., 2017\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Extract texts for embedding\n",
    "kb_texts = [doc[\"text\"] for doc in knowledge_base]\n",
    "kb_titles = [doc[\"title\"] for doc in knowledge_base]\n",
    "\n",
    "print(f\"Knowledge base loaded: {len(knowledge_base)} documents\")\n",
    "for doc in knowledge_base:\n",
    "    print(f\"  [{doc['id']}] {doc['title']} — {doc['text'][:70]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023a304",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Prompt-Level RAG (Early Fusion)\n",
    "\n",
    "The simplest and most common form of RAG operates at the **text/prompt level**. The idea is straightforward:\n",
    "\n",
    "1. **Retrieve** relevant documents for the query\n",
    "2. **Concatenate** them into the prompt as context\n",
    "3. **Generate** an answer conditioned on that context\n",
    "\n",
    "```\n",
    "User Query: \"What is BM25?\"\n",
    "          ↓\n",
    "   [Retriever finds top-3 passages]\n",
    "          ↓\n",
    "   Prompt: \"Given the following context:\n",
    "            [Retrieved passage 1]\n",
    "            [Retrieved passage 2]\n",
    "            [Retrieved passage 3]\n",
    "            \n",
    "            Answer the question: What is BM25?\"\n",
    "          ↓\n",
    "   [LLM generates grounded answer]\n",
    "```\n",
    "\n",
    "## Methods\n",
    "\n",
    "| Method | Description |\n",
    "|---|---|\n",
    "| **Simple Concatenation** | Append top-k retrieved passages before the query |\n",
    "| **Prompt Templates** | Structured injection: \"Given this context: {context}, answer: {query}\" |\n",
    "| **Selective Context Injection** | Only inject the most relevant sentences, not full passages |\n",
    "| **Context Window Management** | Dynamic token allocation — prioritize high-similarity chunks |\n",
    "\n",
    "## Key Papers\n",
    "- **Lewis et al., 2020** — *Retrieval-Augmented Generation for Knowledge-Intensive NLP* (original RAG)\n",
    "- **Izacard & Grave, 2020** — *Fusion-in-Decoder* (FiD) — all passages as decoder inputs\n",
    "\n",
    "Let's build a prompt-level RAG pipeline step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d8a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build the retrieval index\n",
    "\n",
    "# Encode all knowledge base documents\n",
    "kb_embeddings = embedding_model.encode(kb_texts, convert_to_numpy=True, show_progress_bar=False)\n",
    "print(f\"Encoded {len(kb_embeddings)} documents → shape: {kb_embeddings.shape}\")\n",
    "\n",
    "# Build FAISS index (inner product on normalized vectors = cosine similarity)\n",
    "dim = kb_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "kb_embeddings_norm = kb_embeddings.copy()\n",
    "faiss.normalize_L2(kb_embeddings_norm)\n",
    "index.add(kb_embeddings_norm)\n",
    "\n",
    "# Also build BM25 index for hybrid search\n",
    "tokenized_kb = [doc.lower().split() for doc in kb_texts]\n",
    "bm25 = BM25Okapi(tokenized_kb)\n",
    "\n",
    "print(f\"FAISS index built: {index.ntotal} vectors, dim={dim}\")\n",
    "print(f\"BM25 index built: {len(tokenized_kb)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413d04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build retriever functions\n",
    "\n",
    "def retrieve_dense(query, top_k=3):\n",
    "    \"\"\"Retrieve documents using dense (semantic) search.\"\"\"\n",
    "    q_emb = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, indices = index.search(q_emb, top_k)\n",
    "    return [(int(idx), float(score)) for idx, score in zip(indices[0], scores[0])]\n",
    "\n",
    "def retrieve_bm25(query, top_k=3):\n",
    "    \"\"\"Retrieve documents using BM25 keyword search.\"\"\"\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(int(idx), float(scores[idx])) for idx in top_indices]\n",
    "\n",
    "def retrieve_hybrid(query, top_k=3, alpha=0.5):\n",
    "    \"\"\"Hybrid retrieval: combine BM25 and dense scores.\"\"\"\n",
    "    # BM25 scores\n",
    "    bm25_scores = bm25.get_scores(query.lower().split())\n",
    "    \n",
    "    # Dense scores\n",
    "    q_emb = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    dense_scores_raw, dense_ids = index.search(q_emb, len(kb_texts))\n",
    "    dense_scores = np.zeros(len(kb_texts))\n",
    "    for i, idx in enumerate(dense_ids[0]):\n",
    "        dense_scores[idx] = dense_scores_raw[0][i]\n",
    "    \n",
    "    # Normalize both to [0,1]\n",
    "    scaler = MinMaxScaler()\n",
    "    bm25_norm = scaler.fit_transform(bm25_scores.reshape(-1, 1)).flatten()\n",
    "    dense_norm = scaler.fit_transform(dense_scores.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Fuse\n",
    "    hybrid = alpha * bm25_norm + (1 - alpha) * dense_norm\n",
    "    top_indices = np.argsort(hybrid)[::-1][:top_k]\n",
    "    return [(int(idx), float(hybrid[idx])) for idx in top_indices]\n",
    "\n",
    "def rerank(query, candidates):\n",
    "    \"\"\"Re-rank candidates using a cross-encoder.\"\"\"\n",
    "    pairs = [(query, kb_texts[idx]) for idx, _ in candidates]\n",
    "    ce_scores = cross_encoder.predict(pairs)\n",
    "    reranked = sorted(zip([idx for idx, _ in candidates], ce_scores),\n",
    "                      key=lambda x: x[1], reverse=True)\n",
    "    return [(idx, float(score)) for idx, score in reranked]\n",
    "\n",
    "# Test the retriever\n",
    "query = \"How does RAG work?\"\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "\n",
    "for name, results in [(\"Dense\", retrieve_dense(query)),\n",
    "                      (\"BM25\", retrieve_bm25(query)),\n",
    "                      (\"Hybrid\", retrieve_hybrid(query))]:\n",
    "    print(f\"  {name} Top-3:\")\n",
    "    for idx, score in results:\n",
    "        print(f\"    [{kb_titles[idx]:40s}] score={score:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317f0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Construct the RAG prompt\n",
    "\n",
    "def build_rag_prompt(query, retrieved_indices, max_tokens=2000):\n",
    "    \"\"\"\n",
    "    Build a prompt-level RAG prompt with retrieved context.\n",
    "    Includes context window management (token counting).\n",
    "    \"\"\"\n",
    "    # System prompt with instructions\n",
    "    system_prompt = (\n",
    "        \"You are a helpful AI assistant specialized in Information Retrieval and Text Mining. \"\n",
    "        \"Answer the user's question based ONLY on the provided context. \"\n",
    "        \"If the context does not contain enough information to answer, say 'I don't have enough information to answer this.' \"\n",
    "        \"Always cite the source document when making a claim.\"\n",
    "    )\n",
    "    \n",
    "    # Build context from retrieved documents\n",
    "    context_parts = []\n",
    "    total_tokens = len(enc.encode(system_prompt))\n",
    "    \n",
    "    for idx, score in retrieved_indices:\n",
    "        doc = knowledge_base[idx]\n",
    "        passage = f\"[Source: {doc['source']}] {doc['title']}: {doc['text']}\"\n",
    "        passage_tokens = len(enc.encode(passage))\n",
    "        \n",
    "        if total_tokens + passage_tokens > max_tokens:\n",
    "            print(f\"  ⚠ Context window limit reached ({total_tokens}/{max_tokens} tokens), skipping remaining docs\")\n",
    "            break\n",
    "        \n",
    "        context_parts.append(passage)\n",
    "        total_tokens += passage_tokens\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    user_message = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER (cite sources):\"\"\"\n",
    "    \n",
    "    return system_prompt, user_message, total_tokens\n",
    "\n",
    "# Demonstrate prompt construction\n",
    "query = \"What is RAG and how does it work?\"\n",
    "results = retrieve_hybrid(query, top_k=3)\n",
    "results = rerank(query, results)\n",
    "\n",
    "system_prompt, user_message, token_count = build_rag_prompt(query, results)\n",
    "\n",
    "print(f\"Query: \\\"{query}\\\"\")\n",
    "print(f\"Total context tokens: {token_count}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SYSTEM PROMPT:\\n{system_prompt}\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"USER MESSAGE:\\n{user_message[:800]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5781ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate answer with RAG\n",
    "\n",
    "def rag_answer(query, top_k=3, alpha=0.5, use_rerank=True, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Complete RAG pipeline: retrieve → rerank → prompt → generate.\"\"\"\n",
    "    # Retrieve\n",
    "    candidates = retrieve_hybrid(query, top_k=top_k, alpha=alpha)\n",
    "    \n",
    "    # Re-rank\n",
    "    if use_rerank:\n",
    "        candidates = rerank(query, candidates)\n",
    "    \n",
    "    # Build prompt\n",
    "    system_prompt, user_message, tokens = build_rag_prompt(query, candidates)\n",
    "    \n",
    "    # Generate\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.2,  # Low temperature for factual answers\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    \n",
    "    answer = response.choices[0].message.content\n",
    "    \n",
    "    # Collect sources\n",
    "    sources = [knowledge_base[idx] for idx, _ in candidates]\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"context_tokens\": tokens,\n",
    "        \"model\": model,\n",
    "    }\n",
    "\n",
    "# Test the full RAG pipeline\n",
    "result = rag_answer(\"What is RAG and how does it work?\")\n",
    "\n",
    "print(f\"Query: \\\"{result['query']}\\\"\")\n",
    "print(f\"Model: {result['model']} | Context tokens: {result['context_tokens']}\")\n",
    "print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "print(f\"\\nSources:\")\n",
    "for s in result['sources']:\n",
    "    print(f\"  - {s['title']} ({s['source']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd52a4f",
   "metadata": {},
   "source": [
    "### Comparing RAG vs. No-RAG\n",
    "\n",
    "Let's see the difference between asking the LLM directly (no RAG) and using our RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b507cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG vs. no-RAG responses\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the RAGAS framework and how does it detect hallucinations?\",\n",
    "    \"How does kNN-LM augment language model generation?\",\n",
    "    \"What are the differences between TransE and GraphSAGE?\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "    \n",
    "    # No RAG (direct LLM)\n",
    "    no_rag = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": query}],\n",
    "        temperature=0.2,\n",
    "        max_tokens=200,\n",
    "    ).choices[0].message.content\n",
    "    \n",
    "    # With RAG\n",
    "    rag_result = rag_answer(query)\n",
    "    \n",
    "    print(f\"WITHOUT RAG:\")\n",
    "    print(f\"  {no_rag[:300]}...\")\n",
    "    print(f\"\\nWITH RAG:\")\n",
    "    print(f\"  {rag_result['answer'][:300]}...\")\n",
    "    print(f\"  Sources: {', '.join(s['title'] for s in rag_result['sources'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4986a",
   "metadata": {},
   "source": [
    "**Observation:** The RAG-augmented answers are:\n",
    "- **Grounded** in specific, verifiable sources\n",
    "- **More precise** because they draw from curated knowledge\n",
    "- **Citable** — every claim can be traced to a source document\n",
    "\n",
    "Without RAG, the LLM generates from its training data, which may be outdated, incomplete, or fabricated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bf3802",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Vector-Level RAG (Embedding Fusion)\n",
    "\n",
    "While prompt-level RAG injects text into prompts, **vector-level RAG** operates in embedding space:\n",
    "\n",
    "## Key Methods\n",
    "\n",
    "### A. Query Expansion in Embedding Space\n",
    "Expand the query embedding with vectors from relevant documents:\n",
    "- **Averaging:** $\\vec{q}_{\\text{new}} = \\text{mean}(\\vec{q}, \\vec{d}_1, \\vec{d}_2, \\ldots)$\n",
    "- **Weighted averaging:** $\\vec{q}_{\\text{new}} = \\alpha \\cdot \\vec{q} + (1 - \\alpha) \\cdot \\text{mean}(\\vec{d}_1, \\ldots, \\vec{d}_k)$\n",
    "- **Learned transformation:** A small MLP fuses query + context embeddings\n",
    "\n",
    "### B. Cross-Attention Vector Fusion\n",
    "Before generation, the query embedding is modified by attending over retrieved context vectors:\n",
    "1. Query vector attends over retrieved context vectors via attention layers\n",
    "2. New query embedding is computed by weighted attention\n",
    "3. This contextualized query is sent to the LLM\n",
    "\n",
    "### The Alignment Problem\n",
    "Vectors from different sources (text embeddings, KG embeddings, graph embeddings) may live in **different vector spaces**. Before fusion, they must be **aligned**:\n",
    "\n",
    "$$\\hat{v} = \\frac{v}{\\|v\\|}$$\n",
    "\n",
    "Methods for alignment include:\n",
    "- Linear projection layers\n",
    "- Contrastive learning (CLIP-style)\n",
    "- Adapter networks\n",
    "- Joint training\n",
    "\n",
    "Let's demonstrate query expansion in embedding space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bef1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-Level RAG: Query Expansion in Embedding Space\n",
    "\n",
    "def vector_expanded_retrieval(query, top_k_expand=3, top_k_final=3, alpha=0.7):\n",
    "    \"\"\"\n",
    "    1. Retrieve top-k docs using original query\n",
    "    2. Expand query vector using retrieved doc vectors\n",
    "    3. Re-retrieve using expanded query vector\n",
    "    \"\"\"\n",
    "    # Step 1: Initial retrieval\n",
    "    q_emb = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, indices = index.search(q_emb, top_k_expand)\n",
    "    \n",
    "    initial_results = [(int(idx), float(s)) for idx, s in zip(indices[0], scores[0])]\n",
    "    \n",
    "    # Step 2: Vector expansion — weighted average of query + retrieved docs\n",
    "    doc_embs = kb_embeddings_norm[indices[0]]  # Embeddings of retrieved docs\n",
    "    expanded_q = alpha * q_emb + (1 - alpha) * np.mean(doc_embs, axis=0, keepdims=True)\n",
    "    faiss.normalize_L2(expanded_q)\n",
    "    \n",
    "    # Step 3: Re-retrieve with expanded query\n",
    "    scores2, indices2 = index.search(expanded_q, top_k_final)\n",
    "    expanded_results = [(int(idx), float(s)) for idx, s in zip(indices2[0], scores2[0])]\n",
    "    \n",
    "    return initial_results, expanded_results\n",
    "\n",
    "# Compare original vs. expanded retrieval\n",
    "queries = [\n",
    "    \"How to prevent AI from making things up?\",       # Indirect phrasing\n",
    "    \"embedding methods for graph structures\",          # Vague query\n",
    "    \"combining search with text generation\",           # Paraphrase of RAG\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    initial, expanded = vector_expanded_retrieval(query)\n",
    "    \n",
    "    print(f\"Query: \\\"{query}\\\"\")\n",
    "    print(f\"  Original retrieval:  {[kb_titles[idx] for idx, _ in initial]}\")\n",
    "    print(f\"  Expanded retrieval:  {[kb_titles[idx] for idx, _ in expanded]}\")\n",
    "    \n",
    "    # Check if expansion changed the results\n",
    "    orig_set = set(idx for idx, _ in initial)\n",
    "    exp_set = set(idx for idx, _ in expanded)\n",
    "    new_docs = exp_set - orig_set\n",
    "    if new_docs:\n",
    "        print(f\"  ★ New documents found: {[kb_titles[idx] for idx in new_docs]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7797ee17",
   "metadata": {},
   "source": [
    "**Observation:** Vector expansion enriches the query representation with information from initially retrieved documents. This is particularly useful for:\n",
    "- **Vague or paraphrased queries** — the expanded vector captures more relevant semantics\n",
    "- **Domain-specific terminology** — initial results add domain vocabulary to the query vector\n",
    "- **Multi-hop reasoning** — retrieved documents may bridge to other relevant documents\n",
    "\n",
    "This is the embedding-level equivalent of query expansion discussed in Tutorial 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e9faa",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Knowledge Graph Integration (KG-Augmented RAG)\n",
    "\n",
    "Knowledge Graphs provide **structured, verified facts** that complement unstructured text retrieval:\n",
    "\n",
    "## What is a Knowledge Graph?\n",
    "\n",
    "A KG stores information as **triples**: (Subject, Predicate, Object)\n",
    "- Example: `(Albert_Einstein, bornIn, Ulm)`, `(Albert_Einstein, wonAward, Nobel_Prize_Physics)`\n",
    "\n",
    "## Major Public Knowledge Graphs\n",
    "\n",
    "| Knowledge Graph | Description | SPARQL Endpoint |\n",
    "|---|---|---|\n",
    "| **Wikidata** | Community-curated structured data from Wikimedia | query.wikidata.org |\n",
    "| **DBpedia** | Structured content extracted from Wikipedia | dbpedia.org/sparql |\n",
    "| **YAGO** | Combines Wikidata + Wikipedia + WordNet | yago-knowledge.org |\n",
    "| **ConceptNet** | Commonsense knowledge graph | N/A (API-based) |\n",
    "| **UMLS** | Unified Medical Language System | N/A (license required) |\n",
    "\n",
    "## SPARQL — Querying Knowledge Graphs\n",
    "\n",
    "SPARQL is to Knowledge Graphs what SQL is to relational databases:\n",
    "\n",
    "```sparql\n",
    "SELECT ?person ?personLabel ?birthPlace ?birthPlaceLabel\n",
    "WHERE {\n",
    "  ?person wdt:P31 wd:Q5 .          # Instance of human\n",
    "  ?person wdt:P166 wd:Q35637 .     # Won Nobel Prize in Physics\n",
    "  ?person wdt:P19 ?birthPlace .     # Place of birth\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" . }\n",
    "}\n",
    "LIMIT 10\n",
    "```\n",
    "\n",
    "## KG-Augmented RAG Methods\n",
    "\n",
    "| Method | Description |\n",
    "|---|---|\n",
    "| **KG Triples as Text** | Convert triples to natural language, inject into prompt |\n",
    "| **KG Embeddings (TransE)** | Embed entities/relations as vectors: $h + r \\approx t$ |\n",
    "| **GNN Embeddings (GraphSAGE)** | Embed graph structure using message-passing neural networks |\n",
    "| **SPARQL + LLM** | LLM generates SPARQL queries, results injected back into prompt |\n",
    "\n",
    "Let's demonstrate KG-augmented RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13909b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Graph Integration: KG triples as text for RAG\n",
    "\n",
    "# Simulated Knowledge Graph triples (in practice, these come from Wikidata/DBpedia via SPARQL)\n",
    "kg_triples = [\n",
    "    (\"BM25\", \"is_a\", \"retrieval function\"),\n",
    "    (\"BM25\", \"used_in\", \"Elasticsearch\"),\n",
    "    (\"BM25\", \"used_in\", \"Apache Solr\"),\n",
    "    (\"BM25\", \"uses\", \"term frequency\"),\n",
    "    (\"BM25\", \"uses\", \"inverse document frequency\"),\n",
    "    (\"RAG\", \"is_a\", \"hybrid approach\"),\n",
    "    (\"RAG\", \"combines\", \"information retrieval\"),\n",
    "    (\"RAG\", \"combines\", \"language model generation\"),\n",
    "    (\"RAG\", \"proposed_by\", \"Lewis et al. 2020\"),\n",
    "    (\"RAG\", \"reduces\", \"hallucination\"),\n",
    "    (\"FAISS\", \"is_a\", \"vector database\"),\n",
    "    (\"FAISS\", \"developed_by\", \"Meta AI\"),\n",
    "    (\"FAISS\", \"supports\", \"approximate nearest neighbor search\"),\n",
    "    (\"FAISS\", \"complexity\", \"O(log N) with IVF indexing\"),\n",
    "    (\"TransE\", \"is_a\", \"knowledge graph embedding method\"),\n",
    "    (\"TransE\", \"formula\", \"h + r ≈ t\"),\n",
    "    (\"TransE\", \"proposed_by\", \"Bordes et al. 2013\"),\n",
    "    (\"GraphSAGE\", \"is_a\", \"graph neural network\"),\n",
    "    (\"GraphSAGE\", \"uses\", \"neighborhood sampling and aggregation\"),\n",
    "    (\"GraphSAGE\", \"property\", \"inductive (generalizes to unseen nodes)\"),\n",
    "    (\"RAGAS\", \"is_a\", \"evaluation framework\"),\n",
    "    (\"RAGAS\", \"measures\", \"faithfulness\"),\n",
    "    (\"RAGAS\", \"measures\", \"answer relevancy\"),\n",
    "    (\"RAGAS\", \"measures\", \"context precision\"),\n",
    "    (\"RAGAS\", \"detects\", \"hallucination\"),\n",
    "]\n",
    "\n",
    "def kg_to_text(entity, triples):\n",
    "    \"\"\"Convert KG triples about an entity to natural language context.\"\"\"\n",
    "    entity_triples = [(s, p, o) for s, p, o in triples if s.lower() == entity.lower()]\n",
    "    if not entity_triples:\n",
    "        return None\n",
    "    \n",
    "    sentences = []\n",
    "    for s, p, o in entity_triples:\n",
    "        # Convert triple to natural language\n",
    "        p_readable = p.replace(\"_\", \" \")\n",
    "        sentences.append(f\"{s} {p_readable} {o}.\")\n",
    "    \n",
    "    return f\"Knowledge Graph facts about {entity}: \" + \" \".join(sentences)\n",
    "\n",
    "def rag_with_kg(query, top_k=3):\n",
    "    \"\"\"RAG pipeline augmented with Knowledge Graph triples.\"\"\"\n",
    "    # Extract potential entities from query (simple keyword matching)\n",
    "    entities_found = []\n",
    "    for s, p, o in kg_triples:\n",
    "        if s.lower() in query.lower():\n",
    "            if s not in entities_found:\n",
    "                entities_found.append(s)\n",
    "    \n",
    "    # Get KG context\n",
    "    kg_context_parts = []\n",
    "    for entity in entities_found:\n",
    "        kg_text = kg_to_text(entity, kg_triples)\n",
    "        if kg_text:\n",
    "            kg_context_parts.append(kg_text)\n",
    "    \n",
    "    # Get document retrieval context\n",
    "    candidates = retrieve_hybrid(query, top_k=top_k)\n",
    "    candidates = rerank(query, candidates)\n",
    "    \n",
    "    # Build combined context\n",
    "    doc_context = \"\\n\\n\".join(\n",
    "        f\"[Source: {knowledge_base[idx]['source']}] {knowledge_base[idx]['title']}: {knowledge_base[idx]['text']}\"\n",
    "        for idx, _ in candidates\n",
    "    )\n",
    "    kg_context = \"\\n\".join(kg_context_parts) if kg_context_parts else \"No KG triples found.\"\n",
    "    \n",
    "    return doc_context, kg_context, entities_found\n",
    "\n",
    "# Demonstrate KG-augmented RAG\n",
    "query = \"What is FAISS and how does it relate to RAG?\"\n",
    "doc_ctx, kg_ctx, entities = rag_with_kg(query)\n",
    "\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "print(f\"Entities detected: {entities}\")\n",
    "print(f\"\\n--- Knowledge Graph Context ---\")\n",
    "print(kg_ctx)\n",
    "print(f\"\\n--- Document Context (top passages) ---\")\n",
    "print(doc_ctx[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answer with KG-augmented RAG\n",
    "\n",
    "def rag_with_kg_answer(query, top_k=3, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Full KG-augmented RAG pipeline.\"\"\"\n",
    "    doc_ctx, kg_ctx, entities = rag_with_kg(query, top_k)\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are a helpful AI assistant specialized in Information Retrieval and Text Mining. \"\n",
    "        \"Answer based ONLY on the provided context (both document passages and knowledge graph facts). \"\n",
    "        \"If the context does not contain enough information, say so. \"\n",
    "        \"Always cite sources.\"\n",
    "    )\n",
    "    \n",
    "    user_message = f\"\"\"Based on the following context, answer the question.\n",
    "\n",
    "KNOWLEDGE GRAPH FACTS:\n",
    "{kg_ctx}\n",
    "\n",
    "DOCUMENT PASSAGES:\n",
    "{doc_ctx}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "ANSWER (cite sources, distinguish between KG facts and document passages):\"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Test KG-augmented RAG\n",
    "queries = [\n",
    "    \"What is FAISS and how does it relate to RAG?\",\n",
    "    \"How do TransE and GraphSAGE differ for knowledge graph embeddings?\",\n",
    "    \"How does RAGAS detect hallucinations?\",\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"{'='*65}\")\n",
    "    print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "    answer = rag_with_kg_answer(query)\n",
    "    print(f\"Answer:\\n{answer}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a12cf01",
   "metadata": {},
   "source": [
    "**Observation:** The KG-augmented RAG combines:\n",
    "1. **Structured facts** from the Knowledge Graph (precise, verified relationships)\n",
    "2. **Unstructured passages** from the document retrieval (detailed explanations)\n",
    "\n",
    "This provides the LLM with both precise factual anchors (from the KG) and rich contextual detail (from documents). In production systems like **Google's AI Overview**, this combination of structured (Knowledge Graph) and unstructured (web documents) retrieval is key to factual accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca3aa6e",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. RAG Fusion Methods: A Complete Overview\n",
    "\n",
    "The lecture presents six main methods for integrating retrieval with generation. Here is a comprehensive comparison:\n",
    "\n",
    "| Level | Method | How It Works | Vector Fusion? | Key Reference |\n",
    "|---|---|---|---|---|\n",
    "| **Prompt Level** (Early Fusion) | Text Concatenation | Retrieve text → append to prompt → generate | No | Lewis et al. 2020 (RAG) |\n",
    "| **Vector Level** (Embedding Fusion) | Query Expansion / Fusion | Expand query vector with retrieved doc vectors | **Yes** | Xiong 2020 (ANCE), Guu 2020 (REALM) |\n",
    "| **KG-Based** | KG Triples as Text or Embeddings | Convert KG triples to text or embed with TransE/GNN | **Yes** (if embedding) | Zhang 2021 (DKPLM) |\n",
    "| **Late Fusion** | Cross-Attention | Model attends to query + retrieved vectors during decoding | Learned via attention | Lewis 2020 (RAG-Token), Izacard 2020 (FiD) |\n",
    "| **Memory-Augmented** | kNN-LM, RETRO | Query external memory at each generation step; interpolate distributions | At probability level | Khandelwal 2020, Borgeaud 2022 |\n",
    "| **Dynamic Retrieval** | Retrieval during decoding | Re-retrieve as new tokens are generated | Via attention | Shi 2023 (DCR/REPLUG) |\n",
    "\n",
    "## Late Fusion: Cross-Attention during Decoding\n",
    "\n",
    "In late fusion, the model does **not** modify the query embedding before generation. Instead:\n",
    "1. At each decoding step, the token being generated has a decoder hidden state\n",
    "2. This hidden state **cross-attends** to all retrieved document representations\n",
    "3. The generation is influenced by attention over the retrieved passages\n",
    "\n",
    "This is how **Fusion-in-Decoder (FiD)** works: each passage is encoded separately, and the decoder fuses information dynamically.\n",
    "\n",
    "## Memory-Augmented Methods (kNN-LM)\n",
    "\n",
    "At each generation step:\n",
    "$$p_{\\text{final}} = \\lambda \\cdot p_{\\text{kNN}} + (1 - \\lambda) \\cdot p_{\\text{LM}}$$\n",
    "\n",
    "- $p_{\\text{LM}}$ = the language model's own probability distribution\n",
    "- $p_{\\text{kNN}}$ = distribution from nearest neighbor lookup in external datastore\n",
    "- $\\lambda$ = interpolation weight\n",
    "\n",
    "**Analogy:**\n",
    "- Memory-Augmented = like a **library** the model can visit anytime\n",
    "- Dynamic Retrieval = like a **personal librarian** handing over documents while the model writes each word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a0c39",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Building a Complete RAG Pipeline\n",
    "\n",
    "Let's build a more sophisticated RAG pipeline that includes:\n",
    "1. **Document chunking** with overlap\n",
    "2. **Hybrid retrieval** (BM25 + dense)\n",
    "3. **Cross-encoder re-ranking**\n",
    "4. **Context window management** (token counting)\n",
    "5. **Source citation** in generated answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bd42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Chunking with Overlap\n",
    "\n",
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    chunk_size and overlap are in words (not tokens).\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + chunk_size\n",
    "        chunk = \" \".join(words[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  # Overlap\n",
    "    return chunks\n",
    "\n",
    "# Demonstrate chunking on a longer document\n",
    "long_document = (\n",
    "    \"Retrieval-Augmented Generation (RAG) is a paradigm that enhances large language models \"\n",
    "    \"by integrating external knowledge retrieval into the generation process. The core idea is \"\n",
    "    \"that instead of relying solely on the parametric knowledge stored in the model's weights, \"\n",
    "    \"the system retrieves relevant documents from an external corpus and uses them as additional \"\n",
    "    \"context for generation. This approach was first formalized by Lewis et al. in 2020, who \"\n",
    "    \"proposed using a dense passage retriever (DPR) combined with a BART sequence-to-sequence \"\n",
    "    \"model. The retriever finds the most relevant passages from a large document collection, \"\n",
    "    \"and the generator produces an answer conditioned on both the query and the retrieved passages. \"\n",
    "    \"RAG has two variants: RAG-Sequence, where the same set of documents is used for the entire \"\n",
    "    \"generation, and RAG-Token, where different documents can be retrieved for each output token. \"\n",
    "    \"Since its introduction, RAG has become the standard approach for building factually grounded \"\n",
    "    \"AI systems. Modern implementations use FAISS for efficient vector search, sentence transformers \"\n",
    "    \"for embedding generation, and cross-encoders for re-ranking. The RAGAS framework provides \"\n",
    "    \"metrics for evaluating RAG pipeline quality, including faithfulness (detecting hallucinations), \"\n",
    "    \"answer relevancy, context precision, and context recall. Major companies like Google (AI Overview), \"\n",
    "    \"Microsoft (Bing Chat), and Perplexity AI all use RAG-based architectures for their conversational \"\n",
    "    \"search products. Recent advances include memory-augmented methods like kNN-LM, which interpolate \"\n",
    "    \"the model's output distribution with nearest-neighbor lookups in an external datastore, and \"\n",
    "    \"dynamic retrieval methods that re-retrieve as new tokens are generated.\"\n",
    ")\n",
    "\n",
    "chunks = chunk_text(long_document, chunk_size=50, overlap=10)\n",
    "\n",
    "print(f\"Document: {len(long_document.split())} words\")\n",
    "print(f\"Chunks: {len(chunks)} (size=50 words, overlap=10 words)\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    tokens = len(enc.encode(chunk))\n",
    "    print(f\"Chunk {i}: ({len(chunk.split())} words, {tokens} tokens)\")\n",
    "    print(f\"  {chunk[:120]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4624cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete RAG Pipeline with all components\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    A complete RAG pipeline demonstrating:\n",
    "    - Document chunking and indexing\n",
    "    - Hybrid retrieval (BM25 + dense)\n",
    "    - Cross-encoder re-ranking\n",
    "    - Context window management\n",
    "    - Source citation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, documents, embedding_model, cross_encoder, client,\n",
    "                 chunk_size=100, chunk_overlap=20):\n",
    "        self.client = client\n",
    "        self.embedding_model = embedding_model\n",
    "        self.cross_encoder = cross_encoder\n",
    "        self.enc = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "        \n",
    "        # Chunk documents\n",
    "        self.chunks = []\n",
    "        self.chunk_metadata = []\n",
    "        for doc in documents:\n",
    "            doc_chunks = chunk_text(doc[\"text\"], chunk_size, chunk_overlap)\n",
    "            for i, chunk in enumerate(doc_chunks):\n",
    "                self.chunks.append(chunk)\n",
    "                self.chunk_metadata.append({\n",
    "                    \"doc_id\": doc[\"id\"],\n",
    "                    \"title\": doc[\"title\"],\n",
    "                    \"source\": doc[\"source\"],\n",
    "                    \"chunk_idx\": i,\n",
    "                })\n",
    "        \n",
    "        # Build BM25 index\n",
    "        self.tokenized_chunks = [c.lower().split() for c in self.chunks]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_chunks)\n",
    "        \n",
    "        # Build FAISS index\n",
    "        self.chunk_embeddings = self.embedding_model.encode(\n",
    "            self.chunks, convert_to_numpy=True, show_progress_bar=False\n",
    "        )\n",
    "        dim = self.chunk_embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.chunk_embeddings_norm = self.chunk_embeddings.copy()\n",
    "        faiss.normalize_L2(self.chunk_embeddings_norm)\n",
    "        self.index.add(self.chunk_embeddings_norm)\n",
    "        \n",
    "        print(f\"RAG Pipeline initialized:\")\n",
    "        print(f\"  Documents: {len(documents)}\")\n",
    "        print(f\"  Chunks: {len(self.chunks)} (size={chunk_size}, overlap={chunk_overlap})\")\n",
    "        print(f\"  FAISS index: {self.index.ntotal} vectors, dim={dim}\")\n",
    "    \n",
    "    def retrieve(self, query, top_k=5, alpha=0.5):\n",
    "        \"\"\"Hybrid retrieval with BM25 + dense search.\"\"\"\n",
    "        # BM25\n",
    "        bm25_scores = self.bm25.get_scores(query.lower().split())\n",
    "        \n",
    "        # Dense\n",
    "        q_emb = self.embedding_model.encode([query], convert_to_numpy=True)\n",
    "        faiss.normalize_L2(q_emb)\n",
    "        dense_scores_raw, dense_ids = self.index.search(q_emb, len(self.chunks))\n",
    "        dense_scores = np.zeros(len(self.chunks))\n",
    "        for i, idx in enumerate(dense_ids[0]):\n",
    "            dense_scores[idx] = dense_scores_raw[0][i]\n",
    "        \n",
    "        # Normalize and fuse\n",
    "        scaler = MinMaxScaler()\n",
    "        bm25_norm = scaler.fit_transform(bm25_scores.reshape(-1, 1)).flatten()\n",
    "        dense_norm = scaler.fit_transform(dense_scores.reshape(-1, 1)).flatten()\n",
    "        hybrid = alpha * bm25_norm + (1 - alpha) * dense_norm\n",
    "        \n",
    "        top_indices = np.argsort(hybrid)[::-1][:top_k]\n",
    "        return [(int(idx), float(hybrid[idx])) for idx in top_indices]\n",
    "    \n",
    "    def rerank(self, query, candidates):\n",
    "        \"\"\"Cross-encoder re-ranking.\"\"\"\n",
    "        pairs = [(query, self.chunks[idx]) for idx, _ in candidates]\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        reranked = sorted(zip([idx for idx, _ in candidates], scores),\n",
    "                          key=lambda x: x[1], reverse=True)\n",
    "        return [(idx, float(s)) for idx, s in reranked]\n",
    "    \n",
    "    def answer(self, query, top_k=5, max_context_tokens=1500, model=\"gpt-4o-mini\"):\n",
    "        \"\"\"Full RAG: retrieve → rerank → prompt → generate.\"\"\"\n",
    "        # Retrieve and rerank\n",
    "        candidates = self.retrieve(query, top_k=top_k)\n",
    "        candidates = self.rerank(query, candidates)\n",
    "        \n",
    "        # Build context with token management\n",
    "        context_parts = []\n",
    "        sources_used = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for idx, score in candidates:\n",
    "            chunk_tokens = len(self.enc.encode(self.chunks[idx]))\n",
    "            if total_tokens + chunk_tokens > max_context_tokens:\n",
    "                break\n",
    "            context_parts.append(\n",
    "                f\"[{self.chunk_metadata[idx]['title']} — {self.chunk_metadata[idx]['source']}]: \"\n",
    "                f\"{self.chunks[idx]}\"\n",
    "            )\n",
    "            sources_used.append(self.chunk_metadata[idx])\n",
    "            total_tokens += chunk_tokens\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        system_prompt = (\n",
    "            \"You are a helpful AI assistant. Answer based ONLY on the provided context. \"\n",
    "            \"Cite sources in [brackets]. If unsure, say 'I don't have enough information.'\"\n",
    "        )\n",
    "        \n",
    "        user_msg = f\"CONTEXT:\\n{context}\\n\\nQUESTION: {query}\\n\\nANSWER:\"\n",
    "        \n",
    "        response = self.client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_msg}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=400,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"answer\": response.choices[0].message.content,\n",
    "            \"sources\": sources_used,\n",
    "            \"context_tokens\": total_tokens,\n",
    "            \"chunks_used\": len(context_parts),\n",
    "        }\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = RAGPipeline(\n",
    "    documents=knowledge_base,\n",
    "    embedding_model=embedding_model,\n",
    "    cross_encoder=cross_encoder,\n",
    "    client=client,\n",
    "    chunk_size=80,\n",
    "    chunk_overlap=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1b4d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the complete RAG pipeline\n",
    "\n",
    "test_queries = [\n",
    "    \"What is the difference between BM25 and dense retrieval?\",\n",
    "    \"How can knowledge graphs help prevent hallucination?\",\n",
    "    \"Explain the kNN-LM approach to memory-augmented generation.\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    result = pipeline.answer(query)\n",
    "    \n",
    "    print(f\"{'='*65}\")\n",
    "    print(f\"Query: \\\"{result['query']}\\\"\")\n",
    "    print(f\"Chunks used: {result['chunks_used']} | Context tokens: {result['context_tokens']}\")\n",
    "    print(f\"\\nAnswer:\\n{result['answer']}\")\n",
    "    print(f\"\\nSources:\")\n",
    "    for s in result['sources']:\n",
    "        print(f\"  - {s['title']} ({s['source']}, chunk {s['chunk_idx']})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee607c2",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Hallucination Detection with RAGAS Metrics\n",
    "\n",
    "**RAGAS** (Retrieval-Augmented Generation Assessment) is a framework for evaluating RAG pipelines. Its key metric for hallucination detection is **Faithfulness**:\n",
    "\n",
    "$$\\text{Faithfulness} = \\frac{\\text{Number of claims supported by context}}{\\text{Total number of claims in the answer}}$$\n",
    "\n",
    "## RAGAS Metrics Overview\n",
    "\n",
    "| Metric | What It Measures | Range |\n",
    "|---|---|---|\n",
    "| **Faithfulness** | Are all claims in the answer supported by the retrieved context? | 0–1 (1 = perfect) |\n",
    "| **Answer Relevancy** | Is the answer relevant to the question? | 0–1 |\n",
    "| **Context Precision** | Are retrieved passages actually relevant? | 0–1 |\n",
    "| **Context Recall** | Does the retrieved context cover all aspects of the ground truth? | 0–1 |\n",
    "\n",
    "## How Faithfulness Works\n",
    "\n",
    "1. **Claim Extraction:** Break the answer into individual factual claims\n",
    "2. **Context Verification:** Check if each claim is supported by the retrieved context\n",
    "3. **Score Calculation:** Fraction of supported claims\n",
    "\n",
    "### Example\n",
    "\n",
    "| Claim | Supported by Context? |\n",
    "|---|---|\n",
    "| \"BM25 uses term frequency\" | ✅ Yes |\n",
    "| \"BM25 was developed by Google\" | ❌ No (hallucination!) |\n",
    "| \"BM25 is used in Elasticsearch\" | ✅ Yes |\n",
    "\n",
    "Faithfulness = 2/3 = 0.67 → The answer contains a hallucination.\n",
    "\n",
    "Let's implement these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS-Style Evaluation Metrics\n",
    "\n",
    "def extract_claims(answer):\n",
    "    \"\"\"\n",
    "    Simple claim extraction: split answer into sentences.\n",
    "    In production RAGAS, an LLM performs this step.\n",
    "    \"\"\"\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]+', answer)\n",
    "    claims = [s.strip() for s in sentences if len(s.strip()) > 10]\n",
    "    return claims\n",
    "\n",
    "def faithfulness_score(answer, context, model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Measure faithfulness: fraction of answer claims supported by context.\n",
    "    Uses embedding similarity as a proxy for NLI entailment.\n",
    "    \"\"\"\n",
    "    claims = extract_claims(answer)\n",
    "    if not claims:\n",
    "        return 1.0, []\n",
    "    \n",
    "    context_emb = model.encode([context])\n",
    "    claim_embs = model.encode(claims)\n",
    "    \n",
    "    similarities = cosine_similarity(claim_embs, context_emb).flatten()\n",
    "    \n",
    "    claim_results = []\n",
    "    supported = 0\n",
    "    for claim, sim in zip(claims, similarities):\n",
    "        is_supported = sim > threshold\n",
    "        if is_supported:\n",
    "            supported += 1\n",
    "        claim_results.append({\n",
    "            \"claim\": claim,\n",
    "            \"similarity\": float(sim),\n",
    "            \"supported\": is_supported,\n",
    "        })\n",
    "    \n",
    "    score = supported / len(claims)\n",
    "    return score, claim_results\n",
    "\n",
    "def answer_relevancy_score(question, answer, model):\n",
    "    \"\"\"Semantic similarity between question and answer.\"\"\"\n",
    "    embs = model.encode([question, answer])\n",
    "    return float(cosine_similarity([embs[0]], [embs[1]])[0][0])\n",
    "\n",
    "def context_precision_score(question, context_passages, model, threshold=0.35):\n",
    "    \"\"\"Fraction of retrieved passages that are relevant to the question.\"\"\"\n",
    "    q_emb = model.encode([question])\n",
    "    c_embs = model.encode(context_passages)\n",
    "    sims = cosine_similarity(q_emb, c_embs)[0]\n",
    "    relevant = sum(1 for s in sims if s > threshold)\n",
    "    return relevant / len(context_passages), sims\n",
    "\n",
    "# === Demonstration ===\n",
    "context_text = knowledge_base[0][\"text\"]  # BM25 passage\n",
    "\n",
    "# Good answer (grounded in context)\n",
    "good_answer = (\n",
    "    \"BM25 is a probabilistic retrieval function that ranks documents based on \"\n",
    "    \"term frequency and inverse document frequency. It uses parameters k1 and b \"\n",
    "    \"for term frequency saturation and document length normalization respectively. \"\n",
    "    \"BM25 is the default ranking function in Elasticsearch and Apache Solr.\"\n",
    ")\n",
    "\n",
    "# Hallucinated answer\n",
    "bad_answer = (\n",
    "    \"BM25 is a deep learning model developed by Google in 2019. \"\n",
    "    \"It uses transformer attention mechanisms to understand query semantics. \"\n",
    "    \"BM25 requires GPU training on large datasets and outputs dense vectors. \"\n",
    "    \"It is used in Elasticsearch for keyword search.\"\n",
    ")\n",
    "\n",
    "print(\"=== Faithfulness Evaluation ===\\n\")\n",
    "\n",
    "for label, answer in [(\"GOOD (grounded)\", good_answer), (\"BAD (hallucinated)\", bad_answer)]:\n",
    "    score, results = faithfulness_score(answer, context_text, embedding_model)\n",
    "    \n",
    "    print(f\"--- {label} ---\")\n",
    "    print(f\"Faithfulness score: {score:.2f}\\n\")\n",
    "    for r in results:\n",
    "        status = \"✅ Supported\" if r[\"supported\"] else \"❌ NOT supported\"\n",
    "        print(f\"  [{r['similarity']:.3f}] {status}\")\n",
    "        print(f\"    \\\"{r['claim']}\\\"\")\n",
    "    print()\n",
    "\n",
    "# Answer relevancy\n",
    "question = \"What is BM25 and how does it rank documents?\"\n",
    "rel_good = answer_relevancy_score(question, good_answer, embedding_model)\n",
    "rel_bad = answer_relevancy_score(question, bad_answer, embedding_model)\n",
    "print(f\"=== Answer Relevancy ===\")\n",
    "print(f\"  Good answer: {rel_good:.4f}\")\n",
    "print(f\"  Bad answer:  {rel_bad:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a01214",
   "metadata": {},
   "source": [
    "**Observation:** The faithfulness metric clearly distinguishes grounded answers from hallucinated ones:\n",
    "- The **good answer** has high faithfulness — all claims are supported by the BM25 context passage\n",
    "- The **bad answer** contains claims about \"deep learning\", \"transformer attention\", and \"GPU training\" that are NOT in the context → low faithfulness = hallucination detected!\n",
    "\n",
    "In production RAGAS implementations, an LLM performs the claim extraction and verification (using NLI — Natural Language Inference), which is more accurate than our simplified embedding similarity approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaff6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-to-End: RAG Pipeline + Hallucination Detection\n",
    "\n",
    "def evaluate_rag_answer(pipeline, query, ground_truth_doc_ids=None):\n",
    "    \"\"\"Run RAG pipeline and evaluate the answer for hallucination.\"\"\"\n",
    "    result = pipeline.answer(query)\n",
    "    \n",
    "    # Build context from what was actually retrieved\n",
    "    context_chunks = []\n",
    "    for s in result[\"sources\"]:\n",
    "        chunk_idx_in_pipeline = None\n",
    "        for i, meta in enumerate(pipeline.chunk_metadata):\n",
    "            if meta[\"doc_id\"] == s[\"doc_id\"] and meta[\"chunk_idx\"] == s[\"chunk_idx\"]:\n",
    "                chunk_idx_in_pipeline = i\n",
    "                break\n",
    "        if chunk_idx_in_pipeline is not None:\n",
    "            context_chunks.append(pipeline.chunks[chunk_idx_in_pipeline])\n",
    "    \n",
    "    full_context = \" \".join(context_chunks)\n",
    "    \n",
    "    # Faithfulness\n",
    "    faith, claim_details = faithfulness_score(\n",
    "        result[\"answer\"], full_context, embedding_model\n",
    "    )\n",
    "    \n",
    "    # Answer relevancy\n",
    "    relevancy = answer_relevancy_score(query, result[\"answer\"], embedding_model)\n",
    "    \n",
    "    # Context precision\n",
    "    ctx_prec, _ = context_precision_score(query, context_chunks, embedding_model)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"faithfulness\": faith,\n",
    "        \"answer_relevancy\": relevancy,\n",
    "        \"context_precision\": ctx_prec,\n",
    "        \"claims\": claim_details,\n",
    "        \"sources\": result[\"sources\"],\n",
    "    }\n",
    "\n",
    "# Evaluate multiple queries\n",
    "eval_queries = [\n",
    "    \"What is BM25 and how does it work?\",\n",
    "    \"How does the RAGAS framework evaluate RAG pipelines?\",\n",
    "    \"What is Fusion-in-Decoder?\",\n",
    "]\n",
    "\n",
    "print(\"=== End-to-End RAG Evaluation ===\\n\")\n",
    "for query in eval_queries:\n",
    "    eval_result = evaluate_rag_answer(pipeline, query)\n",
    "    \n",
    "    print(f\"Query: \\\"{eval_result['query']}\\\"\")\n",
    "    print(f\"  Faithfulness:      {eval_result['faithfulness']:.2f}\")\n",
    "    print(f\"  Answer Relevancy:  {eval_result['answer_relevancy']:.4f}\")\n",
    "    print(f\"  Context Precision: {eval_result['context_precision']:.2f}\")\n",
    "    \n",
    "    # Flag potential hallucinations\n",
    "    unsupported = [c for c in eval_result['claims'] if not c['supported']]\n",
    "    if unsupported:\n",
    "        print(f\"  ⚠ Potential hallucinations ({len(unsupported)} unsupported claims):\")\n",
    "        for c in unsupported:\n",
    "            print(f\"    ❌ \\\"{c['claim'][:80]}...\\\"\" if len(c['claim']) > 80 else f\"    ❌ \\\"{c['claim']}\\\"\")\n",
    "    else:\n",
    "        print(f\"  ✅ All claims appear supported by context\")\n",
    "    \n",
    "    print(f\"  Sources: {', '.join(s['title'] for s in eval_result['sources'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0592e23",
   "metadata": {},
   "source": [
    "---\n",
    "# 9. RAG Trade-offs: Benefits, Limitations & Computational Complexity\n",
    "\n",
    "## Benefits\n",
    "- **Factual grounding** — answers backed by retrieved evidence\n",
    "- **Explainability** — sources can be cited\n",
    "- **Domain adaptability** — swap the corpus for any domain\n",
    "- **No retraining** — update knowledge by updating the document store\n",
    "- **Smaller models, better results** — retrieval offloads knowledge from model weights\n",
    "\n",
    "## Limitations\n",
    "\n",
    "| Limitation | Description |\n",
    "|---|---|\n",
    "| **Retrieval bottleneck** | If the retriever misses key info, the LLM can't generate a complete answer |\n",
    "| **Restricted answer space** | RAG can only answer about what's in the indexed documents |\n",
    "| **Lexical bias** | Short queries may not retrieve all relevant passages |\n",
    "| **Added latency** | Multiple expensive steps (embedding, retrieval, re-ranking, generation) |\n",
    "| **Context window limits** | LLMs have token limits — can't fit all retrieved documents |\n",
    "\n",
    "## Computational Complexity\n",
    "\n",
    "| RAG Step | Method | Complexity |\n",
    "|---|---|---|\n",
    "| **Embedding** | Dense (BERT-based) | $O(n \\cdot d)$ per query |\n",
    "| **Retrieval** | Brute-force search | $O(N)$ — linear in corpus size |\n",
    "| **Retrieval** | ANN (FAISS with IVF) | $O(\\log N)$ — logarithmic |\n",
    "| **Re-ranking** | Cross-encoder | $O(k \\cdot n)$ — k candidates × sequence length |\n",
    "| **Generation** | LLM inference | $O(T \\cdot k)$ — T output tokens × k context passages |\n",
    "\n",
    "## Optimization Strategies\n",
    "\n",
    "| Strategy | How | Benefit |\n",
    "|---|---|---|\n",
    "| **Hybrid Retrieval** | BM25 + Dense + KG | Better recall across query types |\n",
    "| **Efficient ANN** | FAISS IVF, HNSW | $O(N) \\rightarrow O(\\log N)$ retrieval |\n",
    "| **Dynamic Truncation** | Token-aware context selection | Fit within context window |\n",
    "| **Adaptive Retrieval** | Fact queries → KG, Open queries → Dense | Right tool for each query type |\n",
    "| **Caching** | Cache embeddings and frequent queries | Reduce repeated computation |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f5029",
   "metadata": {},
   "source": [
    "---\n",
    "# 10. Applied RAG Pipeline: Sherlock Holmes Chatbot\n",
    "\n",
    "In Tutorials 03 and 07, you built a search engine and prepared a vector store, Knowledge Graph, atomic facts, and QA test sets from *The Adventures of Sherlock Holmes*. Now we bring everything together to build **a complete RAG chatbot** that can answer questions about the stories.\n",
    "\n",
    "**Data pipeline across tutorials:**\n",
    "```\n",
    "Tutorial 03 → custom_corpus/chunks/           (text chunks)\n",
    "Tutorial 07 → custom_corpus/vector_store/     (FAISS index + embeddings)\n",
    "Tutorial 07 → custom_corpus/kg/               (Knowledge Graph)\n",
    "Tutorial 07 → custom_corpus/evaluation/       (atomic facts + QA test set)\n",
    "Tutorial 11 → RAG chatbot + RAGAS evaluation  (this section)\n",
    "```\n",
    "\n",
    "## 10.1 Load All Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b27d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all artifacts produced by Tutorials 03 and 07\n",
    "import glob\n",
    "\n",
    "CORPUS_BASE = os.path.join(os.path.expanduser(\"~\"), \"custom_corpus\")\n",
    "VECTOR_DIR = os.path.join(CORPUS_BASE, \"vector_store\")\n",
    "KG_DIR = os.path.join(CORPUS_BASE, \"kg\")\n",
    "EVAL_DIR = os.path.join(CORPUS_BASE, \"evaluation\")\n",
    "\n",
    "# --- 1. Load chunk texts ---\n",
    "chunks_path = os.path.join(VECTOR_DIR, \"chunks.json\")\n",
    "if os.path.isfile(chunks_path):\n",
    "    with open(chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sherlock_chunks = json.load(f)\n",
    "else:\n",
    "    # Fallback: load from individual .txt files\n",
    "    chunk_files = sorted(glob.glob(os.path.join(CORPUS_BASE, \"chunks\", \"*.txt\")))\n",
    "    sherlock_chunks = []\n",
    "    for fpath in chunk_files:\n",
    "        with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().strip()\n",
    "            if len(text) > 20:\n",
    "                sherlock_chunks.append(text)\n",
    "\n",
    "print(f\"Loaded {len(sherlock_chunks)} Sherlock Holmes chunks\")\n",
    "\n",
    "# --- 2. Load FAISS index ---\n",
    "faiss_path = os.path.join(VECTOR_DIR, \"faiss_index.bin\")\n",
    "if os.path.isfile(faiss_path):\n",
    "    sherlock_index = faiss.read_index(faiss_path)\n",
    "    print(f\"FAISS index loaded: {sherlock_index.ntotal} vectors\")\n",
    "else:\n",
    "    print(\"WARNING: FAISS index not found. Re-encoding chunks...\")\n",
    "    embs = embedding_model.encode(sherlock_chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "    sherlock_index = faiss.IndexFlatIP(embs.shape[1])\n",
    "    faiss.normalize_L2(embs)\n",
    "    sherlock_index.add(embs)\n",
    "    print(f\"FAISS index rebuilt: {sherlock_index.ntotal} vectors\")\n",
    "\n",
    "# --- 3. Load embeddings ---\n",
    "emb_path = os.path.join(VECTOR_DIR, \"chunk_embeddings.npy\")\n",
    "if os.path.isfile(emb_path):\n",
    "    sherlock_embeddings = np.load(emb_path)\n",
    "    print(f\"Embeddings loaded: {sherlock_embeddings.shape}\")\n",
    "else:\n",
    "    sherlock_embeddings = embedding_model.encode(sherlock_chunks, convert_to_numpy=True)\n",
    "    print(f\"Embeddings recomputed: {sherlock_embeddings.shape}\")\n",
    "\n",
    "# --- 4. Load Knowledge Graph ---\n",
    "kg_path = os.path.join(KG_DIR, \"sherlock_kg.json\")\n",
    "if os.path.isfile(kg_path):\n",
    "    import networkx as nx\n",
    "    with open(kg_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        kg_data = json.load(f)\n",
    "    sherlock_kg = nx.Graph()\n",
    "    for node in kg_data[\"nodes\"]:\n",
    "        sherlock_kg.add_node(node[\"name\"], entity_type=node[\"entity_type\"], frequency=node[\"frequency\"])\n",
    "    for edge in kg_data[\"edges\"]:\n",
    "        sherlock_kg.add_edge(edge[\"source\"], edge[\"target\"], weight=edge[\"weight\"])\n",
    "    print(f\"Knowledge Graph loaded: {sherlock_kg.number_of_nodes()} nodes, {sherlock_kg.number_of_edges()} edges\")\n",
    "else:\n",
    "    sherlock_kg = None\n",
    "    print(\"WARNING: Knowledge Graph not found. KG-augmented RAG will be skipped.\")\n",
    "\n",
    "# --- 5. Load QA test set ---\n",
    "qa_path = os.path.join(EVAL_DIR, \"qa_test_set.json\")\n",
    "if os.path.isfile(qa_path):\n",
    "    with open(qa_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sherlock_qa = json.load(f)\n",
    "    print(f\"QA test set loaded: {len(sherlock_qa)} pairs\")\n",
    "else:\n",
    "    sherlock_qa = []\n",
    "    print(\"WARNING: QA test set not found. Run Tutorial 07 Sections 7-9 first.\")\n",
    "\n",
    "# --- 6. Load atomic facts ---\n",
    "facts_path = os.path.join(EVAL_DIR, \"atomic_facts.json\")\n",
    "if os.path.isfile(facts_path):\n",
    "    with open(facts_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        sherlock_facts = json.load(f)\n",
    "    print(f\"Atomic facts loaded: {len(sherlock_facts)} facts\")\n",
    "else:\n",
    "    sherlock_facts = []\n",
    "    print(\"WARNING: Atomic facts not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19316751",
   "metadata": {},
   "source": [
    "## 10.2 Build the Sherlock Holmes RAG Chatbot\n",
    "\n",
    "We build a RAG chatbot using the same components from Section 7, but now on a real corpus. The chatbot supports three retrieval modes:\n",
    "\n",
    "| Mode | Description |\n",
    "|---|---|\n",
    "| **No-RAG** | LLM answers from its own training data (no grounding) |\n",
    "| **BM25-RAG** | Keyword-based retrieval + LLM generation |\n",
    "| **Dense-RAG** | Semantic FAISS retrieval + cross-encoder reranking + LLM generation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c423f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build BM25 index over Sherlock Holmes chunks\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "sherlock_bm25_tokens = [c.lower().split() for c in sherlock_chunks]\n",
    "sherlock_bm25 = BM25Okapi(sherlock_bm25_tokens)\n",
    "print(f\"BM25 index built over {len(sherlock_chunks)} Sherlock Holmes chunks\")\n",
    "\n",
    "# --- Retrieval functions for the Sherlock corpus ---\n",
    "\n",
    "def sherlock_retrieve_dense(query, top_k=5):\n",
    "    \"\"\"Dense retrieval using FAISS.\"\"\"\n",
    "    q_emb = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, indices = sherlock_index.search(q_emb, top_k)\n",
    "    return [(int(idx), float(score)) for idx, score in zip(indices[0], scores[0])]\n",
    "\n",
    "def sherlock_retrieve_bm25(query, top_k=5):\n",
    "    \"\"\"BM25 keyword retrieval.\"\"\"\n",
    "    scores = sherlock_bm25.get_scores(query.lower().split())\n",
    "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(int(idx), float(scores[idx])) for idx in top_indices]\n",
    "\n",
    "def sherlock_rerank(query, candidates):\n",
    "    \"\"\"Re-rank candidates using cross-encoder.\"\"\"\n",
    "    pairs = [(query, sherlock_chunks[idx]) for idx, _ in candidates]\n",
    "    ce_scores = cross_encoder.predict(pairs)\n",
    "    reranked = sorted(zip([idx for idx, _ in candidates], ce_scores),\n",
    "                      key=lambda x: x[1], reverse=True)\n",
    "    return [(idx, float(s)) for idx, s in reranked]\n",
    "\n",
    "def sherlock_kg_context(query, kg, top_k_entities=3):\n",
    "    \"\"\"Get KG-based context by finding entities mentioned in the query.\"\"\"\n",
    "    if kg is None:\n",
    "        return \"\"\n",
    "    context_parts = []\n",
    "    for node in kg.nodes():\n",
    "        if node.lower() in query.lower():\n",
    "            # Get all neighbours of this entity\n",
    "            neighbors = list(kg.neighbors(node))\n",
    "            node_type = kg.nodes[node].get(\"entity_type\", \"?\")\n",
    "            if neighbors:\n",
    "                neighbor_str = \", \".join(neighbors[:10])\n",
    "                context_parts.append(\n",
    "                    f\"KG: {node} ({node_type}) is connected to: {neighbor_str}\"\n",
    "                )\n",
    "    return \"\\n\".join(context_parts[:top_k_entities])\n",
    "\n",
    "# --- The RAG Chatbot ---\n",
    "\n",
    "def sherlock_rag(query, mode=\"dense\", top_k=5, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Sherlock Holmes RAG chatbot.\n",
    "    \n",
    "    Modes: 'none' (no RAG), 'bm25', 'dense', 'dense+kg'\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    sources = []\n",
    "    \n",
    "    if mode == \"none\":\n",
    "        # No retrieval — LLM answers from its own knowledge\n",
    "        context = \"(No external context provided)\"\n",
    "    \n",
    "    elif mode == \"bm25\":\n",
    "        candidates = sherlock_retrieve_bm25(query, top_k)\n",
    "        sources = candidates\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"[Passage {i+1}]: {sherlock_chunks[idx][:500]}\"\n",
    "            for i, (idx, _) in enumerate(candidates)\n",
    "        )\n",
    "    \n",
    "    elif mode in (\"dense\", \"dense+kg\"):\n",
    "        candidates = sherlock_retrieve_dense(query, top_k)\n",
    "        candidates = sherlock_rerank(query, candidates)\n",
    "        sources = candidates\n",
    "        context = \"\\n\\n\".join(\n",
    "            f\"[Passage {i+1}]: {sherlock_chunks[idx][:500]}\"\n",
    "            for i, (idx, _) in enumerate(candidates)\n",
    "        )\n",
    "        \n",
    "        if mode == \"dense+kg\":\n",
    "            kg_ctx = sherlock_kg_context(query, sherlock_kg)\n",
    "            if kg_ctx:\n",
    "                context = f\"KNOWLEDGE GRAPH FACTS:\\n{kg_ctx}\\n\\nDOCUMENT PASSAGES:\\n{context}\"\n",
    "    \n",
    "    system_prompt = (\n",
    "        \"You are a literary assistant specializing in Sherlock Holmes stories by Arthur Conan Doyle. \"\n",
    "        \"Answer the user's question based ONLY on the provided context passages. \"\n",
    "        \"If the context doesn't contain the answer, say 'The provided passages don't contain this information.' \"\n",
    "        \"Always reference which passage(s) support your answer.\"\n",
    "    )\n",
    "    \n",
    "    user_msg = f\"CONTEXT:\\n{context}\\n\\nQUESTION: {query}\\n\\nANSWER:\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_msg}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "        max_tokens=400,\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"mode\": mode,\n",
    "        \"answer\": response.choices[0].message.content,\n",
    "        \"context\": context,\n",
    "        \"sources\": sources,\n",
    "    }\n",
    "\n",
    "print(\"Sherlock Holmes RAG chatbot ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4befb286",
   "metadata": {},
   "source": [
    "## 10.3 Compare RAG Modes: No-RAG vs BM25 vs Dense vs Dense+KG\n",
    "\n",
    "Let's ask the chatbot the same questions using different retrieval modes and see how grounding affects answer quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ff100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the four RAG modes on Sherlock Holmes questions\n",
    "\n",
    "sherlock_test_queries = [\n",
    "    \"Who is Irene Adler and what is her role in the story?\",\n",
    "    \"What is the significance of 221B Baker Street?\",\n",
    "    \"How does Holmes solve the mystery of the Red-Headed League?\",\n",
    "    \"What is the relationship between Holmes and Watson?\",\n",
    "]\n",
    "\n",
    "modes = [\"none\", \"bm25\", \"dense\", \"dense+kg\"]\n",
    "\n",
    "for query in sherlock_test_queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"QUERY: \\\"{query}\\\"\\n\")\n",
    "    \n",
    "    for mode in modes:\n",
    "        result = sherlock_rag(query, mode=mode)\n",
    "        answer_preview = result[\"answer\"][:250]\n",
    "        n_sources = len(result[\"sources\"])\n",
    "        print(f\"  [{mode.upper():10s}] ({n_sources} passages)\")\n",
    "        print(f\"  {answer_preview}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e04254",
   "metadata": {},
   "source": [
    "**Observation:** Notice how each mode differs:\n",
    "- **No-RAG**: The LLM uses its general knowledge of Sherlock Holmes — may hallucinate details or mix up stories\n",
    "- **BM25-RAG**: Finds passages with matching keywords — good for specific names and terms\n",
    "- **Dense-RAG**: Finds semantically similar passages — better for paraphrased queries  \n",
    "- **Dense+KG**: Adds structured entity relationships from the Knowledge Graph — provides extra factual anchoring\n",
    "\n",
    "The more grounding we provide, the more specific and verifiable the answers become."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26328f5b",
   "metadata": {},
   "source": [
    "---\n",
    "# 11. RAGAS Evaluation on Your Corpus\n",
    "\n",
    "Now we evaluate the Sherlock Holmes chatbot using the RAGAS metrics from Section 8. We use the QA test set and atomic facts generated in Tutorial 07.\n",
    "\n",
    "For each QA pair, we:\n",
    "1. **Retrieve** context using the dense pipeline\n",
    "2. **Generate** a RAG-grounded answer\n",
    "3. **Measure** faithfulness, answer relevancy, and context precision\n",
    "\n",
    "This gives us a quantitative view of how well our RAG pipeline performs on real data — and where it hallucinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a632777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAGAS evaluation of the Sherlock Holmes RAG chatbot\n",
    "# We reuse the faithfulness_score, answer_relevancy_score, context_precision_score\n",
    "# functions from Section 8.\n",
    "\n",
    "def evaluate_sherlock_rag(qa_pairs, mode=\"dense\", max_queries=20):\n",
    "    \"\"\"Run the RAG chatbot on QA pairs and evaluate with RAGAS metrics.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, qa in enumerate(qa_pairs[:max_queries]):\n",
    "        question = qa[\"question\"]\n",
    "        ground_truth = qa[\"ground_truth_answer\"]\n",
    "        gt_chunk_idx = qa.get(\"ground_truth_chunk_idx\", None)\n",
    "        \n",
    "        # Get RAG answer\n",
    "        rag_result = sherlock_rag(question, mode=mode, top_k=5)\n",
    "        answer = rag_result[\"answer\"]\n",
    "        \n",
    "        # Build context string from retrieved passages\n",
    "        context_text = rag_result[\"context\"]\n",
    "        \n",
    "        # Compute RAGAS metrics\n",
    "        faith, claim_details = faithfulness_score(answer, context_text, embedding_model, threshold=0.45)\n",
    "        relevancy = answer_relevancy_score(question, answer, embedding_model)\n",
    "        \n",
    "        # Context precision: did we retrieve the ground-truth chunk?\n",
    "        retrieved_indices = [idx for idx, _ in rag_result[\"sources\"]]\n",
    "        gt_retrieved = 1.0 if gt_chunk_idx in retrieved_indices else 0.0\n",
    "        \n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer[:200],\n",
    "            \"ground_truth\": ground_truth[:200],\n",
    "            \"faithfulness\": faith,\n",
    "            \"answer_relevancy\": relevancy,\n",
    "            \"gt_retrieved\": gt_retrieved,\n",
    "            \"n_unsupported\": len([c for c in claim_details if not c[\"supported\"]]),\n",
    "            \"n_claims\": len(claim_details),\n",
    "        })\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Evaluated {i + 1}/{min(len(qa_pairs), max_queries)} queries...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "if sherlock_qa:\n",
    "    print(f\"Evaluating {min(len(sherlock_qa), 20)} QA pairs with Dense-RAG mode...\\n\")\n",
    "    eval_results = evaluate_sherlock_rag(sherlock_qa, mode=\"dense\", max_queries=20)\n",
    "    \n",
    "    # Summary statistics\n",
    "    avg_faith = np.mean([r[\"faithfulness\"] for r in eval_results])\n",
    "    avg_relev = np.mean([r[\"answer_relevancy\"] for r in eval_results])\n",
    "    avg_gt_ret = np.mean([r[\"gt_retrieved\"] for r in eval_results])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RAGAS Evaluation Summary ({len(eval_results)} queries, mode=dense)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Average Faithfulness:      {avg_faith:.4f}\")\n",
    "    print(f\"  Average Answer Relevancy:  {avg_relev:.4f}\")\n",
    "    print(f\"  Ground-Truth Retrieval:    {avg_gt_ret:.2%}\")\n",
    "    print()\n",
    "    \n",
    "    # Show individual results\n",
    "    print(f\"{'Question':<50s} {'Faith':>6s} {'Relev':>6s} {'GT?':>4s}\")\n",
    "    print(\"-\" * 70)\n",
    "    for r in eval_results:\n",
    "        q_short = r[\"question\"][:48]\n",
    "        gt_mark = \"✅\" if r[\"gt_retrieved\"] else \"❌\"\n",
    "        print(f\"{q_short:<50s} {r['faithfulness']:>6.2f} {r['answer_relevancy']:>6.2f} {gt_mark:>4s}\")\n",
    "else:\n",
    "    print(\"No QA test set available. Please run Tutorial 07 Sections 7-9 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d15547",
   "metadata": {},
   "source": [
    "## 11.1 Compare RAG Modes on RAGAS Metrics\n",
    "\n",
    "Let's compare No-RAG, BM25-RAG, and Dense-RAG quantitatively. We expect Dense-RAG to have the highest faithfulness (least hallucination) because its answers are grounded in semantically relevant passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d442a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG modes on RAGAS metrics\n",
    "# We evaluate a smaller subset to keep API costs low\n",
    "\n",
    "EVAL_SUBSET = 10  # Number of QA pairs per mode\n",
    "\n",
    "if sherlock_qa and len(sherlock_qa) >= EVAL_SUBSET:\n",
    "    mode_results = {}\n",
    "    \n",
    "    for mode in [\"none\", \"bm25\", \"dense\"]:\n",
    "        print(f\"Evaluating mode: {mode}...\")\n",
    "        mode_results[mode] = evaluate_sherlock_rag(\n",
    "            sherlock_qa, mode=mode, max_queries=EVAL_SUBSET\n",
    "        )\n",
    "    \n",
    "    # Build comparison table\n",
    "    print(f\"\\n{'='*65}\")\n",
    "    print(f\"RAGAS Comparison Across RAG Modes ({EVAL_SUBSET} queries each)\")\n",
    "    print(f\"{'='*65}\")\n",
    "    print(f\"{'Mode':<12s} {'Faithfulness':>13s} {'Relevancy':>10s} {'GT Retrieved':>13s}\")\n",
    "    print(f\"{'-'*48}\")\n",
    "    \n",
    "    for mode, results in mode_results.items():\n",
    "        avg_f = np.mean([r[\"faithfulness\"] for r in results])\n",
    "        avg_r = np.mean([r[\"answer_relevancy\"] for r in results])\n",
    "        avg_g = np.mean([r[\"gt_retrieved\"] for r in results])\n",
    "        print(f\"{mode.upper():<12s} {avg_f:>13.4f} {avg_r:>10.4f} {avg_g:>12.2%}\")\n",
    "    \n",
    "    # Visualize comparison\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    modes_list = list(mode_results.keys())\n",
    "    colors = [\"#d62728\", \"#ff7f0e\", \"#2ca02c\"]\n",
    "    \n",
    "    for ax_idx, (metric, label) in enumerate([\n",
    "        (\"faithfulness\", \"Faithfulness\"),\n",
    "        (\"answer_relevancy\", \"Answer Relevancy\"),\n",
    "        (\"gt_retrieved\", \"Ground-Truth Retrieved\"),\n",
    "    ]):\n",
    "        values = [np.mean([r[metric] for r in mode_results[m]]) for m in modes_list]\n",
    "        bars = axes[ax_idx].bar(\n",
    "            [m.upper() for m in modes_list], values,\n",
    "            color=colors, alpha=0.8, edgecolor=\"black\"\n",
    "        )\n",
    "        axes[ax_idx].set_title(label, fontsize=13, fontweight=\"bold\")\n",
    "        axes[ax_idx].set_ylim(0, 1.05)\n",
    "        axes[ax_idx].axhline(y=0.5, color=\"gray\", linestyle=\"--\", alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, val in zip(bars, values):\n",
    "            axes[ax_idx].text(\n",
    "                bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n",
    "                f\"{val:.2f}\", ha=\"center\", va=\"bottom\", fontsize=11, fontweight=\"bold\"\n",
    "            )\n",
    "    \n",
    "    fig.suptitle(\"RAG Mode Comparison — RAGAS Metrics on Sherlock Holmes\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough QA pairs for comparison. Run Tutorial 07 Sections 7-9 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9148ac70",
   "metadata": {},
   "source": [
    "**Key Takeaways:**\n",
    "\n",
    "- **No-RAG** typically has **lower faithfulness** because the LLM generates answers from its training data, which may not match the specific passages in our corpus\n",
    "- **BM25-RAG** improves faithfulness by providing keyword-matched context, but may miss semantically relevant passages\n",
    "- **Dense-RAG** usually achieves the **highest faithfulness** because the semantic search finds the most relevant passages, reducing the LLM's tendency to hallucinate\n",
    "- **Ground-truth retrieval rate** tells us how often the correct source chunk actually appears in the retrieved context — this is effectively the **context recall** metric from RAGAS\n",
    "\n",
    "This demonstrates the fundamental value proposition of RAG: **external retrieval grounds the LLM's output in verifiable evidence**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791dd9f",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "| Concept | What We Learned |\n",
    "|---|---|\n",
    "| **Challenges** | LLMs hallucinate, have stale knowledge, no built-in fact-checking — RAG addresses all of these |\n",
    "| **RAG Pipeline** | Retrieve → Augment → Generate — the standard architecture for grounded AI |\n",
    "| **Prompt-Level RAG** | Inject retrieved text into prompts (early fusion) — simplest and most common |\n",
    "| **Vector-Level RAG** | Expand query vectors with retrieved doc embeddings — captures deeper semantics |\n",
    "| **Knowledge Graphs** | Structured facts via SPARQL, KG embeddings (TransE, GraphSAGE) — factual anchoring |\n",
    "| **Late Fusion** | Cross-attention during decoding (FiD) — dynamic information fusion |\n",
    "| **Memory-Augmented** | kNN-LM interpolates LM output with external memory lookups |\n",
    "| **Hallucination Detection** | RAGAS faithfulness metric: fraction of claims supported by context |\n",
    "| **Trade-offs** | RAG adds latency and is limited by retrieval quality, but provides factual grounding and citability |\n",
    "| **Applied RAG Chatbot** | Built a real chatbot on Sherlock Holmes using artifacts from Tutorials 03 & 07 |\n",
    "| **RAGAS on Real Data** | Compared No-RAG, BM25-RAG, and Dense-RAG — Dense-RAG shows highest faithfulness |\n",
    "\n",
    "### Key RAG Methods Hierarchy\n",
    "\n",
    "```\n",
    "RAG Methods\n",
    "├── Prompt Level (Early Fusion) ← Simple, most common\n",
    "│   └── Text concatenation, templates, selective injection\n",
    "├── Vector Level (Embedding Fusion)\n",
    "│   └── Query expansion, weighted averaging, learned MLP\n",
    "├── KG-Based\n",
    "│   └── TransE embeddings, GraphSAGE, SPARQL + LLM\n",
    "├── Late Fusion\n",
    "│   └── Cross-attention (FiD), re-ranking\n",
    "├── Memory-Augmented\n",
    "│   └── kNN-LM, RETRO\n",
    "└── Dynamic Retrieval\n",
    "    └── Re-retrieve during decoding (DCR/REPLUG)\n",
    "```\n",
    "\n",
    "### Cross-Tutorial Pipeline\n",
    "\n",
    "```\n",
    "Tutorial 03: Text → Chunks → Lucene Index → BM25 Search\n",
    "                 ↓\n",
    "Tutorial 07: Chunks → Knowledge Graph + FAISS Vector Store + Atomic Facts + QA Test Set\n",
    "                 ↓\n",
    "Tutorial 11: All artifacts → RAG Chatbot → RAGAS Evaluation → Hallucination Detection\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "- **Tutorial 12:** Agentic Approaches — agents with memory, tools, guardrails, multi-agent orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de18b2",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercises\n",
    "\n",
    "The following exercises are graded. Please provide your answers in the designated cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4fad6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "89ff0457c270139d77cbed261482fe9b",
     "grade": true,
     "grade_id": "solution_1",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 1 — RAG Fusion Methods Comparison (5 points)\n",
    "\n",
    "Compare **Prompt-Level RAG** (Early Fusion) and **Vector-Level RAG** (Embedding Fusion) as strategies for grounding LLM outputs. In your answer, address:\n",
    "\n",
    "1. How does each method inject retrieved knowledge into the generation process?\n",
    "2. What is the \"alignment problem\" in vector-level RAG and why does it matter?\n",
    "3. In what scenario would you choose prompt-level RAG over vector-level RAG, and vice versa?\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be5da6a",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab29ce9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "de19bff5b0690b4f5aa07262d797cef9",
     "grade": true,
     "grade_id": "solution_2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 2 — Hallucination Detection with RAGAS (5 points)\n",
    "\n",
    "A RAG pipeline retrieves the following context about FAISS:\n",
    "\n",
    "> \"FAISS (Facebook AI Similarity Search) is an open-source library for efficient similarity search. It supports various index types including flat, IVF, and HNSW for approximate nearest neighbor search. FAISS reduces retrieval complexity from O(N) to O(log N).\"\n",
    "\n",
    "The LLM generates this answer:\n",
    "\n",
    "> \"FAISS is a closed-source vector database developed by Google. It uses transformer-based indexing to achieve O(1) constant-time retrieval. FAISS is the only library that supports approximate nearest neighbor search, and it is primarily used for image classification tasks.\"\n",
    "\n",
    "Using the RAGAS faithfulness metric:\n",
    "\n",
    "1. Extract each factual claim from the generated answer.\n",
    "2. For each claim, determine whether it is supported, contradicted, or absent from the context.\n",
    "3. Calculate the faithfulness score.\n",
    "4. Explain what this score tells us about the answer quality.\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd2387",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c50a8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88a759f4721812ce168d36cbdb651920",
     "grade": true,
     "grade_id": "solution_3",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 3 — Implementing Retrieval Evaluation (10 points)\n",
    "\n",
    "Write code that evaluates the **retrieval quality** of three methods (BM25, Dense, Hybrid) on our knowledge base. Your code should:\n",
    "\n",
    "1. Use the retriever functions defined earlier (`retrieve_bm25`, `retrieve_dense`, `retrieve_hybrid`)\n",
    "2. For each query below, compute the **top-3** results from each retrieval method\n",
    "3. Compute **Mean Reciprocal Rank (MRR)** and **Precision@3** for each method\n",
    "4. Store results in: `mrr_bm25`, `mrr_dense`, `mrr_hybrid` (floats) and `p3_bm25`, `p3_dense`, `p3_hybrid` (floats)\n",
    "\n",
    "Use these query-answer pairs (value = index of the expected best document in `knowledge_base`):\n",
    "```python\n",
    "eval_queries = {\n",
    "    \"How does BM25 rank documents?\": 0,                  # BM25 Retrieval\n",
    "    \"What is retrieval augmented generation?\": 2,         # RAG Architecture\n",
    "    \"How do you evaluate RAG for hallucination?\": 5,      # RAGAS Evaluation\n",
    "    \"graph neural networks for knowledge embedding\": 11,  # GraphSAGE\n",
    "    \"memory augmented language model\": 10,                # kNN-LM\n",
    "}\n",
    "```\n",
    "\n",
    "**Precision@3** = fraction of queries where the expected document appears in the top-3 results.\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3674b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line with your solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd58b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell — do not modify\n",
    "assert 'mrr_bm25' in dir(), \"You need to define 'mrr_bm25'\"\n",
    "assert 'mrr_dense' in dir(), \"You need to define 'mrr_dense'\"\n",
    "assert 'mrr_hybrid' in dir(), \"You need to define 'mrr_hybrid'\"\n",
    "assert 'p3_bm25' in dir(), \"You need to define 'p3_bm25'\"\n",
    "assert 'p3_dense' in dir(), \"You need to define 'p3_dense'\"\n",
    "assert 'p3_hybrid' in dir(), \"You need to define 'p3_hybrid'\"\n",
    "assert isinstance(mrr_bm25, float), \"mrr_bm25 should be a float\"\n",
    "assert isinstance(mrr_dense, float), \"mrr_dense should be a float\"\n",
    "assert isinstance(mrr_hybrid, float), \"mrr_hybrid should be a float\"\n",
    "assert isinstance(p3_bm25, float), \"p3_bm25 should be a float\"\n",
    "assert isinstance(p3_dense, float), \"p3_dense should be a float\"\n",
    "assert isinstance(p3_hybrid, float), \"p3_hybrid should be a float\"\n",
    "assert 0 <= mrr_bm25 <= 1 and 0 <= mrr_dense <= 1 and 0 <= mrr_hybrid <= 1\n",
    "assert 0 <= p3_bm25 <= 1 and 0 <= p3_dense <= 1 and 0 <= p3_hybrid <= 1\n",
    "print(f\"{'Method':<12} {'MRR':>8} {'P@3':>8}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'BM25':<12} {mrr_bm25:>8.4f} {p3_bm25:>8.2f}\")\n",
    "print(f\"{'Dense':<12} {mrr_dense:>8.4f} {p3_dense:>8.2f}\")\n",
    "print(f\"{'Hybrid':<12} {mrr_hybrid:>8.4f} {p3_hybrid:>8.2f}\")\n",
    "print(f\"\\nAll auto-graded tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c0f75",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbe9cd5cf457a1b6bf4fc96f6a85f572",
     "grade": true,
     "grade_id": "solution_4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": true
    }
   },
   "source": [
    "## Exercise 4 — Build & Evaluate a RAG Chatbot on Your Own Corpus (15 points)\n",
    "\n",
    "In Tutorials 03 and 07 (Exercise 4), you built a search engine, Knowledge Graph, FAISS index, and QA test set over your own text corpus. Now build a **RAG chatbot** and evaluate it with RAGAS metrics.\n",
    "\n",
    "Your task:\n",
    "1. **Load your corpus data** — chunks, FAISS index, KG, and QA test set from your custom corpus (the data you created in Tutorials 03 and 07). *(2 pts)*\n",
    "2. **Build a RAG chatbot** — implement at least two modes: No-RAG and Dense-RAG. Use the `sherlock_rag()` function as a template. *(4 pts)*\n",
    "3. **Generate RAG answers** for at least 10 questions from your QA test set using both modes. *(3 pts)*\n",
    "4. **Evaluate with RAGAS** — compute faithfulness and answer relevancy for each mode. Store the average scores in `my_faith_norag`, `my_faith_dense`, `my_relev_norag`, `my_relev_dense` (floats). *(4 pts)*\n",
    "5. **Analyze** (in a markdown cell, minimum 100 words): What patterns do you observe? Does Dense-RAG always outperform No-RAG on faithfulness? Where does the pipeline fail? *(2 pts)*\n",
    "\n",
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea34c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE — Exercise 4\n",
    "# Follow the steps outlined above.\n",
    "# Store your results in: my_faith_norag, my_faith_dense, my_relev_norag, my_relev_dense\n",
    "\n",
    "raise NotImplementedError(\"Replace this line with your solution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb58704",
   "metadata": {},
   "source": [
    "YOUR ANALYSIS HERE (minimum 100 words on RAG vs No-RAG faithfulness patterns, pipeline failures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell — do not modify\n",
    "assert 'my_faith_norag' in dir(), \"You need to define 'my_faith_norag'\"\n",
    "assert 'my_faith_dense' in dir(), \"You need to define 'my_faith_dense'\"\n",
    "assert 'my_relev_norag' in dir(), \"You need to define 'my_relev_norag'\"\n",
    "assert 'my_relev_dense' in dir(), \"You need to define 'my_relev_dense'\"\n",
    "assert isinstance(my_faith_norag, float), \"my_faith_norag should be a float\"\n",
    "assert isinstance(my_faith_dense, float), \"my_faith_dense should be a float\"\n",
    "assert isinstance(my_relev_norag, float), \"my_relev_norag should be a float\"\n",
    "assert isinstance(my_relev_dense, float), \"my_relev_dense should be a float\"\n",
    "assert 0 <= my_faith_norag <= 1 and 0 <= my_faith_dense <= 1\n",
    "assert 0 <= my_relev_norag <= 1 and 0 <= my_relev_dense <= 1\n",
    "print(f\"{'Mode':<12s} {'Faithfulness':>13s} {'Relevancy':>10s}\")\n",
    "print(\"-\" * 37)\n",
    "print(f\"{'No-RAG':<12s} {my_faith_norag:>13.4f} {my_relev_norag:>10.4f}\")\n",
    "print(f\"{'Dense-RAG':<12s} {my_faith_dense:>13.4f} {my_relev_dense:>10.4f}\")\n",
    "print(f\"\\nAll auto-graded tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
