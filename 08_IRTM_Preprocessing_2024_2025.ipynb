{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Start by copying this into your Google Drive!!"
      ],
      "metadata": {
        "id": "PFE9HH4ULCdR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![maastricht-university-logo.jpeg](data:image/jpeg;base64,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)\n",
        "#Faculty of Science and Engineering - Department of Advanced Computer Sciences\n",
        "# Course Information Retrieval and Text Mining - Tutorial Preprocessing for IRTM"
      ],
      "metadata": {
        "id": "8S4ipRELHrMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By Jan Scholtes\n",
        "\n",
        "Version 2024-2025\n",
        "\n",
        "Welcome to the tutorial on grammar-based NLP. In this notebook you will learn how traditional, grammar-based approaches to deal with syntactic structures, semantics, co-reference & pronoun resolution, and negation handling as discussed in the course Syntax and Semantics.\n",
        "\n",
        "These methods have been used since the 1970's. In many aspects, they are limited and often also slow. The goal of this tutorial is to let you experience hands-on the challenges in NLP (especially dealing with ambiguity), but also with the limitations of these grammar-based approaches (e.g. not being able to deal with ambiguity, wrong spelling or wrong grammatical use of words or other unexpected situations). This will help you understand why statistical and deep-learning methods are so much better for many of the NLP tasks we discuss in this course.\n",
        "\n",
        "In this notebook, we will use the Stanford NLTK library.\n",
        "\n"
      ],
      "metadata": {
        "id": "Df_xXvSeHsxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to load these libaries, for later use:"
      ],
      "metadata": {
        "id": "37bw-BxAOIV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install requirements\n",
        "!pip install python-crfsuite\n",
        "!pip install sklearn-crfsuite"
      ],
      "metadata": {
        "id": "PeUAi6YeOLgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load NLTK"
      ],
      "metadata": {
        "id": "CB_nqIUFLPaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "JvVPpppcHohQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK comes with many corpora, toy grammars, trained models, etc. A complete list is posted at: https://www.nltk.org/nltk_data/\n",
        "\n",
        "To install the data, first install NLTK (see https://www.nltk.org/install.html), then use NLTK’s data downloader as described below.\n",
        "\n",
        "Apart from individual data packages, you can download the entire collection (using “all”), or just the data required for the examples and exercises in the book (using “book”), or just the corpora and no grammars or trained models (using “all-corpora”).\n",
        "\n",
        "The command below will start the interactive downloader, that also provides you the option to list all modules present in NLTK.\n",
        "\n",
        "Make sure to quit the interactive download in order to continue!"
      ],
      "metadata": {
        "id": "cGmUkekBMHJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "id": "vranMzadQIcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load most of the most important components we need today:"
      ],
      "metadata": {
        "id": "XRwKFovvMMJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('book')"
      ],
      "metadata": {
        "id": "dF8aWv8XMPmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "RlAs0KpsLSKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "oErFbBTKNMKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use Moby Dick for now and print the first 1000 chars of the book."
      ],
      "metadata": {
        "id": "jjZFuyVlk5ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_of_text = text1\n",
        "print(list_of_text[0:1000:1]) # print first 1000 chars"
      ],
      "metadata": {
        "id": "vJJupWcJlF06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's convert this list to a long string as this is what NLTK requires as input and this is what you would normally get form any preprocessing text-extraction process."
      ],
      "metadata": {
        "id": "hSO0lyB6nvcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RawTextMobyDick = \"\"\n",
        "for x in list_of_text:\n",
        "  RawTextMobyDick = RawTextMobyDick + \" \" + x\n",
        "print(RawTextMobyDick)"
      ],
      "metadata": {
        "id": "hYAn_7yFnypw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence Detection"
      ],
      "metadata": {
        "id": "RTX9xUgDMBum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we recognize sentences."
      ],
      "metadata": {
        "id": "d287EVN9mVNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
        "sentences = tokenizer.tokenize(RawTextMobyDick)\n",
        "i = 0\n",
        "for x in sentences:\n",
        "  print(x)\n",
        "  i += 1\n",
        "  if i == 30:   #limit sentences that are printed to 30\n",
        "    break\n"
      ],
      "metadata": {
        "id": "K6rNQphcme5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize"
      ],
      "metadata": {
        "id": "12HlVXm6IFam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we tokenize the text. We will leave the punctuation in as we need them later for the linguistic operations we plan to performs."
      ],
      "metadata": {
        "id": "cwI-ITipmRUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = nltk.tokenize.word_tokenize(RawTextMobyDick)\n",
        "i = 0\n",
        "for x in words:\n",
        "  print(x)\n",
        "  i += 1\n",
        "  if i == 50:   #limit words that are printed to 50\n",
        "    break"
      ],
      "metadata": {
        "id": "rqgkhLInr7VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming in NLTK (this is done by using rules)"
      ],
      "metadata": {
        "id": "0YXDOVMFLpN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('tagsets_json')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "i = 0\n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))\n",
        "    i += 1\n",
        "    if i == 100:\n",
        "      break"
      ],
      "metadata": {
        "id": "r-p0qIS-sgjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 1:\n",
        "We immediately observe the limits of stemming.\n",
        "1. Explain below\n",
        "2. Why is lemmatization better?\n",
        "3. Why do we need POS tagging for lemmatization?\n"
      ],
      "metadata": {
        "id": "BEy62sivs9-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "mm4OYyLmtPfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging"
      ],
      "metadata": {
        "id": "oCt6d7xtLdI4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In corpus linguistics, part-of-speech tagging (POS tagging or PoS tagging or POST), also called grammatical tagging is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,based on both its definition and its context. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.\n",
        "\n",
        "Schools commonly teach that there are 9 parts of speech in English: noun, verb, article, adjective, preposition, pronoun, adverb, conjunction, and interjection.\n",
        "\n",
        "However, there are clearly many more categories and sub-categories. For nouns, the plural, possessive, and singular forms can be distinguished. In many languages words are also marked for their \"case\" (role as subject, object, etc.), grammatical gender, and so on; while verbs are marked for tense, aspect, and other things. In some tagging systems, different inflections of the same root word will get different parts of speech, resulting in a large number of tags. For example, NN for singular common nouns, NNS for plural common nouns, NP for singular proper nouns. Other tagging systems use a smaller number of tags and ignore fine differences or model them as features somewhat independent from part-of-speech."
      ],
      "metadata": {
        "id": "cJ1w4PkmLcwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In linguistics, a treebank is a parsed text corpus that annotates syntactic or semantic sentence structure. The construction of parsed corpora in the early 1990s revolutionized computational linguistics, which benefitted from large-scale empirical data.\n",
        "\n",
        "The Penn-Tree Bank is a well known annotated corpus. The Penn Treebank (PTB) project selected 2,499 stories from a three year Wall Street Journal (WSJ) collection of 98,732 stories for syntactic annotation. It contains various syntactic tags, among them Part-of-Speech tags. The Penn-Tree bank is included in NLTK.\n",
        "\n",
        "The following function can be called to view a list of all possible part-of-speech tags. The extensive list includes PoS tags such as VB (verb in base form), VBD (verb in past tense), VBG (verb as present participle) and so on."
      ],
      "metadata": {
        "id": "WrpEVxioJzDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "MGNjVGaoJw4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try POS tagging on our example text. For this, we can either use a list of sentences or a long lists of words. In our example we will use the list of words."
      ],
      "metadata": {
        "id": "X7W4FCWPGzLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "words_with_pos = nltk.pos_tag(words)\n",
        "i = 0\n",
        "for w in words_with_pos:\n",
        "    print(w)\n",
        "    i += 1\n",
        "    if i == 100:\n",
        "      break"
      ],
      "metadata": {
        "id": "KmLBP3L9Gzmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Better Lemmatization with POS tags"
      ],
      "metadata": {
        "id": "ZjEw3qxNFwpU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we discussed last week: Stemming algorithms just remove a suffix or prefix from a word. This results in many non-linguistic words.\n",
        "\n",
        "Lemmatization takes more linguistic knowledge into consideration and implements a so-called morphological analysis of the words. It returns the \"lemma\" which is the base form of all its inflectional forms. For this, it needs to know the grammatical role of a word. We can use the Part-of-Speech tag for this."
      ],
      "metadata": {
        "id": "K9xhEzWewd7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that the stemming function takes different input than the POS tags output above. E.g. you need to include the syntactic role when you call the function. In this, it only accepts 'v' for all VERB types, 'a' for adjectives, 'n' for all nouns. So you need to capture the above NNP, NNS, and NN and convert them to 'n'when you call the stemming function form NLTK. Try words like \"better\" and \"worse\", you will see how they are brought back to their base form."
      ],
      "metadata": {
        "id": "1MfVFYb-O31a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2:\n",
        "Now can you write the code below for a better lemmatizer where you use both the original word and the POS tag."
      ],
      "metadata": {
        "id": "3IY2pwkiwDzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOU CODE HERE"
      ],
      "metadata": {
        "id": "BBRh-coxwOEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What do observe compared with Stemming?"
      ],
      "metadata": {
        "id": "dQuLgU9cxN91"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also write some code to extract all Noun Phrases"
      ],
      "metadata": {
        "id": "MwbcDywY9GLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "sentence = str(sentences[4]) # we will take one sentence from the list of sentences\n",
        "\n",
        "print(sentence)\n",
        "sentence_words = nltk.tokenize.word_tokenize(sentence)\n",
        "pos_sentence = nltk.pos_tag(sentence_words)\n",
        "\n",
        "for w in pos_sentence:\n",
        "    if w[1]=='NN':\n",
        "      print(w)\n",
        "    if w[1]=='NNS':\n",
        "      print(w)\n"
      ],
      "metadata": {
        "id": "UdYWS2fH9R4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phrase Detection"
      ],
      "metadata": {
        "id": "bujVoL9zL3eG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In sentences, there is a higher syntactic structure above that of POS-tages. The most common ones are Noun Phrases and Verb Phrases. Detecting these can be very usefull for various linguistic operations where we need to understand the beginning and the end of a NOUN or to understand if we are dealing with SINGULAR or PLURAL NOUNS. Let's first detect a NOUN PHRASE"
      ],
      "metadata": {
        "id": "MiutjBp0x1j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "\n",
        "sentence = str(sentences[3]) # we will take one sentence from the list of sentences\n",
        "sentence = \"We can't deal with this\"\n",
        "sentence = sentence.replace('\"','')\n",
        "print(sentence)\n",
        "sentence_words = nltk.tokenize.word_tokenize(sentence)\n",
        "pos_sentence = nltk.pos_tag(sentence_words)\n",
        "\n",
        "i = 0\n",
        "for w in pos_sentence:\n",
        "    print(w)\n",
        "    i += 1\n",
        "    if i == 100:\n",
        "      break\n",
        "\n",
        "grammar = 'NP: {<DT>?<JJ>*<NN>}'  # now define what we are looking for in NP's\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "result = cp.parse(pos_sentence)\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "ISKkzyGFyq_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical methods for POS Tagging\n",
        "\n",
        "Hidden Markov Models (HMMs) largely used to assign the correct label sequence to sequential data or assess the probability of a given label and data sequence. These models are finite state machines characterised by a number of states, transitions between these states, and output symbols emitted while in each state. The HMM is an extension to the Markov chain, where each state corresponds deterministically to a given event. In the HMM the observation is a probabilistic function of the state. HMMs share the Markov chain's assumption, being that the probability of transition from one state to another only depends on the current state - i.e. the series of states that led to the current state are not used. They are also time invariant.\n",
        "\n",
        "The HMM is a directed graph, with probability weighted edges (representing the probability of a transition between the source and sink states) where each vertex emits an output symbol when entered. The symbol (or observation) is non-deterministically generated. For this reason, knowing that a sequence of output observations was generated by a given HMM does not mean that the corresponding sequence of states (and what the current state is) is known. This is the 'hidden' in the hidden markov model.\n",
        "\n",
        "Formally, a HMM can be characterised by:\n",
        "\n",
        "* the output observation alphabet. This is the set of symbols which may be observed as output of the system.\n",
        "* the set of states.\n",
        "* the transition probabilities a_{ij} = P(s_t = j | s_{t-1} = i). These represent the probability of transition to each state from a given state.\n",
        "* the output probability matrix b_i(k) = P(X_t = o_k | s_t = i). These represent the probability of observing each symbol in a given state.\n",
        "* the initial state distribution. This gives the probability of starting in each state.\n",
        "\n",
        "To ground this discussion, take a common NLP application, part-of-speech (POS) tagging. An HMM is desirable for this task as the highest probability tag sequence can be calculated for a given sequence of word forms. This differs from other tagging techniques which often tag each word individually, seeking to optimise each individual tagging greedily without regard to the optimal combination of tags for a larger unit, such as a sentence. The HMM does this with the Viterbi algorithm, which efficiently computes the optimal path through the graph given the sequence of words forms.\n",
        "\n",
        "In POS tagging the states usually have a 1:1 correspondence with the tag alphabet - i.e. each state represents a single tag. The output observation alphabet is the set of word forms (the lexicon), and the remaining three parameters are derived by a training regime. With this information the probability of a given sentence can be easily derived, by simply summing the probability of each distinct path through the model. Similarly, the highest probability tagging sequence can be derived with the Viterbi algorithm, yielding a state sequence which can be mapped into a tag sequence.\n",
        "\n",
        "This discussion assumes that the HMM has been trained. This is probably the most difficult task with the model, and requires either Maximum Likelihood Estimation (MLE) estimates of the parameters or unsupervised learning using the Baum-Welch algorithm, a variant of Entropy Modeling (EM).\n",
        "\n",
        "Let's start training a HMM model and takeit from there...\n",
        "\n"
      ],
      "metadata": {
        "id": "WKFNUKxq7scH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hidden Markov Models\n",
        "\n",
        "Training a HMM model for POS tagging.\n",
        "\n",
        "First import probability FreqDist\n",
        "\n",
        "We will use the Penn treebank corpus in the NLTK data to train the HMM tagger. To import the treebank use the following code:"
      ],
      "metadata": {
        "id": "SRtep1G48ZXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('treebank')\n",
        "from nltk.corpus import treebank"
      ],
      "metadata": {
        "id": "12BSgaLS9G3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "4raYgOX-9LpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the distribution of tags in the corpus"
      ],
      "metadata": {
        "id": "CO_ZrYJj9NL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fd = FreqDist()\n",
        "for word, tag in treebank.tagged_words():\n",
        "    fd[tag] += 1\n",
        "fd.items()"
      ],
      "metadata": {
        "id": "SpUIRAaM9RGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to import the HMM module as well, using the following code:"
      ],
      "metadata": {
        "id": "G_r9tzFt9XwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import hmm"
      ],
      "metadata": {
        "id": "KB_igBlR9vFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can instantiate a HMM-Trainer object and assign it to a trainer variable using:"
      ],
      "metadata": {
        "id": "LL_PnnNCSy5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = hmm.HiddenMarkovModelTrainer()"
      ],
      "metadata": {
        "id": "vDL-LtnGvMsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can investigate the tagged words in the corpus:"
      ],
      "metadata": {
        "id": "kyn8Y1woUOaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "treebank.tagged_words()[:10]"
      ],
      "metadata": {
        "id": "c9Xgq9KLSLNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function returns the first two tagged sentences from the corpus:"
      ],
      "metadata": {
        "id": "_w5lT7VZS1iv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "treebank.tagged_sents()[:2]\n"
      ],
      "metadata": {
        "id": "XboYtIvevPfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total tagged sentences: \")\n",
        "print(len(treebank.tagged_sents()))"
      ],
      "metadata": {
        "id": "MLdO_RFYG1mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised training maximising the joint probability of the symbol and\n",
        "state sequences. This is done via collecting frequencies of\n",
        "transitions between states, symbol observations while within each\n",
        "state and which states start a sentence. These frequency distributions\n",
        "are then normalised into probability estimates, which can be\n",
        "smoothed if desired.\n",
        "\n",
        "It is also possible to train the HMM unsupervised. We will get back to that later in the Entropy Modeling section."
      ],
      "metadata": {
        "id": "tefVRX1DIlYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NLTK HMM-module offers supervised and unsupervised training methods. Here we train an HMM using a supervised (or Maximum Likelihood Estimate) method:"
      ],
      "metadata": {
        "id": "LDekbovaL2qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = trainer.train_supervised(treebank.tagged_sents())"
      ],
      "metadata": {
        "id": "ubU8BIDMvSea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is in the tagger?"
      ],
      "metadata": {
        "id": "xPUPYoZQc3Dt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagger"
      ],
      "metadata": {
        "id": "_428MbAmcykd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a HMM model for POS tagging. First load a tokenizer from NLTK"
      ],
      "metadata": {
        "id": "Bi-QoRSzubqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "13HNhMqtvXkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(\"Today is a good day.\")"
      ],
      "metadata": {
        "id": "egs2j9XlvZwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load the tokenized structure into the HMM tagger"
      ],
      "metadata": {
        "id": "GpnXhBtAZkKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(word_tokenize(\"Today is a good day. Yesterday was also a great day\"))"
      ],
      "metadata": {
        "id": "q8o6sqv6vcNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the probability of this sequence."
      ],
      "metadata": {
        "id": "TiLfegF1btmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.probability(tagger.tag(word_tokenize(\"Today is a good day. Yesterday was also a great day\")))"
      ],
      "metadata": {
        "id": "8O9gDpC9bj0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we discussed, probabilities can become very small, resulting in a floating point under flow. This is why we aften use log probability."
      ],
      "metadata": {
        "id": "T65yHSRUbw8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.log_probability(tagger.tag(word_tokenize(\"Today is a good day. Yesterday was also a great day\")))"
      ],
      "metadata": {
        "id": "Yv8GjgcVbGDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(word_tokenize(\"Jan Scholtes is a name that does not occur in the corpus. What do you observe? Can you explain?\"))"
      ],
      "metadata": {
        "id": "UfbWTfj0MIBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.probability(tagger.tag(word_tokenize(\"Jan Scholtes is a name that does not occur in the corpus. What do you observe? Can you explain?\")))"
      ],
      "metadata": {
        "id": "06UtKWLqb7gW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.log_probability(tagger.tag(word_tokenize(\"Jan Scholtes is a name that does not occur in the corpus. What do you observe? Can you explain?\")))"
      ],
      "metadata": {
        "id": "j2cHzWK-benV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional Random Fields"
      ],
      "metadata": {
        "id": "GNbYS8WuteMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the toolkit and tags\n",
        "from nltk.corpus import treebank"
      ],
      "metadata": {
        "id": "SDi8hq_UeGLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CRF module\n",
        "\n",
        "import pycrfsuite\n",
        "from nltk.tag import CRFTagger\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KhYjMJUoeLxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train data - pretagged\n",
        "train_data = treebank.tagged_sents()     #<same_as_previous>\n",
        "train_data[:2]"
      ],
      "metadata": {
        "id": "SzkOty9leOnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's re-check how many tagged sentences we have in our training data set:"
      ],
      "metadata": {
        "id": "nPAYMqhYtVgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total tagged sentences: \")\n",
        "print(len(train_data))\n"
      ],
      "metadata": {
        "id": "zvt-8xdQtVD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a trainer with default(None) values\n",
        "# Train with the data\n",
        "# This will take a few minutes...\n",
        "taggerCRF = CRFTagger(verbose=True)\n",
        "taggerCRF.train(train_data, 'model.crf.tagger')"
      ],
      "metadata": {
        "id": "wRgYTS26eWwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3: Conditional Random Fields (CRF)"
      ],
      "metadata": {
        "id": "JX7iEUjTy8Ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the function: taggerCRF._get_features(sentence_tokens) we can analyze the internal feature functions of the CRF parser. This question is about internal feature functions in the parser. If you check out the parser's documentation, you'll see a function that can create features for a tokenized text. In the comments of that function you'll see the internal feature functions of that parser. For this question, you'll need to create a tokenized sentence that uses these feature functions.\n",
        "\n",
        "a) Can you list the values of the feature functions for 5 training samples?\n",
        "\n",
        "\n",
        "\n",
        "b) Can you show the CRF value for 5 (random) training examples (from train_data).\n",
        "\n",
        "It is important to observe that even with a few feature functions, the model already performs as ood as it does.\n"
      ],
      "metadata": {
        "id": "tZus_8LxyimK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "FZL_9A4Ly-6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "YOUR CODE HERE"
      ],
      "metadata": {
        "id": "qCv2XG0FzA8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "7Ak5WHHDTFe3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try the same sentences we analyzed with the HMM with a CRF."
      ],
      "metadata": {
        "id": "Q8U9v_L9SHf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "taggerCRF.tag(word_tokenize(\"Today is a good day. Yesterday was also a great day\"))\n"
      ],
      "metadata": {
        "id": "DH0vVrkLel3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "taggerCRF.tag(word_tokenize(\"Jan Scholtes is a name that does not occur in the corpus. What do you observe? Can you explain?\"))"
      ],
      "metadata": {
        "id": "ppC2YErPwffn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4:"
      ],
      "metadata": {
        "id": "l1jnvmNiy3xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why does the CRF parser do a better job than the HMM parser for this sentence? It was trained on the same data as the HMM model and the words \"Jan Scholtes\" were still not part of the training data."
      ],
      "metadata": {
        "id": "WpvuPD6jyn__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "_iENYwMly0VR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deeper Syntactic Analysis"
      ],
      "metadata": {
        "id": "Skoo4P8ML6lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen in the lecture, the deep structure of a linguistic expression is a theoretical construct that seeks to unify several related structures.\n",
        "\n",
        "NP's and VP's are a first step to discover such deeper structures. But there are other methods as well. We will look into Dependency Grammars."
      ],
      "metadata": {
        "id": "dwgi_gQ1W1j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependency Grammar"
      ],
      "metadata": {
        "id": "foz04AX8MR6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency Parsing is the process to analyze the grammatical structure in a sentence and find out related words as well as the type of the relationship between them.\n",
        "Dependency grammars allow us to understand (long-term) relations between words and phrases in a sentence.\n",
        "\n",
        "Dependency grammars are well suited for co-referece and pronoun resolution and for negations handling.\n",
        "\n",
        "Let's take a closer look..."
      ],
      "metadata": {
        "id": "SRb9tj4hlCfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.grammar import DependencyGrammar\n",
        "from nltk.parse import (\n",
        "  DependencyGraph,\n",
        "  ProjectiveDependencyParser,\n",
        "  NonprojectiveDependencyParser,\n",
        "  )\n",
        "treebank_data = \"\"\"Pierre  NNP     2       NMOD\n",
        "Vinken  NNP     8       SUB\n",
        ",       ,       2       P\n",
        "61      CD      5       NMOD\n",
        "years   NNS     6       AMOD\n",
        "old     JJ      2       NMOD\n",
        ",       ,       2       P\n",
        "will    MD      0       ROOT\n",
        "join    VB      8       VC\n",
        "the     DT      11      NMOD\n",
        "board   NN      9       OBJ\n",
        "as      IN      9       VMOD\n",
        "a       DT      15      NMOD\n",
        "nonexecutive    JJ      15      NMOD\n",
        "director        NN      12      PMOD\n",
        "Nov.    NNP     9       VMOD\n",
        "29      CD      16      NMOD\n",
        ".       .       9       VMOD\n",
        "\"\"\"\n",
        "dg = DependencyGraph(treebank_data)\n",
        "dg.tree().pprint()\n"
      ],
      "metadata": {
        "id": "wvVUt-8BlYDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for head, rel, dep in dg.triples():\n",
        "    print(\n",
        "        '({h[0]}, {h[1]}), {r}, ({d[0]}, {d[1]})'\n",
        "        .format(h=head, r=rel, d=dep)\n",
        "    )"
      ],
      "metadata": {
        "id": "iHbzYEsNnq_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Co-reference and Pronoun Resolution"
      ],
      "metadata": {
        "id": "C7CH-q8oMr2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 5:\n",
        "Describe how a dependency grammar could be used for pronoun resolution. Explain with an example."
      ],
      "metadata": {
        "id": "ELTwH4pSoD29"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR EXPLANATION HERE"
      ],
      "metadata": {
        "id": "l98Aww5KoRpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Negation Handling"
      ],
      "metadata": {
        "id": "EtUKFfxqMwxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 6:\n",
        "Describe how a dependency grammar could be used for the scope detection in negation handling. Explain with an example."
      ],
      "metadata": {
        "id": "q5JbduApoVez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "QOOFV4oI3f67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Submission\n",
        "Please share your Colab notebook by clicking File on the top-left corner. Click under Download on Download .ipynb and upload that file to Canvas."
      ],
      "metadata": {
        "id": "I7zIiCpoph2r"
      }
    }
  ]
}