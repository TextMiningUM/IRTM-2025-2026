{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ff5dd7",
   "metadata": {},
   "source": [
    "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
    "\n",
    "# Information Retrieval and Text Mining Course\n",
    "## Tutorial 09 — Detecting Patterns and Organizing Text: Clustering, Topic Modeling, and Anomaly Detection (Part 2)\n",
    "\n",
    "**Author:** Jan Scholtes\n",
    "\n",
    "**Edition 2025-2026**\n",
    "\n",
    "Department of Advanced Computer Sciences — Maastricht University"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a354ea37",
   "metadata": {},
   "source": [
    "Welcome to Tutorial 09 on **Unsupervised Text Analysis**. While Tutorial 08 covered *supervised* classification (where we have labeled training data), this tutorial focuses on *unsupervised* methods that discover structure in text without labels. The topics covered are:\n",
    "\n",
    "1. **Text Clustering with K-Means** — partitioning documents into groups based on TF-IDF similarity.\n",
    "2. **Hierarchical Agglomerative Clustering (HAC)** — building dendrograms with different linkage methods.\n",
    "3. **Topic Modeling with LSA** — Latent Semantic Analysis using Singular Value Decomposition (SVD).\n",
    "4. **Topic Modeling with LDA** — Latent Dirichlet Allocation, a probabilistic generative model.\n",
    "5. **Topic Modeling with NMF** — Non-Negative Matrix Factorization with TF-IDF.\n",
    "6. **Comparing LSA vs LDA vs NMF** — coherence scores and interpretability.\n",
    "7. **BERTopic** — modern topic modeling combining BERT embeddings, UMAP, and HDBSCAN.\n",
    "8. **Anomaly Detection in Text** — detecting unusual text patterns and code-word detection with BERT MLM.\n",
    "\n",
    "At the end you will find the **Exercises** section with graded assignments.\n",
    "\n",
    "> **Note:** This course is about Information Retrieval, Text Mining, and Conversational Search — not about programming skills. The code cells below show you *how* these methods work in practice using Python libraries. Focus on understanding the **concepts** and **results**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e2e85",
   "metadata": {},
   "source": [
    "## Library Installation\n",
    "\n",
    "We install all required packages in a single cell. Run this cell once at the beginning of your session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a21ef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    \"gensim\",\n",
    "    \"pyLDAvis\",\n",
    "    \"bertopic\",\n",
    "    \"umap-learn\",\n",
    "    \"hdbscan\",\n",
    "    \"plotly\",\n",
    "]\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da7ac3",
   "metadata": {},
   "source": [
    "## Library Imports\n",
    "\n",
    "All imports are grouped here so the notebook is easy to set up and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Data & visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.decomposition import TruncatedSVD, NMF, LatentDirichletAllocation\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel, LdaModel, CoherenceModel\n",
    "\n",
    "# scipy (for dendrograms)\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# PyTorch & Transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"All libraries loaded successfully.\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e40a85",
   "metadata": {},
   "source": [
    "---\n",
    "# Dataset: The 20 Newsgroups Corpus\n",
    "\n",
    "We use the **20 Newsgroups** dataset, a classic text classification benchmark. It contains ~18,000 newsgroup posts across 20 topics. For clustering and topic modeling, we select a subset of 5 categories to make the results more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset of 20 Newsgroups for clarity\n",
    "categories = ['rec.sport.baseball', 'sci.space', 'comp.graphics', 'talk.politics.mideast', 'rec.autos']\n",
    "\n",
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='all',\n",
    "    categories=categories,\n",
    "    remove=('headers', 'footers', 'quotes'),  # remove metadata for cleaner text\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(newsgroups.data)} documents across {len(categories)} categories\")\n",
    "print(f\"Categories: {newsgroups.target_names}\")\n",
    "\n",
    "# Show a sample document\n",
    "print(f\"\\n--- Sample document (category: {newsgroups.target_names[newsgroups.target[0]]}) ---\")\n",
    "print(newsgroups.data[0][:300], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af44f630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF representation of the corpus\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.95,         # ignore terms appearing in >95% of documents\n",
    "    min_df=2,            # ignore terms appearing in fewer than 2 documents\n",
    "    max_features=5000,   # keep top 5000 features\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(newsgroups.data)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "print(f\"  → {tfidf_matrix.shape[0]} documents, {tfidf_matrix.shape[1]} features\")\n",
    "print(f\"  → Sparsity: {100 * (1 - tfidf_matrix.nnz / (tfidf_matrix.shape[0] * tfidf_matrix.shape[1])):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be542917",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Text Clustering with K-Means\n",
    "\n",
    "**Clustering** is the task of grouping objects into classes of similar objects — it is a form of **unsupervised learning** since we do not use labeled training data.\n",
    "\n",
    "**Key issues in text clustering:**\n",
    "- **Document representation**: how do we represent each document? (bag-of-words, TF-IDF, embeddings)\n",
    "- **Similarity measure**: how do we measure similarity between documents? (cosine similarity, Euclidean distance)\n",
    "- **Number of clusters K**: how many clusters should we use?\n",
    "\n",
    "**K-Means algorithm:**\n",
    "1. Select K random documents as initial cluster centroids (seeds)\n",
    "2. Assign each document to the nearest centroid\n",
    "3. Recompute each centroid as the mean of all documents assigned to it\n",
    "4. Repeat steps 2–3 until convergence\n",
    "\n",
    "K-Means is a special case of the **Expectation-Maximization (EM)** algorithm. Its time complexity is $O(IKNM)$ where $I$ = iterations, $K$ = clusters, $N$ = documents, $M$ = dimensions.\n",
    "\n",
    "**Sensitivity to seed choice:** Different random initializations can lead to different clustering results. The `n_init` parameter in scikit-learn runs the algorithm multiple times with different seeds and keeps the best result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35510b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means clustering\n",
    "num_clusters = 5  # we know there are 5 categories\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=SEED, n_init=10, max_iter=300)\n",
    "kmeans_labels = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# Evaluate clustering quality\n",
    "sil_score = silhouette_score(tfidf_matrix, kmeans_labels)\n",
    "ari_score = adjusted_rand_score(newsgroups.target, kmeans_labels)\n",
    "\n",
    "print(f\"K-Means Clustering Results (K={num_clusters}):\")\n",
    "print(f\"  Silhouette Score: {sil_score:.3f}  (range: -1 to 1, higher = better separation)\")\n",
    "print(f\"  Adjusted Rand Index: {ari_score:.3f}  (1.0 = perfect match with true labels)\")\n",
    "print(f\"\\nCluster sizes: {np.bincount(kmeans_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ed595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top terms per cluster (the \"representative words\")\n",
    "print(\"Top 10 terms per cluster:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    top_terms = [feature_names[ind] for ind in order_centroids[i, :10]]\n",
    "    print(f\"  Cluster {i}: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63832675",
   "metadata": {},
   "source": [
    "## 1.1 Choosing K: How Many Clusters?\n",
    "\n",
    "In practice, we often do not know the number of clusters in advance. A common approach is to try different values of K and measure the quality of the clustering. Two useful metrics:\n",
    "\n",
    "- **Silhouette Score**: measures how similar a document is to its own cluster compared to other clusters (higher is better)\n",
    "- **Sum of Squared Distances (SSE / Inertia)**: the total within-cluster sum of squared distances to the centroid (the \"elbow method\" — look for where the curve bends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37f14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal K using the elbow method and silhouette scores\n",
    "K_range = range(2, 12)\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "\n",
    "for k in K_range:\n",
    "    km = KMeans(n_clusters=k, random_state=SEED, n_init=10)\n",
    "    labels = km.fit_predict(tfidf_matrix)\n",
    "    inertias.append(km.inertia_)\n",
    "    silhouettes.append(silhouette_score(tfidf_matrix, labels))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax1.plot(K_range, inertias, 'bo-')\n",
    "ax1.set_xlabel('Number of clusters K')\n",
    "ax1.set_ylabel('Inertia (SSE)')\n",
    "ax1.set_title('Elbow Method')\n",
    "ax1.axvline(x=5, color='r', linestyle='--', alpha=0.7, label='K=5 (true)')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(K_range, silhouettes, 'go-')\n",
    "ax2.set_xlabel('Number of clusters K')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis')\n",
    "ax2.axvline(x=5, color='r', linestyle='--', alpha=0.7, label='K=5 (true)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best silhouette score: K={K_range[np.argmax(silhouettes)]} (score={max(silhouettes):.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8597c53",
   "metadata": {},
   "source": [
    "## 1.2 Seed Sensitivity\n",
    "\n",
    "K-Means results depend on the initial random seed. Different initializations can produce different clusters. Let's demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate seed sensitivity\n",
    "print(\"K-Means with different random seeds (K=5, single init each):\")\n",
    "print(\"-\" * 50)\n",
    "for seed in [0, 7, 42, 99, 123]:\n",
    "    km = KMeans(n_clusters=5, random_state=seed, n_init=1)\n",
    "    labels = km.fit_predict(tfidf_matrix)\n",
    "    ari = adjusted_rand_score(newsgroups.target, labels)\n",
    "    sil = silhouette_score(tfidf_matrix, labels)\n",
    "    print(f\"  Seed {seed:3d}: ARI={ari:.3f}, Silhouette={sil:.3f}\")\n",
    "\n",
    "print(f\"\\nWith n_init=10 (best of 10 runs): ARI={ari_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609dd521",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Hierarchical Agglomerative Clustering (HAC)\n",
    "\n",
    "Unlike K-Means (a **flat** algorithm), **Hierarchical Agglomerative Clustering (HAC)** builds a tree of clusters (a **dendrogram**) by iteratively merging the most similar clusters:\n",
    "\n",
    "1. Start with each document as its own cluster\n",
    "2. Find the two most similar clusters\n",
    "3. Merge them into a single cluster\n",
    "4. Repeat until only one cluster remains\n",
    "\n",
    "The key difference between HAC variants is the **linkage method** — how we define the similarity between two clusters:\n",
    "\n",
    "| Linkage | Definition | Characteristic |\n",
    "|---------|-----------|---------------|\n",
    "| **Single-link** | Maximum similarity between any pair | Produces \"straggly\" elongated clusters |\n",
    "| **Complete-link** | Minimum similarity between any pair | Produces tight, spherical clusters |\n",
    "| **Average-link** | Average similarity across all pairs | Compromise between single and complete |\n",
    "| **Ward** | Minimizes increase in total variance | Tends to produce equal-sized clusters |\n",
    "\n",
    "HAC complexity: $O(N^2)$ to $O(N^3)$ depending on implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4cec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAC with different linkage methods\n",
    "# Use a smaller subset for the dendrogram visualization (100 documents)\n",
    "np.random.seed(SEED)\n",
    "sample_idx = np.random.choice(len(newsgroups.data), size=100, replace=False)\n",
    "tfidf_sample = tfidf_matrix[sample_idx].toarray()\n",
    "true_labels_sample = newsgroups.target[sample_idx]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "for ax, method in zip(axes.ravel(), linkage_methods):\n",
    "    Z = linkage(tfidf_sample, method=method, metric='euclidean' if method == 'ward' else 'cosine')\n",
    "    dendrogram(Z, ax=ax, truncate_mode='level', p=5, no_labels=True,\n",
    "               color_threshold=0.7 * max(Z[:, 2]))\n",
    "    ax.set_title(f'{method.capitalize()} Linkage', fontsize=13)\n",
    "    ax.set_ylabel('Distance')\n",
    "\n",
    "plt.suptitle('Hierarchical Clustering Dendrograms (100-document sample)', fontsize=14, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c262aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linkage methods quantitatively on the full dataset\n",
    "print(\"HAC with different linkage methods (5 clusters):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for method in ['ward', 'complete', 'average']:\n",
    "    hac = AgglomerativeClustering(n_clusters=5, linkage=method)\n",
    "    hac_labels = hac.fit_predict(tfidf_matrix.toarray())\n",
    "    ari = adjusted_rand_score(newsgroups.target, hac_labels)\n",
    "    sil = silhouette_score(tfidf_matrix, hac_labels)\n",
    "    print(f\"  {method:12s}: ARI={ari:.3f}, Silhouette={sil:.3f}\")\n",
    "\n",
    "print(f\"\\n  K-Means:      ARI={ari_score:.3f}, Silhouette={sil_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a26848",
   "metadata": {},
   "source": [
    "**Observation:** Ward linkage typically performs best for text clustering because it minimizes within-cluster variance (similar to K-Means' objective). Single-link is avoided for text because it produces degenerate \"chain-shaped\" clusters.\n",
    "\n",
    "---\n",
    "# 3. Topic Modeling\n",
    "\n",
    "**Topic Modeling** differs from clustering in an important way:\n",
    "- **Clustering** assigns each document to exactly one group based on overall similarity\n",
    "- **Topic Modeling** treats each document as a **mixture of topics**, where each topic is a probability distribution over words\n",
    "\n",
    "This is more realistic for text — a news article about \"space technology funding\" could belong to both a *science* topic and a *politics* topic simultaneously.\n",
    "\n",
    "The three classical topic modeling approaches are:\n",
    "1. **LSA** (Latent Semantic Analysis) — based on SVD\n",
    "2. **LDA** (Latent Dirichlet Allocation) — a probabilistic generative model\n",
    "3. **NMF** (Non-Negative Matrix Factorization) — factorizes the TF-IDF matrix into non-negative components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3594733e",
   "metadata": {},
   "source": [
    "## 3.1 Latent Semantic Analysis (LSA / LSI)\n",
    "\n",
    "LSA uses **Singular Value Decomposition (SVD)** to find a low-rank approximation of the document-term matrix:\n",
    "\n",
    "$$A \\approx U_k \\Sigma_k V_k^T$$\n",
    "\n",
    "where:\n",
    "- $U_k$ = document-topic matrix (how much each document belongs to each topic)\n",
    "- $\\Sigma_k$ = diagonal matrix of singular values (topic importance)\n",
    "- $V_k^T$ = topic-term matrix (how much each term contributes to each topic)\n",
    "- $k$ = number of topics (typically 100–300)\n",
    "\n",
    "**Why does this work?** SVD captures the latent semantic structure by grouping together words that frequently co-occur across documents. For example, \"car\" and \"automobile\" will have similar representations because they appear in similar document contexts.\n",
    "\n",
    "**Limitations:** LSA topics can contain negative values, which are harder to interpret than the non-negative weights in NMF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0168d96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LSA with scikit-learn ──\n",
    "num_topics = 5\n",
    "\n",
    "lsa_model = TruncatedSVD(n_components=num_topics, random_state=SEED)\n",
    "lsa_doc_topics = lsa_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(f\"LSA: explained variance ratio = {lsa_model.explained_variance_ratio_.sum():.3f}\")\n",
    "print(f\"\\nTop 10 terms per LSA topic:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for topic_idx, topic in enumerate(lsa_model.components_):\n",
    "    top_term_indices = topic.argsort()[:-11:-1]\n",
    "    top_terms = [feature_names[i] for i in top_term_indices]\n",
    "    print(f\"  Topic {topic_idx}: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eae7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LSA with Gensim (includes coherence scoring) ──\n",
    "# Prepare Gensim corpus\n",
    "en_stop = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def preprocess_for_gensim(doc_set):\n",
    "    \"\"\"Tokenize, lowercase, remove stopwords.\"\"\"\n",
    "    texts = []\n",
    "    for doc in doc_set:\n",
    "        tokens = tokenizer.tokenize(doc.lower())\n",
    "        stopped = [t for t in tokens if t not in en_stop and len(t) > 2]\n",
    "        texts.append(stopped)\n",
    "    return texts\n",
    "\n",
    "clean_docs = preprocess_for_gensim(newsgroups.data)\n",
    "dictionary = corpora.Dictionary(clean_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in clean_docs]\n",
    "\n",
    "print(f\"Gensim dictionary: {len(dictionary)} unique tokens\")\n",
    "print(f\"Corpus: {len(doc_term_matrix)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4bcf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSA with Gensim\n",
    "lsa_gensim = LsiModel(doc_term_matrix, num_topics=num_topics, id2word=dictionary)\n",
    "\n",
    "print(\"LSA Topics (Gensim):\")\n",
    "print(\"=\" * 70)\n",
    "for topic_num, words in lsa_gensim.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "    terms = [f\"{word} ({weight:.3f})\" for word, weight in words]\n",
    "    print(f\"  Topic {topic_num}: {', '.join(terms)}\")\n",
    "\n",
    "# Coherence score\n",
    "coherence_lsa = CoherenceModel(model=lsa_gensim, texts=clean_docs, dictionary=dictionary, coherence='c_v')\n",
    "print(f\"\\nLSA Coherence Score (c_v): {coherence_lsa.get_coherence():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49611be3",
   "metadata": {},
   "source": [
    "## 3.2 Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA is a **generative probabilistic model** that assumes the following process for generating a document:\n",
    "\n",
    "1. Choose a distribution over topics from a Dirichlet prior $Dir(\\alpha)$\n",
    "2. For each word in the document:\n",
    "   a. Choose a topic from the document's topic distribution\n",
    "   b. Choose a word from that topic's word distribution\n",
    "\n",
    "LDA infers two distributions:\n",
    "- **Topic-per-document** distribution: $\\theta_d$ — what topics does document $d$ discuss?\n",
    "- **Word-per-topic** distribution: $\\phi_k$ — what words characterize topic $k$?\n",
    "\n",
    "The Dirichlet prior $\\alpha$ controls how many topics each document is expected to cover — a low $\\alpha$ produces sparser topic mixtures (each document focuses on fewer topics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fdbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LDA with Gensim ──\n",
    "lda_gensim = LdaModel(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    "    random_state=SEED,\n",
    "    passes=15,           # number of passes through the corpus\n",
    "    alpha='auto',        # learn optimal alpha from data\n",
    "    per_word_topics=True\n",
    ")\n",
    "\n",
    "print(\"LDA Topics (Gensim):\")\n",
    "print(\"=\" * 70)\n",
    "for topic_num, words in lda_gensim.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
    "    terms = [f\"{word} ({weight:.3f})\" for word, weight in words]\n",
    "    print(f\"  Topic {topic_num}: {', '.join(terms)}\")\n",
    "\n",
    "# Coherence score\n",
    "coherence_lda = CoherenceModel(model=lda_gensim, texts=clean_docs, dictionary=dictionary, coherence='c_v')\n",
    "print(f\"\\nLDA Coherence Score (c_v): {coherence_lda.get_coherence():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── LDA with scikit-learn ──\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.95, min_df=2, max_features=5000, stop_words='english'\n",
    ")\n",
    "count_matrix = count_vectorizer.fit_transform(newsgroups.data)\n",
    "count_feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "lda_sklearn = LatentDirichletAllocation(\n",
    "    n_components=num_topics, random_state=SEED, max_iter=20, learning_method='batch'\n",
    ")\n",
    "lda_sklearn.fit(count_matrix)\n",
    "\n",
    "print(\"LDA Topics (scikit-learn):\")\n",
    "print(\"=\" * 70)\n",
    "for topic_idx, topic in enumerate(lda_sklearn.components_):\n",
    "    top_term_indices = topic.argsort()[:-11:-1]\n",
    "    top_terms = [count_feature_names[i] for i in top_term_indices]\n",
    "    print(f\"  Topic {topic_idx}: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b54e3e3",
   "metadata": {},
   "source": [
    "## 3.3 Interactive LDA Visualization with pyLDAvis\n",
    "\n",
    "**pyLDAvis** provides an interactive visualization of LDA topics. Each circle represents a topic — the size indicates the topic's prevalence in the corpus, and the distance between circles reflects how different the topics are. Clicking a topic shows its most relevant terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4dad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive LDA visualization\n",
    "import pyLDAvis\n",
    "from pyLDAvis import gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_vis = gensim_models.prepare(lda_gensim, doc_term_matrix, dictionary, mds='mmds', R=30)\n",
    "lda_vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81959a62",
   "metadata": {},
   "source": [
    "## 3.4 Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "NMF factorizes the document-term matrix $V$ into two non-negative matrices:\n",
    "\n",
    "$$V \\approx W \\times H$$\n",
    "\n",
    "where:\n",
    "- $V$ = document-term matrix (TF-IDF weighted, all values $\\geq 0$)\n",
    "- $W$ = document-topic matrix (how much each document belongs to each topic)\n",
    "- $H$ = topic-term matrix (how much each term contributes to each topic)\n",
    "\n",
    "**Why NMF often outperforms LSA and LDA for topic modeling:**\n",
    "1. **Non-negativity constraint** aligns naturally with word counts (you can't have a negative word frequency)\n",
    "2. **Sparse, additive topics** — each topic is a sparse weighted combination of terms\n",
    "3. **Works directly with TF-IDF** — no need for count-based matrices like LDA\n",
    "4. **Higher coherence scores** — NMF topics tend to be more interpretable\n",
    "\n",
    "**How many topics?** A common approach is to try values from 10–100 and select the number that maximizes **Topic Coherence**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90e84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── NMF with scikit-learn ──\n",
    "nmf_model = NMF(n_components=num_topics, random_state=SEED, max_iter=500, init='nndsvd')\n",
    "nmf_doc_topics = nmf_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "print(f\"NMF reconstruction error: {nmf_model.reconstruction_err_:.3f}\")\n",
    "print(f\"\\nNMF Topics:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_term_indices = topic.argsort()[:-11:-1]\n",
    "    top_terms = [f\"{feature_names[i]} ({topic[i]:.3f})\" for i in top_term_indices]\n",
    "    print(f\"  Topic {topic_idx}: {', '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb92522d",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Comparing LSA vs LDA vs NMF\n",
    "\n",
    "Let's compare all three topic modeling approaches on the same dataset using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde024dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Find optimal number of topics for each method ──\n",
    "topic_range = range(3, 12)\n",
    "coherence_scores = {'LSA': [], 'LDA': [], 'NMF': []}\n",
    "\n",
    "print(\"Computing coherence scores for different numbers of topics...\")\n",
    "for n in topic_range:\n",
    "    # LSA\n",
    "    lsa_temp = LsiModel(doc_term_matrix, num_topics=n, id2word=dictionary)\n",
    "    cm_lsa = CoherenceModel(model=lsa_temp, texts=clean_docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_scores['LSA'].append(cm_lsa.get_coherence())\n",
    "\n",
    "    # LDA\n",
    "    lda_temp = LdaModel(corpus=doc_term_matrix, id2word=dictionary, num_topics=n,\n",
    "                        random_state=SEED, passes=10)\n",
    "    cm_lda = CoherenceModel(model=lda_temp, texts=clean_docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_scores['LDA'].append(cm_lda.get_coherence())\n",
    "\n",
    "    # NMF (use Gensim wrapper for coherence)\n",
    "    nmf_temp = NMF(n_components=n, random_state=SEED, max_iter=300, init='nndsvd')\n",
    "    nmf_temp.fit(tfidf_matrix)\n",
    "    # Extract top words per topic for coherence\n",
    "    nmf_topics = []\n",
    "    for topic in nmf_temp.components_:\n",
    "        top_indices = topic.argsort()[:-11:-1]\n",
    "        nmf_topics.append([feature_names[i] for i in top_indices])\n",
    "    cm_nmf = CoherenceModel(topics=nmf_topics, texts=clean_docs, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_scores['NMF'].append(cm_nmf.get_coherence())\n",
    "\n",
    "    print(f\"  K={n}: LSA={coherence_scores['LSA'][-1]:.3f}, LDA={coherence_scores['LDA'][-1]:.3f}, NMF={coherence_scores['NMF'][-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68345e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coherence comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "for method, scores in coherence_scores.items():\n",
    "    plt.plot(topic_range, scores, 'o-', label=method, linewidth=2)\n",
    "\n",
    "plt.xlabel('Number of Topics', fontsize=12)\n",
    "plt.ylabel('Coherence Score (c_v)', fontsize=12)\n",
    "plt.title('Topic Model Comparison: Coherence vs Number of Topics', fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nSummary (at K=5):\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  {'Method':8s}  {'Coherence (c_v)':>15s}\")\n",
    "print(f\"  {'LSA':8s}  {coherence_scores['LSA'][2]:>15.3f}\")\n",
    "print(f\"  {'LDA':8s}  {coherence_scores['LDA'][2]:>15.3f}\")\n",
    "print(f\"  {'NMF':8s}  {coherence_scores['NMF'][2]:>15.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292bbf3",
   "metadata": {},
   "source": [
    "### Comparison: LSA vs LDA vs NMF\n",
    "\n",
    "| Property | LSA | LDA | NMF |\n",
    "|----------|-----|-----|-----|\n",
    "| **Method** | SVD (matrix factorization) | Probabilistic (Dirichlet prior) | Non-negative matrix factorization |\n",
    "| **Input** | TF-IDF or count matrix | Count matrix (bag of words) | TF-IDF matrix |\n",
    "| **Topic values** | Can be negative | Probabilities (0–1) | Non-negative |\n",
    "| **Interpretability** | Lower (negative weights) | Medium (probabilistic) | **Highest** (sparse, additive) |\n",
    "| **Speed** | Fast | Slower (iterative inference) | Fast |\n",
    "| **Coherence** | Typically lowest | Medium | **Typically highest** |\n",
    "| **Best for** | Dimensionality reduction | Generative modeling, discovering topic distributions | Interpretable topic extraction |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b4aac2",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. BERTopic — Modern Topic Modeling\n",
    "\n",
    "**BERTopic** combines the power of pre-trained language models with traditional clustering:\n",
    "\n",
    "1. **Document Embedding** — use a pre-trained BERT model to create dense vector representations of documents (768 dimensions)\n",
    "2. **Dimensionality Reduction (UMAP)** — reduce 768D embeddings to 5–50D while preserving local and global structure\n",
    "3. **Clustering (HDBSCAN)** — density-based clustering that automatically finds the number of clusters and handles varying cluster densities\n",
    "4. **Topic Representation (c-TF-IDF)** — class-based TF-IDF to extract the most representative words per cluster/topic\n",
    "\n",
    "**Why BERTopic?**\n",
    "- Captures **semantic meaning** (not just word co-occurrence)\n",
    "- Does **not** require specifying the number of topics in advance\n",
    "- Handles documents that don't fit any topic (outlier detection)\n",
    "- Produces more **coherent** topics than classical methods\n",
    "- Offers rich **visualization** capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddab003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── BERTopic ──\n",
    "from bertopic import BERTopic\n",
    "\n",
    "# Create BERTopic model\n",
    "# BERTopic automatically handles: embedding → UMAP → HDBSCAN → c-TF-IDF\n",
    "topic_model = BERTopic(\n",
    "    language=\"english\",\n",
    "    calculate_probabilities=True,\n",
    "    verbose=True,\n",
    "    nr_topics=\"auto\",          # let BERTopic decide\n",
    "    min_topic_size=15,         # minimum documents per topic\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Fit on our documents\n",
    "topics, probs = topic_model.fit_transform(newsgroups.data)\n",
    "\n",
    "print(f\"\\nBERTopic found {len(set(topics)) - (1 if -1 in topics else 0)} topics\")\n",
    "print(f\"Outlier documents (topic -1): {sum(1 for t in topics if t == -1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d5466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the top topics\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(\"Top 10 Topics by frequency:\")\n",
    "print(topic_info.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show representative words for the top topics\n",
    "print(\"\\nTop words per topic:\")\n",
    "print(\"=\" * 70)\n",
    "for topic_id in range(min(8, len(set(topics)) - 1)):\n",
    "    topic_words = topic_model.get_topic(topic_id)\n",
    "    if topic_words:\n",
    "        words = [f\"{word} ({score:.3f})\" for word, score in topic_words[:8]]\n",
    "        print(f\"  Topic {topic_id}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e33d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic hierarchy\n",
    "fig = topic_model.visualize_hierarchy()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6884a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic similarity as a heatmap\n",
    "fig = topic_model.visualize_heatmap()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topics in 2D using UMAP projections\n",
    "fig = topic_model.visualize_topics()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90b5b2",
   "metadata": {},
   "source": [
    "**Observation:** BERTopic typically discovers more fine-grained topics than classical methods because it works with semantic embeddings rather than just word co-occurrence. The hierarchical view shows how closely related topics can be merged into broader themes.\n",
    "\n",
    "---\n",
    "# 6. Anomaly Detection in Text\n",
    "\n",
    "**Text anomaly detection** identifies text that deviates from expected norms at various levels:\n",
    "- **Orthographic** — unusual spelling patterns\n",
    "- **Lexical** — unexpected vocabulary\n",
    "- **Syntactic** — unusual sentence structure\n",
    "- **Semantic** — words used in unexpected contexts\n",
    "\n",
    "**Approaches:**\n",
    "- **Supervised**: requires labeled data (normal vs. anomalous) — Decision Trees, SVM, Neural Networks\n",
    "- **Unsupervised**: no labels needed — clustering-based (outliers = anomalies), distance-based (KNN, LOF), reconstruction-based (autoencoders), Isolation Forests\n",
    "- **LLM-based**: use language model predictions to detect unlikely word usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2865214",
   "metadata": {},
   "source": [
    "## 6.1 Code-Word Detection with BERT Masked Language Model\n",
    "\n",
    "A creative application of anomaly detection is **code-word detection** — identifying words that are used in an unusual context, potentially as code words for illicit communication.\n",
    "\n",
    "The idea: use BERT's **Masked Language Model (MLM)** to predict what word *should* appear in a given position. If the actual word has very low probability according to BERT, it may be a code word.\n",
    "\n",
    "For example: *\"The **watermelon** is in the **fridge**\"* — BERT might find \"watermelon\" perfectly normal in this context. But *\"The **package** is in the **fridge**\"* in a conversation between known drug dealers could be suspicious because \"package\" has a much lower MLM probability in the surrounding discourse context.\n",
    "\n",
    "The key insight is that BERT, trained on billions of words of natural language, has learned what words are *contextually likely*. Words that deviate strongly from BERT's expectations may carry hidden meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6428d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Code-word detection with BERT MLM ──\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load BERT for Masked Language Model\n",
    "mlm_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\").to(device)\n",
    "mlm_model.eval()\n",
    "\n",
    "def get_word_probability(sentence, target_word, position=None):\n",
    "    \"\"\"\n",
    "    Calculate the probability that BERT assigns to a specific word at its position.\n",
    "    Lower probability = more anomalous/unexpected in context.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    if position is None:\n",
    "        # Find the target word position\n",
    "        position = next((i for i, w in enumerate(words) if w.lower() == target_word.lower()), None)\n",
    "        if position is None:\n",
    "            return None\n",
    "\n",
    "    # Replace the target word with [MASK]\n",
    "    masked_words = words.copy()\n",
    "    masked_words[position] = \"[MASK]\"\n",
    "    masked_sentence = \" \".join(masked_words)\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = mlm_tokenizer(masked_sentence, return_tensors=\"pt\").to(device)\n",
    "    mask_token_index = (inputs[\"input_ids\"] == mlm_tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "    if len(mask_token_index) == 0:\n",
    "        return None\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = mlm_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Get probability for the target word\n",
    "    mask_logits = logits[0, mask_token_index[0], :]\n",
    "    probs = torch.softmax(mask_logits, dim=-1)\n",
    "\n",
    "    target_token_id = mlm_tokenizer.convert_tokens_to_ids(target_word.lower())\n",
    "    if target_token_id == mlm_tokenizer.unk_token_id:\n",
    "        return None\n",
    "\n",
    "    return probs[target_token_id].item()\n",
    "\n",
    "print(\"BERT MLM model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test code-word detection\n",
    "test_cases = [\n",
    "    # Normal sentences — all words should have reasonable probability\n",
    "    (\"The cat is sleeping on the couch\", \"cat\"),\n",
    "    (\"The cat is sleeping on the couch\", \"sleeping\"),\n",
    "    (\"She drove her car to the office\", \"car\"),\n",
    "\n",
    "    # Potentially suspicious — \"watermelon\" in drug code context\n",
    "    (\"The watermelon is in the fridge\", \"watermelon\"),\n",
    "    (\"The watermelon is ready for pickup\", \"watermelon\"),\n",
    "\n",
    "    # Obviously anomalous — unusual word in context\n",
    "    (\"The elephant is in the fridge\", \"elephant\"),\n",
    "    (\"The spaceship is in the fridge\", \"spaceship\"),\n",
    "\n",
    "    # More subtle code-word examples\n",
    "    (\"I left the cookies on the table\", \"cookies\"),\n",
    "    (\"I left the merchandise on the table\", \"merchandise\"),\n",
    "    (\"I left the product at the usual place\", \"product\"),\n",
    "]\n",
    "\n",
    "print(\"Code-Word Detection using BERT MLM\")\n",
    "print(\"=\" * 75)\n",
    "print(f\"{'Sentence':<50s} {'Target':<15s} {'Prob':>8s}  {'Assessment'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for sentence, target in test_cases:\n",
    "    prob = get_word_probability(sentence, target)\n",
    "    if prob is not None:\n",
    "        if prob > 0.05:\n",
    "            assessment = \"Normal\"\n",
    "        elif prob > 0.005:\n",
    "            assessment = \"Unusual\"\n",
    "        else:\n",
    "            assessment = \"ANOMALOUS\"\n",
    "        print(f\"{sentence:<50s} {target:<15s} {prob:>8.4f}  {assessment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a161ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Compare BERT's top predictions with actual words\n",
    "def show_top_predictions(sentence, mask_position, top_k=10):\n",
    "    \"\"\"Show BERT's top-k predictions for a masked position.\"\"\"\n",
    "    words = sentence.split()\n",
    "    actual_word = words[mask_position]\n",
    "    masked_words = words.copy()\n",
    "    masked_words[mask_position] = \"[MASK]\"\n",
    "    masked_sentence = \" \".join(masked_words)\n",
    "\n",
    "    inputs = mlm_tokenizer(masked_sentence, return_tensors=\"pt\").to(device)\n",
    "    mask_idx = (inputs[\"input_ids\"] == mlm_tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = mlm_model(**inputs).logits\n",
    "\n",
    "    probs = torch.softmax(logits[0, mask_idx[0], :], dim=-1)\n",
    "    top_probs, top_indices = probs.topk(top_k)\n",
    "\n",
    "    print(f\"\\nSentence: \\\"{sentence}\\\"\")\n",
    "    print(f\"Masked word: \\\"{actual_word}\\\" (position {mask_position})\")\n",
    "    print(f\"BERT's top {top_k} predictions:\")\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = mlm_tokenizer.convert_ids_to_tokens(idx.item())\n",
    "        marker = \" <-- ACTUAL\" if token == actual_word.lower() else \"\"\n",
    "        print(f\"  {i+1:2d}. {token:15s} ({prob.item():.4f}){marker}\")\n",
    "\n",
    "# Example: is \"watermelon\" expected or not?\n",
    "show_top_predictions(\"The watermelon is in the fridge\", mask_position=1)\n",
    "print()\n",
    "show_top_predictions(\"The elephant is in the fridge\", mask_position=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3037faf7",
   "metadata": {},
   "source": [
    "**Observation:** BERT assigns lower probability to contextually unexpected words. In a code-word detection scenario, investigators would compare the MLM probability of suspicious words against a threshold — words with very low probability in their context are candidates for code words that carry hidden meaning.\n",
    "\n",
    "This technique is related to **perplexity-based anomaly detection**: the higher the perplexity of a sentence under a language model, the more \"surprising\" (and potentially anomalous) it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad1424",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercises\n",
    "\n",
    "The following exercises are graded. Please provide your answers in the designated cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83badc06",
   "metadata": {},
   "source": [
    "## Exercise 1 — Clustering vs Topic Modeling (5 points)\n",
    "\n",
    "Compare and contrast **K-Means clustering** and **LDA topic modeling** as methods for organizing a text corpus. In your answer, address:\n",
    "\n",
    "1. How does each method assign documents to groups/topics? What is the fundamental difference?\n",
    "2. What are the advantages and disadvantages of each approach?\n",
    "3. In what scenario would you prefer K-Means clustering over LDA, and vice versa?\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364facb5",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e7f78",
   "metadata": {},
   "source": [
    "## Exercise 2 — BERTopic vs Classical Topic Models (5 points)\n",
    "\n",
    "BERTopic uses a fundamentally different pipeline than LSA, LDA, or NMF. In your answer, address:\n",
    "\n",
    "1. Explain the four main steps of the BERTopic pipeline (embedding, UMAP, HDBSCAN, c-TF-IDF) and what each step contributes.\n",
    "2. Why does BERTopic typically produce more coherent topics than classical methods?\n",
    "3. What are the limitations of BERTopic compared to simpler methods like NMF?\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236882a8",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e5c872",
   "metadata": {},
   "source": [
    "## Exercise 3 — Optimal Number of Topics (10 points)\n",
    "\n",
    "Write code to find the optimal number of topics for **NMF** on the 20 Newsgroups dataset. Your code should:\n",
    "\n",
    "1. Try NMF with $K \\in \\{3, 5, 7, 10, 15, 20\\}$ topics using the `tfidf_matrix` and `feature_names` from earlier\n",
    "2. For each K, compute the topic coherence using Gensim's `CoherenceModel` with `coherence='c_v'`\n",
    "3. Store the best number of topics in a variable called `best_k` and the corresponding coherence score in `best_coherence`\n",
    "4. Print the top 10 words for each topic at the optimal K\n",
    "\n",
    "You may reuse the `clean_docs`, `dictionary`, and helper code from the sections above.\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line with your solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e64e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell — do not modify\n",
    "assert 'best_k' in dir(), \"You need to define 'best_k'\"\n",
    "assert 'best_coherence' in dir(), \"You need to define 'best_coherence'\"\n",
    "assert best_k in [3, 5, 7, 10, 15, 20], \"best_k should be one of the tested values\"\n",
    "assert isinstance(best_coherence, float), \"best_coherence should be a float\"\n",
    "assert best_coherence > 0, \"best_coherence should be positive\"\n",
    "print(f\"Best K = {best_k}, Best Coherence = {best_coherence:.3f}\")\n",
    "print(\"All auto-graded tests passed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
