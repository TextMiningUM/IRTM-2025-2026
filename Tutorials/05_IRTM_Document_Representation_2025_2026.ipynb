{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIBbWrF5_DXC"
      },
      "source": [
        "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
        "\n",
        "# Information Retrieval and Text Mining Course - Tutorial Document Representation\n",
        "Author: Gijs Wijngaard and Jan Scholtes\n",
        "\n",
        "Version: 2025-2026"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14xe-BeO_G34"
      },
      "source": [
        "Welcome to the tutorial about **document representation**. A fundamental challenge in Information Retrieval and Text Mining is converting unstructured text into numerical representations that algorithms can process. In this notebook you will explore a progression of methods — from simple counting-based approaches to modern neural embeddings:\n",
        "\n",
        "1. **One-Hot Encoding** — the simplest binary representation\n",
        "2. **N-grams & Bag-of-Words** — counting word occurrences\n",
        "3. **TF-IDF** — weighting terms by importance\n",
        "4. **Cosine Similarity** — measuring document similarity\n",
        "5. **Word2Vec** — learning dense word embeddings\n",
        "6. **Sentence Transformers** — contextual sentence embeddings\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. One-Hot Encoding\n",
        "\n",
        "The simplest way to represent words as numbers is **one-hot encoding**. Given a vocabulary $V = \\{w_1, w_2, \\ldots, w_{|V|}\\}$, each word $w_i$ is represented as a binary vector $\\mathbf{e}_i \\in \\{0, 1\\}^{|V|}$ where:\n",
        "\n",
        "$$\n",
        "\\mathbf{e}_i[j] = \\begin{cases} 1 & \\text{if } j = i \\\\ 0 & \\text{otherwise} \\end{cases}\n",
        "$$\n",
        "\n",
        "For example, with vocabulary $V = \\{\\text{cat}, \\text{dog}, \\text{fox}\\}$:\n",
        "- cat $\\rightarrow [1, 0, 0]$\n",
        "- dog $\\rightarrow [0, 1, 0]$\n",
        "- fox $\\rightarrow [0, 0, 1]$\n",
        "\n",
        "**Limitations:**\n",
        "- Vectors are very high-dimensional (size of vocabulary, typically 10,000+)\n",
        "- All word vectors are **orthogonal** — there is no notion of similarity ($\\text{cosine}(\\mathbf{e}_i, \\mathbf{e}_j) = 0$ for $i \\neq j$)\n",
        "- No semantic information is captured (\"king\" and \"queen\" are as different as \"king\" and \"banana\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "sentence = \"the quick brown fox jumps over the lazy dog\"\n",
        "words = sentence.split()\n",
        "vocabulary = sorted(set(words))\n",
        "\n",
        "print(f\"Vocabulary ({len(vocabulary)} unique words): {vocabulary}\\n\")\n",
        "\n",
        "# Create one-hot vectors for each unique word\n",
        "one_hot = {word: np.eye(len(vocabulary))[i].astype(int) for i, word in enumerate(vocabulary)}\n",
        "\n",
        "for word, vector in one_hot.items():\n",
        "    print(f\"  {word:6s} → {vector}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Ih_CLWBDAe"
      },
      "source": [
        "## 2. N-grams and Bag-of-Words\n",
        "\n",
        "Instead of representing individual words, we can capture **sequences** of words. An **n-gram** is a contiguous sequence of $n$ items from a text. Given a sequence of tokens $w_1, w_2, \\ldots, w_m$, the set of n-grams is:\n",
        "\n",
        "$$\n",
        "\\text{ngrams}(n) = \\{(w_i, w_{i+1}, \\ldots, w_{i+n-1}) \\mid 1 \\leq i \\leq m - n + 1\\}\n",
        "$$\n",
        "\n",
        "Let's see this in practice with our example sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOx7S5c5BCr-"
      },
      "outputs": [],
      "source": [
        "sentence = \"the quick brown fox jumps over the lazy dog\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcKShOniC-gK"
      },
      "source": [
        "A **bigram** ($n=2$) groups two consecutive words together. Bigrams capture local word co-occurrence patterns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1D1tNh-CXaG"
      },
      "outputs": [],
      "source": [
        "splitted = sentence.split(\" \")\n",
        "[bigram for bigram in zip(splitted, splitted[1:])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H_ZcDytEF3o"
      },
      "source": [
        "With the grouping of 3 words together, we call it a trigram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edis7nAnD_J7"
      },
      "outputs": [],
      "source": [
        "[trigram for trigram in zip(splitted, splitted[1:], splitted[2:])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXF2g4anFN0L"
      },
      "source": [
        "### Bag-of-Words (BoW)\n",
        "\n",
        "A **Bag-of-Words** representation ignores word order entirely and represents a document as a vector of word counts. Given vocabulary $V = \\{w_1, \\ldots, w_{|V|}\\}$, a document $d$ is represented as:\n",
        "\n",
        "$$\n",
        "\\mathbf{d} = [\\text{tf}(w_1, d), \\text{tf}(w_2, d), \\ldots, \\text{tf}(w_{|V|}, d)]\n",
        "$$\n",
        "\n",
        "where $\\text{tf}(w, d)$ is the **term frequency** — the number of times word $w$ appears in document $d$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0mi71kCE6Aq"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "bag_of_words = Counter(splitted)\n",
        "print(\"Bag-of-Words representation:\")\n",
        "for word, count in bag_of_words.items():\n",
        "    print(f\"  {word}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iTvjbKCFYjX"
      },
      "source": [
        "Notice that \"the\" has a count of 2, while all other words appear once. Words like \"the\", \"a\", \"and\" are called **stop words** — they are extremely common but carry little meaning about the document's topic. We need a weighting scheme that reduces the importance of such words. This is where **TF-IDF** comes in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_UDIfVi39c1"
      },
      "source": [
        "<a name=\"dataset\"></a>\n",
        "\n",
        "## 3. Dataset\n",
        "\n",
        "We use a [movie review dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/) from NLTK. This dataset contains **1000 positive** and **1000 negative** movie reviews, making it suitable for **sentiment analysis** — classifying reviews as positive or negative based on their word content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz8sw8uN38x8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words, movie_reviews as mr\n",
        "nltk_words = set(words.words())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlGANEGI8d_O"
      },
      "source": [
        "We first remove the punctuation from all the words, and afterwards we count the most common words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fazKk8kh5Peg"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "def remove_punct(word):\n",
        "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "    return word if word in nltk_words else ''\n",
        "all_words = Counter(filter(remove_punct, mr.words()))\n",
        "all_words.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvMOSCRS81nm"
      },
      "source": [
        "The same problem we have here. Words such as *the* and *a* are the most common amongst the movie reviews of our dataset. However, to do something with the movie review, such as classifying it, we should give a lower probability to these words, as they do not say much about the content itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty641CpU_1xk"
      },
      "outputs": [],
      "source": [
        "documents = [(list(filter(remove_punct, mr.words(f))), mr.categories(f)) for f in mr.fileids()]\n",
        "print(\"Total number of documents:\", len(documents))\n",
        "print(\"Total number of words in first document:\", len(documents[0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6DBjrwDGoow"
      },
      "source": [
        "## 4. TF-IDF (Term Frequency–Inverse Document Frequency)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXl-jPkoGsZW"
      },
      "source": [
        "TF-IDF assigns a **weight** to each term in a document that reflects how important that term is relative to the collection. The score **increases** with the number of occurrences in a document and **increases** with the rarity of the term across all documents.\n",
        "\n",
        "The TF-IDF weight of term $t$ in document $d$ is:\n",
        "\n",
        "$$\n",
        "w_{t,d} = \\text{tf-idf}(t, d) = \\log(1 + \\text{tf}_{t,d}) \\times \\log_{10}\\!\\left(\\frac{N}{\\text{df}_t}\\right)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\text{tf}_{t,d}$ = **term frequency**: number of times term $t$ appears in document $d$\n",
        "- $\\text{df}_t$ = **document frequency**: number of documents in the collection containing term $t$\n",
        "- $N$ = total number of documents in the collection\n",
        "- $\\log(1 + \\text{tf}_{t,d})$ applies **sublinear** scaling — a word appearing 10× is not 10× as important\n",
        "- $\\log_{10}(N / \\text{df}_t)$ is the **inverse document frequency (IDF)** — rare terms get higher weight\n",
        "\n",
        "**Key insight**: A term gets a high TF-IDF score when it appears frequently in a specific document (high TF) but rarely across the collection (high IDF). Common words like \"the\" will have $\\text{df}_t \\approx N$, giving $\\text{IDF} \\approx 0$.\n",
        "\n",
        "> **Note:** There are many TF-IDF variants. A popular alternative is **BM25** (Best Matching 25), which adds document length normalization and a saturation parameter. BM25 is used by search engines like Elasticsearch and Solr."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27SyhyooCggX"
      },
      "source": [
        "Lets start with calculating the term frequency (tf). Now, we calculated the number of words for all documents. However, to calculate the tf-idf score we need to calculate the term-frequency for each term per document. Thus, we need to loop over the documents and count the occurrences of the terms per document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61lfkNqbkSU8"
      },
      "outputs": [],
      "source": [
        "tf = [Counter(words) for words, category in documents]\n",
        "tf[0].most_common(10) # Most common terms for the first document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV9FnDB2oJxH"
      },
      "source": [
        "Now let's calculate the **document frequency** ($\\text{df}$). For each word in our vocabulary, we count how many documents contain that word. We convert each document to a `set` (unique words) so that membership lookup is $O(1)$ instead of $O(n)$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2WbKAjXhs5a"
      },
      "outputs": [],
      "source": [
        "setted_docs = [set(doc) for doc, category in documents]\n",
        "df = {word: sum([1 for doc in setted_docs if word in doc]) for word in all_words.keys()}\n",
        "list(df.items())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSkSlby1JVEV"
      },
      "source": [
        "### Exercise 1 — Implement TF-IDF (4 points)\n",
        "\n",
        "Implement the TF-IDF score for each word per document yourself using the formula:\n",
        "\n",
        "$$\n",
        "w_{t,d} = \\log(1 + \\text{tf}_{t,d}) \\times \\log_{10}\\!\\left(\\frac{N}{\\text{df}_t}\\right)\n",
        "$$\n",
        "\n",
        "Use `numpy` for `log` and `log10`. Store the result as a list of dictionaries called `tfidf`, where each dictionary maps words to their TF-IDF scores for that document.\n",
        "\n",
        "> **Hint**: Use `tf[i]` to get the term frequency dictionary for document `i`, `df[word]` for the document frequency of a word, and `len(documents)` for $N$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdIeXFByuDd6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "N = len(documents)\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "tfidf = []\n",
        "for i in range(N):\n",
        "    tfidf_doc = {}\n",
        "    for word, count in tf[i].items():\n",
        "        tfidf_doc[word] = np.log(1 + count) * np.log10(N / df[word])\n",
        "    tfidf.append(tfidf_doc)\n",
        "### END SOLUTION\n",
        "\n",
        "# Check: print top 5 TF-IDF terms for the first document\n",
        "sorted_first = sorted(tfidf[0].items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "print(\"Top 5 TF-IDF terms for document 1:\")\n",
        "for word, score in sorted_first:\n",
        "    print(f\"  {word}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### BEGIN HIDDEN TESTS\n",
        "# Test that tfidf is a list of dictionaries with correct length\n",
        "assert isinstance(tfidf, list), \"tfidf should be a list\"\n",
        "assert len(tfidf) == len(documents), f\"tfidf should have {len(documents)} entries, got {len(tfidf)}\"\n",
        "assert isinstance(tfidf[0], dict), \"Each element should be a dictionary\"\n",
        "\n",
        "# Test that common words get low TF-IDF scores\n",
        "# \"the\" appears in almost all documents, so its IDF should be near 0\n",
        "if \"the\" in tfidf[0]:\n",
        "    assert tfidf[0][\"the\"] < 0.1, \"Common words like 'the' should have very low TF-IDF\"\n",
        "\n",
        "# Test that all values are non-negative\n",
        "for doc_tfidf in tfidf[:5]:\n",
        "    for word, score in doc_tfidf.items():\n",
        "        assert score >= 0, f\"TF-IDF scores should be non-negative, got {score} for '{word}'\"\n",
        "\n",
        "print(\"All Exercise 1 tests passed!\")\n",
        "### END HIDDEN TESTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsgtPmwkaN9f"
      },
      "source": [
        "### Exercise 2 — Analyze TF-IDF (2 points)\n",
        "\n",
        "**a.** Using your TF-IDF scores, compute the **average TF-IDF score** for each word across all **positive** reviews and all **negative** reviews separately. Then get the top 50 words by average TF-IDF for each sentiment.\n",
        "\n",
        "Store the top 50 positive words as `top_positive` and top 50 negative words as `top_negative` (each as a list of `(word, score)` tuples, sorted descending by score).\n",
        "\n",
        "**b.** What do you notice? Write your observations in the text cell below. Are there differences between the two lists? Could we train a classifier based on these TF-IDF features?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4aqUOZ0pZZ0"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "# Separate positive and negative documents\n",
        "pos_indices = [i for i, (_, cat) in enumerate(documents) if 'pos' in cat]\n",
        "neg_indices = [i for i, (_, cat) in enumerate(documents) if 'neg' in cat]\n",
        "\n",
        "# Average TF-IDF per word for positive reviews\n",
        "pos_avg = {}\n",
        "for i in pos_indices:\n",
        "    for word, score in tfidf[i].items():\n",
        "        pos_avg[word] = pos_avg.get(word, 0) + score\n",
        "pos_avg = {w: s / len(pos_indices) for w, s in pos_avg.items()}\n",
        "\n",
        "# Average TF-IDF per word for negative reviews\n",
        "neg_avg = {}\n",
        "for i in neg_indices:\n",
        "    for word, score in tfidf[i].items():\n",
        "        neg_avg[word] = neg_avg.get(word, 0) + score\n",
        "neg_avg = {w: s / len(neg_indices) for w, s in neg_avg.items()}\n",
        "\n",
        "top_positive = sorted(pos_avg.items(), key=lambda x: x[1], reverse=True)[:50]\n",
        "top_negative = sorted(neg_avg.items(), key=lambda x: x[1], reverse=True)[:50]\n",
        "### END SOLUTION\n",
        "\n",
        "print(\"Top 20 words in POSITIVE reviews:\")\n",
        "for word, score in top_positive[:20]:\n",
        "    print(f\"  {word}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nTop 20 words in NEGATIVE reviews:\")\n",
        "for word, score in top_negative[:20]:\n",
        "    print(f\"  {word}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO0X_3Ktkr2p"
      },
      "source": [
        "### BEGIN SOLUTION\n",
        "**Observations:** Both lists share many words related to movies (film, movie, character, story). However, some sentiment-specific words differ — positive reviews may feature words like \"best\", \"great\", \"excellent\" while negative reviews may include \"bad\", \"worst\", \"boring\". Despite overlap, a classifier could leverage the different TF-IDF weight distributions to distinguish sentiment. The overlap occurs because both types of reviews discuss the same domain (movies).\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing TF-IDF Weights\n",
        "\n",
        "Let's visualize how TF-IDF assigns different weights to words. We use scikit-learn's `TfidfVectorizer` to compute TF-IDF on a small set of example documents and display the result as a heatmap:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Small example corpus\n",
        "example_docs = [\n",
        "    \"the cat sat on the mat\",\n",
        "    \"the dog chased the cat\",\n",
        "    \"the fox jumped over the lazy dog\",\n",
        "    \"a quick brown fox\"\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(example_docs)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame for visualization\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names,\n",
        "                        index=[f\"Doc {i+1}\" for i in range(len(example_docs))])\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "sns.heatmap(tfidf_df, annot=True, fmt=\".2f\", cmap=\"YlOrRd\", linewidths=0.5)\n",
        "plt.title(\"TF-IDF Weights per Document\")\n",
        "plt.ylabel(\"Documents\")\n",
        "plt.xlabel(\"Terms\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nNotice how common words like 'the' get lower weights,\")\n",
        "print(\"while distinctive words like 'mat', 'jumped', 'quick' get higher weights.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Cosine Similarity\n",
        "\n",
        "Once we have vector representations of documents (whether BoW, TF-IDF, or embeddings), we need a way to measure **how similar** two documents are. The most commonly used measure in IR is **cosine similarity**.\n",
        "\n",
        "For two vectors $\\mathbf{a}$ and $\\mathbf{b}$, cosine similarity is defined as:\n",
        "\n",
        "$$\n",
        "\\text{cos}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\, \\|\\mathbf{b}\\|} = \\frac{\\sum_{i=1}^{n} a_i \\, b_i}{\\sqrt{\\sum_{i=1}^{n} a_i^2} \\cdot \\sqrt{\\sum_{i=1}^{n} b_i^2}}\n",
        "$$\n",
        "\n",
        "- The result ranges from $-1$ (opposite) to $1$ (identical direction), with $0$ meaning orthogonal (unrelated).\n",
        "- For non-negative vectors (like TF-IDF), the range is $[0, 1]$.\n",
        "- Cosine similarity measures **angle**, not magnitude — a long document and a short document with the same word proportions will have high similarity.\n",
        "\n",
        "Let's compute the cosine similarity between our example TF-IDF documents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Compute pairwise cosine similarity on TF-IDF vectors\n",
        "cos_sim = cosine_similarity(tfidf_matrix)\n",
        "cos_df = pd.DataFrame(cos_sim, \n",
        "                       index=[f\"Doc {i+1}\" for i in range(len(example_docs))],\n",
        "                       columns=[f\"Doc {i+1}\" for i in range(len(example_docs))])\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cos_df, annot=True, fmt=\".3f\", cmap=\"Blues\", vmin=0, vmax=1, linewidths=0.5)\n",
        "plt.title(\"Cosine Similarity between Documents (TF-IDF)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Documents:\")\n",
        "for i, doc in enumerate(example_docs):\n",
        "    print(f\"  Doc {i+1}: \\\"{doc}\\\"\")\n",
        "print(\"\\nDoc 1 and Doc 2 share 'the' and 'cat' → moderate similarity\")\n",
        "print(\"Doc 3 and Doc 4 share 'fox' → some similarity\")\n",
        "print(\"Doc 1 and Doc 4 share nothing meaningful → low similarity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yams1sh5wljY"
      },
      "source": [
        "## 6. Word2Vec — Dense Word Embeddings\n",
        "\n",
        "So far, our representations have been **sparse** and **high-dimensional** (one dimension per vocabulary word). Word2Vec (Mikolov et al., 2013) learns **dense, low-dimensional** vectors (typically 100-300 dimensions) where **semantically similar words are close together** in vector space.\n",
        "\n",
        "The core idea is the **distributional hypothesis**: *\"A word is characterized by the company it keeps.\"* (Firth, 1957)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br2FrWAyzZtG"
      },
      "source": [
        "Word2Vec has two training architectures:\n",
        "\n",
        "1. **Skip-gram**: Given a center word $w_t$, predict context words $w_{t+j}$ within a window of size $c$. The objective maximizes:\n",
        "\n",
        "$$\n",
        "\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{-c \\leq j \\leq c, \\, j \\neq 0} \\log P(w_{t+j} \\mid w_t)\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "$$\n",
        "P(w_O \\mid w_I) = \\frac{\\exp(\\mathbf{v}'_{w_O} \\cdot \\mathbf{v}_{w_I})}{\\sum_{w=1}^{|V|} \\exp(\\mathbf{v}'_w \\cdot \\mathbf{v}_{w_I})}\n",
        "$$\n",
        "\n",
        "2. **CBOW (Continuous Bag of Words)**: Given context words, predict the center word. Faster to train but less effective on rare words.\n",
        "\n",
        "In practice, **negative sampling** is used instead of the full softmax to make training tractable.\n",
        "\n",
        "The most common implementation for Word2Vec in Python is [gensim](https://radimrehurek.com/gensim/models/word2vec.html). Let's train Word2Vec on our movie reviews:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWcCTa7Vx28V"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences=[doc for doc, cat in documents])\n",
        "word_vectors = model.wv\n",
        "word_vectors['the']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUWGDYUc0qFQ"
      },
      "source": [
        "We can find the most similar vector nearby a word using `most_similar`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbk1ZsaUzrGe"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar('king')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu_9uBVx3Mfu"
      },
      "source": [
        "And we can even do arithmetic with it. The most famous example of this is the `king + man - woman = queen` analogy. By adding the vector of king and man to each other, and subtracting the vector of woman, we should get the queen vector. Lets try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrD-Erlzzuvb"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar(positive=['king','woman'],negative=['man'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfQD-O4P3199"
      },
      "source": [
        "We get queen as the second most similar vector. We only trained our word2vec model on our reviews dataset which is a small dataset for word2vec standards, so that makes sense.\n",
        "\n",
        "Lastly, lets plot the data. For this, we need to represent our vectors as a 2-d space. For this, we need a dimensionality reduction technique, such as PCA or t-SNE. We use t-SNE (invented by someone who did the same master as you are doing!). It might take a while to compute the vectors below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm5VC75O4l8U"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "vectors = tsne.fit_transform(np.asarray(model.wv.vectors))\n",
        "x, y = zip(*vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMg0YUpX8_0R"
      },
      "outputs": [],
      "source": [
        "len(x), len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K__i6RQ7Jzf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.scatter(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdtYf5ehuhM6"
      },
      "source": [
        "## 7. Pretrained Word Embeddings (GloVe)\n",
        "\n",
        "Word2Vec works best with **pretrained embeddings** — vectors trained on massive corpora by researchers and made publicly available.\n",
        "\n",
        "**GloVe** (Global Vectors, Pennington et al., 2014) is a popular alternative to Word2Vec. While Word2Vec uses local context windows, GloVe combines:\n",
        "- **Global co-occurrence statistics** (how often words appear together across the entire corpus)  \n",
        "- **Local context** (word-word co-occurrence within windows)\n",
        "\n",
        "GloVe optimizes the objective:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i,j=1}^{|V|} f(X_{ij}) \\left( \\mathbf{w}_i^T \\tilde{\\mathbf{w}}_j + b_i + \\tilde{b}_j - \\log X_{ij} \\right)^2\n",
        "$$\n",
        "\n",
        "where $X_{ij}$ is the co-occurrence count and $f$ is a weighting function that prevents very common co-occurrences from dominating.\n",
        "\n",
        "Let's download pretrained GloVe vectors (trained on Wikipedia + Gigaword, 6B tokens):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cO1sUsQOu2x6"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader\n",
        "glove = gensim.downloader.load('glove-wiki-gigaword-50')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTBHPYXQvmTI"
      },
      "outputs": [],
      "source": [
        "glove[\"king\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0DHPtgXpXf_"
      },
      "source": [
        "### Exercise 3 — Sentiment Classification with GloVe (4 points)\n",
        "\n",
        "Using our `documents`, perform sentiment classification:\n",
        "\n",
        "1. For each document, get the GloVe pretrained word vector for every word (skip words not in the vocabulary)\n",
        "2. **Average** all word vectors for each document to create a single document embedding\n",
        "3. Split the data into **80/20 train/test** (use `train_test_split` with `random_state=42`)\n",
        "4. Train a `LogisticRegression` classifier on the averaged vectors\n",
        "5. Store predictions as `y_pred` and compute accuracy as `glove_accuracy`\n",
        "\n",
        "> **Hint**: Use `glove[word]` to get vectors. Use `word in glove` to check if a word exists. Use `np.mean(vectors, axis=0)` to average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-v99KUopn_s"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "# Create averaged GloVe vectors for each document\n",
        "X_glove = []\n",
        "y_labels = []\n",
        "for words, categories in documents:\n",
        "    word_vectors = [glove[w] for w in words if w in glove]\n",
        "    if word_vectors:\n",
        "        X_glove.append(np.mean(word_vectors, axis=0))\n",
        "    else:\n",
        "        X_glove.append(np.zeros(50))  # GloVe-50d\n",
        "    y_labels.append(1 if 'pos' in categories else 0)\n",
        "\n",
        "X_glove = np.array(X_glove)\n",
        "y_labels = np.array(y_labels)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_glove, y_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train classifier\n",
        "clf_glove = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_glove.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = clf_glove.predict(X_test)\n",
        "glove_accuracy = accuracy_score(y_test, y_pred)\n",
        "### END SOLUTION\n",
        "\n",
        "print(f\"GloVe + LogisticRegression Accuracy: {glove_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### BEGIN HIDDEN TESTS\n",
        "assert glove_accuracy > 0.55, f\"Accuracy should be above chance (50%), got {glove_accuracy:.4f}\"\n",
        "assert len(y_pred) == len(y_test), \"Predictions should match test set size\"\n",
        "assert X_glove.shape[0] == len(documents), \"Should have one vector per document\"\n",
        "assert X_glove.shape[1] == 50, \"GloVe-50d should produce 50-dimensional vectors\"\n",
        "print(f\"All Exercise 3 tests passed! Accuracy: {glove_accuracy:.4f}\")\n",
        "### END HIDDEN TESTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Rj4cz1zMCeL"
      },
      "source": [
        "## Bias in Word2Vec\n",
        "One of the problems with Word2Vec (and with machine learning in general) is that there is lots of biases assumed by the model. Examples of biases that can be harmful when using these algorithms include gender bias and ethnicity bias. Lets check for example what happens if we take the female equivalent of `doctor`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAjcNaqcMF_T"
      },
      "outputs": [],
      "source": [
        "glove.most_similar(positive=['doctor','woman'],negative=['man'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDdSAOEuT0b5"
      },
      "source": [
        "### Exercise 4 — Bias in Word Embeddings (2 points)\n",
        "\n",
        "Think of **at least 2 other examples** of bias in Word2Vec/GloVe (e.g., gender bias, racial bias, professional stereotypes). Use the `glove.most_similar()` function to demonstrate them with code, and explain why these biases are harmful in real-world applications.\n",
        "\n",
        "> **Hint**: Try analogies like `man:programmer :: woman:?` or `white:wealthy :: black:?`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXAt62XxURRM"
      },
      "source": [
        "### BEGIN SOLUTION\n",
        "**Examples of bias:**\n",
        "\n",
        "1. **Gender bias in professions**: `man:computer_programmer :: woman:homemaker` — the model associates technical professions with men and domestic roles with women. This is harmful because it can reinforce stereotypes in hiring systems or recommendation engines.\n",
        "\n",
        "2. **Racial/ethnic bias**: Word embeddings trained on internet text encode societal biases about race, associating certain ethnicities with negative attributes. This is harmful in applications like resume screening, criminal justice risk assessment, or content moderation.\n",
        "\n",
        "These biases exist because the training data (web text, news articles) reflects historical and societal prejudices. When these embeddings are used in downstream applications (search engines, chatbots, hiring tools), they can perpetuate and amplify discrimination.\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIVT4MwNUQ86"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "# Example 1: Gender bias in professions\n",
        "print(\"man:programmer :: woman:?\")\n",
        "print(glove.most_similar(positive=['programmer', 'woman'], negative=['man'])[:5])\n",
        "\n",
        "print(\"\\nman:doctor :: woman:?\")\n",
        "print(glove.most_similar(positive=['doctor', 'woman'], negative=['man'])[:5])\n",
        "\n",
        "# Example 2: Try another analogy\n",
        "print(\"\\nfather:doctor :: mother:?\")\n",
        "print(glove.most_similar(positive=['doctor', 'mother'], negative=['father'])[:5])\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGeOKjbPmc46"
      },
      "source": [
        "## 8. Sentence Transformers — Contextual Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmZj5iBvmhOX"
      },
      "source": [
        "A key limitation of Word2Vec and GloVe is that they produce **static embeddings** — each word has exactly one vector regardless of context. The word \"bank\" gets the same vector whether it means \"river bank\" or \"financial bank\".\n",
        "\n",
        "**Transformer-based models** (like BERT) solve this by producing **contextual embeddings** — the vector for each word depends on its surrounding context. These models use the **self-attention mechanism**:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
        "$$\n",
        "\n",
        "where $Q$, $K$, $V$ are query, key, and value matrices derived from input embeddings, and $d_k$ is the dimension of the keys.\n",
        "\n",
        "**Sentence Transformers** (Reimers & Gurevych, 2019) extend BERT by applying **mean pooling** over all token embeddings to produce a single fixed-size vector for an entire sentence. This is efficient and well-suited for tasks like:\n",
        "- Semantic search\n",
        "- Sentence similarity\n",
        "- Clustering\n",
        "- Sentiment classification\n",
        "\n",
        "| Feature | Word2Vec / GloVe | Sentence Transformers |\n",
        "|:--------|:-----------------|:---------------------|\n",
        "| Type | Static | Contextual |\n",
        "| Granularity | Word-level | Sentence-level |\n",
        "| Polysemy | One vector per word | Context-dependent |\n",
        "| Training data | Co-occurrence | Masked language model |\n",
        "| Typical dims | 50–300 | 384–1024 |\n",
        "\n",
        "Let's load a sentence transformer model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvSjvFzQm-g-"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRRDncajnI0R"
      },
      "source": [
        "We can encode any sentence like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGubLNl4nBwc"
      },
      "outputs": [],
      "source": [
        "sentence_embedding = sentence_model.encode(\"the quick brown fox jumps over the lazy dog\")\n",
        "sentence_embedding.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE9xT8UJnR5s"
      },
      "source": [
        "We now get a vector of 384 instead of a matrix of 11 by 768. This makes it much easier to deal with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT8Xg5I6eW_W"
      },
      "source": [
        "### Exercise 5 — Sentiment Classification with Sentence Transformers (4 points)\n",
        "\n",
        "Now repeat the sentiment classification task using **Sentence Transformers** instead of GloVe:\n",
        "\n",
        "1. Convert all documents to sentence strings (join the word lists with spaces)\n",
        "2. Use `sentence_model.encode()` to compute embeddings for all documents\n",
        "3. Use the same 80/20 split with `random_state=42`\n",
        "4. Train a `LogisticRegression` classifier\n",
        "5. Store predictions as `y_pred_st` and accuracy as `st_accuracy`\n",
        "\n",
        "Then answer: Do you see a difference compared to Exercise 3? Why might that be?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu12iOsop8qE"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "# Convert documents to sentence strings\n",
        "doc_strings = [\" \".join(words) for words, cat in documents]\n",
        "\n",
        "# Encode with sentence transformers\n",
        "X_st = sentence_model.encode(doc_strings, show_progress_bar=True)\n",
        "\n",
        "# Same labels as before\n",
        "X_train_st, X_test_st, y_train_st, y_test_st = train_test_split(\n",
        "    X_st, y_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train classifier\n",
        "clf_st = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_st.fit(X_train_st, y_train_st)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_st = clf_st.predict(X_test_st)\n",
        "st_accuracy = accuracy_score(y_test_st, y_pred_st)\n",
        "### END SOLUTION\n",
        "\n",
        "print(f\"Sentence Transformers + LogisticRegression Accuracy: {st_accuracy:.4f}\")\n",
        "print(f\"GloVe + LogisticRegression Accuracy:                 {glove_accuracy:.4f}\")\n",
        "print(f\"Improvement: {(st_accuracy - glove_accuracy)*100:+.1f} percentage points\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### BEGIN HIDDEN TESTS\n",
        "assert st_accuracy > 0.55, f\"Sentence Transformer accuracy should be above chance, got {st_accuracy:.4f}\"\n",
        "assert len(y_pred_st) == len(y_test_st), \"Predictions should match test set size\"\n",
        "assert X_st.shape[0] == len(documents), \"Should have one embedding per document\"\n",
        "assert X_st.shape[1] == 384, \"all-MiniLM-L6-v2 should produce 384-dimensional vectors\"\n",
        "print(f\"All Exercise 5 tests passed! Accuracy: {st_accuracy:.4f}\")\n",
        "### END HIDDEN TESTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkF-yx-Yp2op"
      },
      "source": [
        "Do you see a difference between the accuracy at Exercise 3 (GloVe) and Exercise 5 (Sentence Transformers)? Why do you think this is? How could we further improve accuracy?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYHX3RD7p-6E"
      },
      "source": [
        "### BEGIN SOLUTION\n",
        "Sentence Transformers typically achieve higher accuracy than GloVe because:\n",
        "1. **Contextual embeddings** capture word meaning in context, while GloVe uses static vectors\n",
        "2. **Sentence-level representations** capture the overall meaning instead of just averaging word vectors\n",
        "3. **Pre-training on NLI tasks** makes sentence transformers better at understanding semantic relationships\n",
        "\n",
        "To further improve accuracy, we could:\n",
        "- Fine-tune the sentence transformer on our specific dataset\n",
        "- Use a larger transformer model (e.g., `all-mpnet-base-v2`)\n",
        "- Use a more powerful classifier (e.g., fine-tuned BERT end-to-end)\n",
        "- Augment the training data\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary — Comparison of Document Representation Methods\n",
        "\n",
        "| Method | Type | Dimensions | Captures Semantics? | Handles Polysemy? | Key Advantage | Key Limitation |\n",
        "|:-------|:-----|:-----------|:--------------------|:-----------------|:-------------|:--------------|\n",
        "| **One-Hot** | Sparse, binary | $|V|$ (10K+) | No | No | Simple, interpretable | No similarity between words |\n",
        "| **Bag-of-Words** | Sparse, count | $|V|$ | No | No | Captures word frequency | Ignores word order and importance |\n",
        "| **TF-IDF** | Sparse, weighted | $|V|$ | Partially | No | Weights by importance | Still high-dimensional, no semantics |\n",
        "| **Word2Vec** | Dense, static | 100–300 | Yes | No | Captures analogies and similarity | One vector per word |\n",
        "| **GloVe** | Dense, static | 50–300 | Yes | No | Combines global + local statistics | One vector per word |\n",
        "| **Sentence Transformers** | Dense, contextual | 384–1024 | Yes | Yes | Context-aware, sentence-level | Computationally expensive |\n",
        "\n",
        "**The evolution**: From sparse, high-dimensional, context-free representations → dense, low-dimensional, context-aware embeddings. Each step captures more linguistic information but requires more computational resources."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv (3.12.10)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
