{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7tNwXXJ_BBb"
      },
      "source": [
        "## Start by copying this into your Google Drive!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIBbWrF5_DXC"
      },
      "source": [
        "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
        "\n",
        "# Information Retrieval and Text Mining Course - Tutorial Information Extraction\n",
        "Author: Jan Scholtes and Gijs Wijngaard\n",
        "\n",
        "Edition 2024-2025\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14xe-BeO_G34"
      },
      "source": [
        "Welcome to the tutorial on Information Extraction. In this notebook you will learn several methods for information extraction. We will start with Named Entity Recognition in NLTK. Next we will look at BERT and Context Sensitive Embeddings, then move on to Sentiment Analysis with BERT and finally return to NER with BERT."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NTLK and Named Entity Extraction\n",
        "\n"
      ],
      "metadata": {
        "id": "3JM-ZSJfCLsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named-entity recognition (NER) is a problem that has a goal to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, etc.\n",
        "\n",
        "NER is used in many fields in NPL and it can help to answer some questions such as:\n",
        "Which companies and persons are mentioned in the documents?\n",
        "In which articles or posts is specified product mentioned?\n",
        "Does the article contain medical terms and which ones?\n",
        "State-of-the-art NER systems for English produce near-human performance where the best system scored 93.39% of F-measure while human annotators have a score around 97%.\n",
        "\n",
        "Named-entity recognition is often broken down into two distinct problems:\n",
        "Detection of names\n",
        "Classification of the names by the type of entity they refer to (person, organization or location)\n",
        "Detection of names is typical simplified to a segmentation problem where a single name might be constructed of several substrings. For example \"Bank of America\" is a single name despite that the substring \"America\" is itself a name. Classification of names requires choosing an ontology by which to organize categories of things.\n",
        "\n",
        "While doing NER, besides the correct and incorrect predicted terms, we'll probably face some \"partially correct\" predictions. For example:\n",
        "Uncomplete names (missing the last token of \"John Smith, M.D.\")\n",
        "Names with more tokens (including the \"mr.\" token in \"mr. John Smith\")\n",
        "Partitioning adjacent entities differently (treating two names as one \"Hans, Jones Blick\")\n",
        "Assigning related but inexact type (for example, \"substance\" vs. \"drug\", or \"school\" vs. \"organization\")\n",
        "Correctly identifying an entity, when what the user wanted was a smaller- or larger-scope entity (for example, identifying \"James Madison\" as a personal name, when it's part of \"James Madison University\")"
      ],
      "metadata": {
        "id": "RNt6BSrGDD4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applications of Named Entity Extraction\n",
        "\n",
        "NER can be appllied in many real-world situations when analyzing a large quantity of text is helpful. Some examples of NER includes:\n",
        "Improve customer support by categorizing and filtering user requests, complaints, and questions. It can help businesses obtain insights about their customers.\n",
        "Help categorize applicants’ CVs and speed up the process.\n",
        "Improve search and recommendation engines using recognized entities.\n",
        "Search and extract useful information from documents and blog posts."
      ],
      "metadata": {
        "id": "VYv9k8V6DI8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NLTK"
      ],
      "metadata": {
        "id": "3UwFZCA4CPvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Tagging (Parts of Speech Tagging) is a process to mark up the words in text format for a particular part of a speech based on its definition and context. It is responsible for text reading in a language and assigning some specific token (Parts of Speech) to each word. It is also called grammatical tagging."
      ],
      "metadata": {
        "id": "Yd6a-MnEDW3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "ex = 'Experts in France begin examining airplane debris found on Reunion Island: French air accident experts stated last Wednesday'\n",
        "# tokenize the sencence and apply POS tagging\n",
        "sent = pos_tag(word_tokenize(ex))\n",
        "sent"
      ],
      "metadata": {
        "id": "247RdenzDZgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chunking\n",
        "Chunking in NLP is a process of grouping small pieces of information into large units. The primary use of Chunking is making groups of \"noun phrases.\" It is used to add structure to the sentence by following POS tagging combined with regular expressions. The resulted group of words are called \"chunks.\" There are no pre-defined rules for Chunking, but we can make according to our needs. Thus, if we want to chunk only 'NN' tags, we need to use pattern.\n",
        "\n",
        "\n",
        "`mychunk:{<NN>}`\n",
        "\n",
        "but if we need to chunk all types of tags which start with 'NN', we'll use\n",
        "\n",
        "`mychunk:{<NN.*>}`.\n",
        "\n",
        "More about regex patterns can be found here"
      ],
      "metadata": {
        "id": "jLBEwWh0DiYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "id": "NN4pYSxTD0P1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import RegexpParser\n",
        "from nltk.draw.tree import TreeView\n",
        "from IPython.display import Image\n",
        "import svgling\n",
        "\n",
        "# chunk all adjacence nouns\n",
        "patterns= \"\"\"mychunk:{<NN.*>+}\"\"\"\n",
        "chunker = RegexpParser(patterns)\n",
        "output = chunker.parse(sent)\n",
        "print(\"After Chunking\",output)\n",
        "svgling.draw_tree(output)"
      ],
      "metadata": {
        "id": "sV7yEKmPD217"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly as part-of-speech tags, IOB tags are a slightly different way for representing chunk structures. This format can denote the inside, outside, and beginning of a chunk."
      ],
      "metadata": {
        "id": "6wa2py3hD7nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "\n",
        "from pprint import pprint\n",
        "\n",
        "iob_tagged = tree2conlltags(output)\n",
        "iob_tagged"
      ],
      "metadata": {
        "id": "siOpL8frD9Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NER\n",
        "\n",
        "Recognizing a named entity is a specific kind of chunk extraction that uses entity tags along with chunk tags. Common entity tags include PERSON, LOCATION, and ORGANIZATION. NLTK has already a pre-trained named entity chunker which can be used using ne_chunk() method in the nltk.chunk module."
      ],
      "metadata": {
        "id": "9IaMt_20D_1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "from nltk.chunk import ne_chunk\n",
        "\n",
        "def extract_ne(trees, labels):\n",
        "\n",
        "    ne_list = []\n",
        "    for tree in ne_res:\n",
        "        if hasattr(tree, 'label'):\n",
        "            if tree.label() in labels:\n",
        "                ne_list.append(tree)\n",
        "\n",
        "    return ne_list\n",
        "\n",
        "ne_res = ne_chunk(pos_tag(word_tokenize(ex)))\n",
        "labels = ['ORGANIZATION']\n",
        "\n",
        "print(extract_ne(ne_res, labels))"
      ],
      "metadata": {
        "id": "7wXUliteEE1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try a larger text."
      ],
      "metadata": {
        "id": "4MycXWMRGDFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "article = 'Bill Gates ia one of the founders from Microsoft Corporation. Bill Gates lives in Seattle in the Washington State. I wrote all of this myself!! Special Chair of Text-Mining Jan Scholtes on how ChatGPT actually works, why it’s an amazing achievement and where we should probably exercise a bit of caution. “My students were very excited! We had discussed the mathematics of a previous GPT-version just the week before so they knew exactly how it worked. Within 24 hours of the release, they were using it to answer questions in the tutorial of my course on advanced natural language processing.” Thus Jan Scholtes, Special Chair of Text-Mining at the Faculty of Science and Engineering’s Department of Advanced Computing Sciences, on the release of AI chatbot ChatGPT last December. “The reason it was such a success is because it’s the first computational linguistic model that takes all characteristics, problems and features of natural language seriously. So far, all of them have taken shortcuts due to the complexity of human language.” Scholtes points out that GPT even passed a draft exam for his course Advanced Natural Language Processing with excellent grades. The transformer model The GPT models are based on Google’s transformer architecture, which was introduced in 2017. “The original transformer includes an encoder and a decoder, designed to deal with complex sequence-to-sequence patterns that are both left- and right-side context-sensitive.” The latter refers to the meaning of a word only becoming evident from the context, i.e. preceding and following words. Both the encoder and decoder have several layers of self-attention, in the case the large version of GPT-3, which is the architecture used for ChatGPT, a full 96, which is how it can deal with linguistic complexity and master phenomena from punctuation, to morphology, to syntax, to semantics, to more complex relations. In the case of Google Translate that would mean, for example, that the encoder creates a numeric representation of a sentence and extracts its features, and the decoder uses those features to generate an output sentence, i.e. the translation. Having been trained on vast amounts of text in the target language, the decoder predicts e.g. the most likely word order of the translation stochastically. The translation is created iteratively, i.e. word by word, with each next-word suggestion (similar to predictive texting) from the decoder going through the self-attention loops to improve the level of disambiguation (e.g. whether ‘piano piece’ implies ‘mechanical part’ or ‘musical composition’). “This is a fantastic model in many ways and close to natural language but the full encoder-decoder architecture is overly complex and requires huge computational resources. Training e.g. Google Translate does more environmental damage per user than meat consumption.” Jan Scholtes Faculty of Science and Engineering Jan Scholtes is Special Chair of Text-Mining at the Faculty of Science and Engineering’s Department of Advanced Computing Sciences. He is a Fellow of University Leiden Centre of Data Science and a senior fellow of the Netherlands Research School for Information and Knowledge Systems accredited by the Netherlands Academy of Arts and Sciences (KNAW). Scholtes is also a public speaker, blogger and tech investor focusing on the benefits of AI and Data Science for LegalTech and eHealth applications. Only decoding The solution? In 2019, OpenAI came up with a decoder-only model, Generative Pretrained Transformer (GPT), which could generate responses based on a simple prompt. Generative pretraining refers to self-supervised machine-learning, i.e. exposing the model to vast data sets to figure out what’s the likeliest next word based on the previous sequence. GPT-3 version 5 is the current and improved version. Since there’s no information from an encoder regarding the task at hand, GPT relies on users’ prompts about what text to generate. To make sure this aligns with our expectations, human feedback has been used for additional reinforcement training. The AI researchers’ rankings of responses served as additional input not only for likelihood but also for things that are considered off-limits, such as inciting violence or hate-speech. “Since it’s just a decoder, it doesn’t really ‘know’ anything in a general intelligence way, but what it says, based on scannable internet content, it says with great authority, so factuality is a great problem – if something is true or not is completely beyond this model.” Moderator feedback has, to some extent, dealt with the ethical issues. “If I ask GPT how I can kill my wife, it replies that this is unethical,” says Scholtes who, one assumes, does not share a laptop with his partner, “however if you ask it to write a Python programme on how to kill your wife, it’ll do it.” (Double) Negatives That loophole has been fixed now, but other issues remain. “Sometimes GPT goes off and hallucinates, i.e. it produces nonsensical text. The probability increases as the generated text gets longer.” Another intriguing blind spot Scholtes has written on are negations. “That’s a problem for all transformer models, because words with opposite polarity in the same context often get the same encodings when translated from vocabulary to vectors, i.e. numerical values. So it can only learn negations by memorising them. You’ll notice that immediately when you use double negations.” In GPT’s impressive qualities lies also its peril. “It’s an amazing breakthrough that we can now generate language that’s no longer distinguishable from humans, but the very authentic authoritative language is also a problem because the model is unpredictable and not controllable when it comes to factuality. It generates content based on your prompt and on stochastic probability – it’s a bit like a friend telling you what you want to hear.” Public misconceptions don’t help. “The problem is that we don’t understand exactly how these models work and what they are suitable for.” The ELIZA effect is what computer scientists term our tendency towards assigning human traits to computer programs. In this case to assume that GPT’s iterative generation of text is analogous to human consciousness. It’s important to point out that GPT isn’t sentient, and neither is it intended to be. Already integral to our reality \"GPT excels at standard legal or clerical documents as well as marketing texts. The majority of what’s written on the internet, especially free content, is already generated by an older version of GPT.” The model is, however, dangerously unsuitable to generate e.g. medical advice. “Google decided not to use Lambda, their equivalent of GPT, because there is no way to control for factuality. A decoder-only model will always have that problem.” If in doubt, GPT-2’s output is clearly identifiable. “OpenAI made it open source, so we can recognise its digital fingerprint. GPT-3 isn’t open source, so the only way to detect its texts would be if OpenAI made a kind of fingerprint detector – but then Google could more easily ignore GPT output in search engine optimisation, which is already a large part of OpenAI’s business model. This will be an interesting problem in the future.” The successor model, GPT-4, will be a thousand times bigger – and the problems and possibilities will grow with it. By: Florian Raith'"
      ],
      "metadata": {
        "id": "XkAuLTHrHJVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize the article into sentences: sentences\n",
        "print(article)\n",
        "sentences = sent_tokenize(article)\n",
        "\n",
        "# Tokenize each sentence into words: token_sentences\n",
        "token_sentences = [word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Tag each tokenized sentence into parts of speech: pos_sentences\n",
        "pos_sentences = [nltk.pos_tag(sent) for sent in token_sentences]\n",
        "\n",
        "# Create the named entity chunks: chunked_sentences\n",
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=True)\n",
        "\n",
        "# ner_categories = defaultdict(int)\n",
        "# print(ner_categories)\n",
        "\n",
        "# Test for stems of the tree with 'NE' tags\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, \"label\"):\n",
        "            print(chunk)\n",
        "\n"
      ],
      "metadata": {
        "id": "LFgCDIERGFuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, here, we have only recognized that a text phrase is a Named Entity (NE), but not which one. We can now classify them into the categories known to NLTK: PERSON (Per) ORGANIZATION (Org) Geographical Entity (Geo) and Geopolitical Entity (GPE), which is everything with a governing body like cities and countries."
      ],
      "metadata": {
        "id": "Me8z3XgGPCc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunked_sentences = nltk.ne_chunk_sents(pos_sentences, binary=False)"
      ],
      "metadata": {
        "id": "KcoddOxXOdB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also categorize extracted named entities."
      ],
      "metadata": {
        "id": "yem8MyAGGexj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "# Create the defaultdict: ner_categories\n",
        "ner_categories = defaultdict(int)\n",
        "\n",
        "# Create the nested for loop\n",
        "for sent in chunked_sentences:\n",
        "    for chunk in sent:\n",
        "        if hasattr(chunk, 'label'):\n",
        "            ner_categories[chunk.label()] += 1\n",
        "            print(chunk)\n",
        "\n",
        "# Create a list from the dictionary keys for the cart labels: labels\n",
        "labels = list(ner_categories.keys())\n",
        "\n",
        "# Create a list of the values: values\n",
        "values = [ner_categories.get(l) for l in labels]\n",
        "\n",
        "# Create the pie chart\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140);"
      ],
      "metadata": {
        "id": "qB9p7N9HGpfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1:\n",
        "\n",
        "1. What do you observe with respect to the quality of the NE detection and the classification?\n",
        "\n",
        "2. How could we improve on that?"
      ],
      "metadata": {
        "id": "gj2PSDO_QbAA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "d8URklv5QmoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this notebook we would need our GPU again (in the end). You can activate your GPU by clicking on Runtime, then Change runtime type and pressing GPU. If you hit the Google's GPU limits for usage, you can use other free GPU services such as Kaggle GPU's (recommended), Amazon's GPU's or Paperspace Gradient or of course your local GPU on your computer (if you have one)."
      ],
      "metadata": {
        "id": "jqxiBArNRtFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BERT and Huggingface\n",
        "\n",
        "So far, we have worked with a various amount of BERT models. BERT has a lot to offer:\n",
        "\n",
        "a) pretrained models\n",
        "\n",
        "b) bi-directional word embeddings\n",
        "\n",
        "c) ability to deal with out-of vacabulairy words using sub-words (including byte pair enciding, byte-level byte pairing and the WordPiece tokenizer)\n",
        "\n",
        "d) mask language modeling\n",
        "\n",
        "e) next sentence prediction\n",
        "\n",
        "Remember, there are 1000's of different BERT models, varying in size (different number of encoding layers L, different number of attention heads A, different number of hidden units H), and with different pre-training data."
      ],
      "metadata": {
        "id": "OwuWa6ewCScL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjTrV91KLrSC"
      },
      "source": [
        "Hugging Face is an organization that is behind the very popular open-source library `transformers`. It is very useful and powerful for several NLP and NLU tasks. Transformers models work well when already pretrained, and the `transformers` library includes thousands of pre-trained models in about 100+ natural languages. Its ease of use and extendability is also a plus. You can run state-of-the-art models with just a few lines of code.\n",
        "\n",
        "We can install `transformers` (and the `datasets` library) directly using pip as shown in the following:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "o7PQZSHt_FKT"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets make sure we get the same results every time we run the same code in the notebook (reproducability). We can do this by setting the seeds of the packages we use. Like this:"
      ],
      "metadata": {
        "id": "fHnYEcQFQq4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "miUHuY2hQqK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary and Tokenizers"
      ],
      "metadata": {
        "id": "21r9pHeldV4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can tokenize a piece of text by importing the tokenizer from the `transformers` library. Then to tokenize we use the `tokenize()` function. This function splits the texts based on the items in the vocabulary."
      ],
      "metadata": {
        "id": "IkUZmXK1epUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "p_hMv2uZe35T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT also has extra tokens involved, for example:\n",
        "\n",
        "1. The classification token `[CLS]`. This token is for classification of the transformer. We attend to this token and we can take an embedding representation of a whole sentence by just using this token.\n",
        "2. The separation token `[SEP]`. This token is to let the model know we have separate sentences within a full text we pass to the model."
      ],
      "metadata": {
        "id": "AQq9S2rm11jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Here is the sentence I want embeddings for.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "\n",
        "# Tokenize our sentence with the BERT tokenizer.\n",
        "tokenized_text = tokenizer.tokenize(marked_text)\n",
        "\n",
        "# Print out the tokens.\n",
        "tokenized_text"
      ],
      "metadata": {
        "id": "xM7H5JkefIRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This text is not yet something we can pass through our model yet. For that, we need it converted to integers (ids). We can do this with the `convert_tokens_to_ids` function:"
      ],
      "metadata": {
        "id": "Nk4bnxAd1mmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.convert_tokens_to_ids(tokenized_text)"
      ],
      "metadata": {
        "id": "mICf1JCk1ebY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function just looks up the tokens in the vocabulary. For example, the id 5000 means `knight`, because it is the 5000th index in the vocabulary:"
      ],
      "metadata": {
        "id": "CflPdyWr3zqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(tokenizer.vocab.keys())[5000:5020]"
      ],
      "metadata": {
        "id": "ZRywEv0Z3zJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can convert ids back to tokens with the `convert_ids_to_tokens` function:"
      ],
      "metadata": {
        "id": "4qy-eYrP5eky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "tokenizer.convert_ids_to_tokens(ids)"
      ],
      "metadata": {
        "id": "i2wULAmP5PvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normally, we would just use `encode` or `encode_plus`. `encode` adds the `CLS` and `SEP`, add the `convert_tokens_to_ids` and `tokenize` steps into one step. `encode_plus` does this and also appends a `attention_mask` and `token_type_ids`."
      ],
      "metadata": {
        "id": "GOQcC1RU2-IZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "p7a4dC1O3b8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQhSjlX8LrSF"
      },
      "source": [
        "## BERT Models\n",
        "In this section, we will learn how to extract embeddings from the pre-trained BERT. Consider the sentence 'I love Maastricht'. Let's see how to obtain the contextualized word embedding of all the words in the sentence using the pre-trained BERT model with Hugging Face's transformer library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_k8AHt0LrSH"
      },
      "source": [
        "Lets consider the `HuggingFace` library. We can check all the available pre-trained models [here](https://huggingface.co/models). For BERT, these models we can filter down on the `bert` [tag](https://huggingface.co/models?other=bert). For now, we use the [bert-base-uncased](https://huggingface.co/bert-base-uncased) model. As the name suggests, it a BERT with 12 encoders and it is trained with uncased tokens. The representation size will be 768. The `uncased` means that we have only lowercase letters in our tokenizer.\n",
        "\n",
        "We can download and load the pretrained model like this. Lets look how the model is implemented. Can you notice the 12 layers/encoders of the model? Also notice the different type of inputs we have in the embedding layer. The word embedding, that converts the token ids we have (30522 of them) into 768. Same holds for the position embeddings and the token_type_embeddings we could put in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swqqjz08_FKj"
      },
      "outputs": [],
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8El2dcG_FKs"
      },
      "source": [
        "## Preprocessing the input\n",
        "Define the sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8Hkc5o3B_FKu"
      },
      "outputs": [],
      "source": [
        "sentence = 'I love AI'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVn3-2XM_FKv"
      },
      "source": [
        "Tokenize the sentence and obtain the tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "4Ui-3oWr_FKw"
      },
      "outputs": [],
      "source": [
        "tokens = tokenizer.tokenize(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSYRLcvV_FKy"
      },
      "source": [
        "Let's print the tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGHtPj5H_FKz"
      },
      "outputs": [],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rL8VgpcM_FK0"
      },
      "source": [
        "Now, we will add the `[CLS]` token at the beginning and `[SEP]` token at the end of the tokens list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3CpARvDc_FK0"
      },
      "outputs": [],
      "source": [
        "tokens = ['[CLS]'] + tokens + ['[SEP]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbFEyrk7_FK2"
      },
      "source": [
        "Let's look at our updated tokens list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uaf_JScw_FK3"
      },
      "outputs": [],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVD29L7A_FK3"
      },
      "source": [
        "As we can observe, we have `[CLS]` token at the beginning and sep token at the end of our tokens list. We can also observe that length of our tokens is 5.\n",
        "\n",
        "Say, we need to keep the length of our tokens list to 7, then, in that case, we will add two `[PAD]` tokens at the end as shown in the following:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cqMI--rA_FK4"
      },
      "outputs": [],
      "source": [
        "tokens = tokens + ['[PAD]'] + ['[PAD]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEEHs7CV_FK6"
      },
      "source": [
        "Let's print our updated tokens list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tKmh_hk_FK6"
      },
      "outputs": [],
      "source": [
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPd9TPWE_FK7"
      },
      "source": [
        "\n",
        "\n",
        "As we can observe, now we have the tokens list consists of `[PAD]` tokens and the length of our tokens list is 7.\n",
        "\n",
        "Next, we create the attention mask. The attention mask is there to let the model know which items should be taken into account when calculating the attentions. Since `[PAD]` tokens are just to pad the string and do not have any semantic meaning, we should let the model know not to take them into account.\n",
        "\n",
        "We set the attention mask value to 1 if the token is not a `[PAD]` token else we will set the attention mask to 0 as shown below:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3SbiaBws_FK8"
      },
      "outputs": [],
      "source": [
        "attention_mask1 = [1 if i!= '[PAD]' else 0 for i in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SELKgxsG_FK9"
      },
      "source": [
        "Let's print the attention_mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmtZKFFq_FK-"
      },
      "outputs": [],
      "source": [
        "print(attention_mask1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvtvC8HW_FK-"
      },
      "source": [
        "\n",
        "As we can observe, we have attention mask values 0 at the position where have `[PAD]` token and 1 at other positions.\n",
        "\n",
        "Next, we convert all the tokens to their token_ids as shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KGbsHfxL_FK_"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaQWwz_y_FK_"
      },
      "source": [
        "\n",
        "Let's have a look at the token_ids:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iih_wWeT_FLA"
      },
      "outputs": [],
      "source": [
        "print(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9RL1cBs_FLA"
      },
      "source": [
        "\n",
        "From the above output, we can observe that each token is mapped to a unique token id.\n",
        "\n",
        "Now, we convert the token_ids and attention_mask to tensors as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KQzsDUez_FLB"
      },
      "outputs": [],
      "source": [
        "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
        "attention_mask1 = torch.tensor(attention_mask1).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQn5de_a_FLC"
      },
      "source": [
        "\n",
        "That's it. Next, we feed the token_ids and attention_mask to the pre-trained BERT model and get the embedding."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYIDWAO-_FLD"
      },
      "source": [
        "## Getting the embedding\n",
        "\n",
        "As shown in the following code, we feed the token_ids, and attention_mask to the model and get the output of the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Rh-Ohh71_FLE"
      },
      "outputs": [],
      "source": [
        "print(token_ids)\n",
        "print(attention_mask1)\n",
        "last_hidden_state, pooler_output = model(token_ids, attention_mask = attention_mask1).to_tuple()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Kfvd3F_FLF"
      },
      "source": [
        "This output contains two keys or more keys, depending on the task at hand. In the case of `bert-base-uncased`, we get"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx1RfsaU_FLF"
      },
      "outputs": [],
      "source": [
        "last_hidden_state.shape, pooler_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `pooler_output` is basically the `last_hidden_state`'s `[CLS]` token through the pooling module (linear layer + activation) at the end:"
      ],
      "metadata": {
        "id": "9Dsj4xWesaSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.allclose(pooler_output, model.pooler(last_hidden_state))"
      ],
      "metadata": {
        "id": "C4IjdcYzs6HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWxFowIS_FLG"
      },
      "source": [
        "The size `[1,7,768]` indicates the `[batch_size, sequence_length, hidden_size]`.\n",
        "\n",
        "Our batch size is 1, the sequence length is the token length, since we have 7 tokens, the sequence length is 7, and the hidden size is the representation (embedding) size and it is 768 for the BERT-base model.\n",
        "\n",
        "We can obtain the representation of each token as:\n",
        "\n",
        "- `output.last_hidden_state[0][0]` gives the representation of the first token which is `[CLS]`\n",
        "- `output.last_hidden_state[0][1]` gives the representation of the second token which is 'I'\n",
        "- `output.last_hidden_state[0][2]` gives the representation of the third token which is 'love'.\n",
        "\n",
        "We can also index it with `output.last_hidden_state[:, 0, :]`. In this way, we specify that we want to have the zeroth element on the second index, we get a matrix in return with both the batch size and the embedding size (since we put a colon there).\n",
        "\n",
        "In this way, we can obtain the contextual representation of all the tokens. This is basically the contextualized word embeddings of all the words in the given sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        "1. Now write your own code. Put the following text (the one defined under here with three times `bank` in it) to the tokenizer, add `[CLS]` and `[SEP]` values to it and pass it through the model. Get the vector representations of each of the three tokens of `bank` in the sentence and compare them.\n",
        "2. How does this differ from putting the text through a Word2Vec model?\n",
        "2. Put a string with only \"bank\" (add `[CLS]` and `[SEP]`) through the tokenizer and model. How does this vector of the word \"bank\" differ from the other \"bank\" vectors?"
      ],
      "metadata": {
        "id": "zzsYzhiRaW1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WRITE YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "m3u2M6MHe5Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
        "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
        "       \"fishing on the Mississippi river bank.\"\n",
        "# WRITE YOUR CODE HERE"
      ],
      "metadata": {
        "id": "kesZGOOhe4-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLbXrzA0_ndC"
      },
      "source": [
        "# Extracting embeddings from all encoder layers of BERT\n",
        "We learned how to extract the embedding from the pre-trained BERT in the previous section. We learned that they are the embeddings obtained from the final encoder layer. Now the question is should we consider the embedding obtained only from the final encoder layer (final hidden state), or should we also consider the embedding obtained from all the encoder layers (all hidden states)? Let's explore more about this.\n",
        "\n",
        "Instead of taking the embeddings (representation) only from the final encoder layer, the researchers of the BERT have experimented with taking embeddings from different encoder layers.\n",
        "\n",
        "For instance, for a named-entity recognition task, the researchers have used the pre-trained BERT for extracting features. Instead of using the embedding only from the final encoder layer (final hidden layer) as a feature, they have experimented using embedding  from other encoder layers (other hidden layers) as a feature and obtained the following F1 score:\n",
        "<a name=\"table\"></a>\n",
        "$$\n",
        "\\begin{array}{lll}\n",
        "\\textbf{Feature}                 & \\textbf{Notation} & \\textbf{F1 score} \\\\ \\hline\n",
        "\\text{Embeddings}                       & h_0              & 91.0             \\\\\n",
        "\\text{Second to last hidden}            & h_{11}         & 95.6             \\\\\n",
        "\\text{Last hidden}                      & h_{12}         & 94.9             \\\\\n",
        "\\text{Sum of last four hidden} & h_9\\text{ to }h_{12} & 95.9             \\\\\n",
        "\\text{Concat last four hidden}          & h_9\\text{ to }h_{12} & 96.1             \\\\\n",
        "\\text{Sum of all 12 layers}     & h_1\\text{ to }h_{12} & 95.5            \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "As we can observe from the preceding table, concatenating the embeddings of the last 4 encoder layers (last 4 hidden layers) gives us a greater F1 score of 96.1% in the  NER task. Thus, instead of taking the embeddings only from the final encoder layer (final hidden layer), we can also use embeddings from the other encoder layers.\n",
        "\n",
        "Now, we will learn how to extract the embeddings from all the encoder layers using the transformers library.\n",
        "\n",
        "## Extracting the embeddings\n",
        "First, let us import the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KSwq5tr9_ndJ"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oI91BJ8H_ndJ"
      },
      "source": [
        "\n",
        "Next, download the pre-trained BERT model and tokenizer. As we can notice while downloading the pre-trained BERT model. We need to set `output_hidden_states= True`. By setting this to true helps us to obtain embeddings from all the encoder layers:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM0UkRBp_ndK"
      },
      "outputs": [],
      "source": [
        "model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6lcm9mn_ndK"
      },
      "source": [
        "\n",
        "Next, we preprocess the input before feeding it to the model.\n",
        "\n",
        "## Preprocess the input\n",
        "Let's consider the same sentence we saw in the previous section. First, we tokenize the sentence and add [CLS] token at the beginning and [SEP] token at the end:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1-hLzFu6_ndL"
      },
      "outputs": [],
      "source": [
        "sentence = 'I love AI'\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUKVAdmT_ndL"
      },
      "source": [
        "\n",
        "Suppose, we need to keep the token length to 7. So, we add the [PAD] tokens and also define the attention mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5OISI_-t_ndM"
      },
      "outputs": [],
      "source": [
        "tokens = tokens + ['[PAD]'] + ['[PAD]']\n",
        "attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic5yVHnB_ndM"
      },
      "source": [
        "\n",
        "Next, we convert the tokens to their token_ids:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8YtAw5Gr_ndM"
      },
      "outputs": [],
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MmpLWP6_ndN"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Now, we convert the token_ids and attention_mask to tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lm7Nqj6w_ndN"
      },
      "outputs": [],
      "source": [
        "token_ids = torch.tensor(token_ids).unsqueeze(0)\n",
        "attention_mask = torch.tensor(attention_mask).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FQ7RzxU_ndN"
      },
      "source": [
        "\n",
        "\n",
        "Now that we preprocessed the input, let's get the embedding.\n",
        "\n",
        "## Getting the embedding\n",
        "Since we set `output_hidden_states = True` while defining the model for getting the embeddings from all the encoder layers, now the model returns an output tuple with three values as shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DhmqjLG1_ndN"
      },
      "outputs": [],
      "source": [
        "last_hidden_state, pooler_output, hidden_states = model(token_ids, attention_mask = attention_mask).to_tuple()\n",
        "last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjN7Ojnh_ndO"
      },
      "source": [
        "Again, the first value `last_hidden_state` contains the representation of all the tokens obtained only from the final encoder layer (encoder 12).\n",
        "\n",
        "The `pooler_output` indicates the representation of the [CLS] token from the final encoder layer which is further processed by a linear and tanh activation function.\n",
        "\n",
        "`hidden_states` contains the representation of all the tokens obtained from all the final encoder layers. Lets look at this last one now specifically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzF8Qtxu_ndP"
      },
      "outputs": [],
      "source": [
        "len(hidden_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZBdQEeI_ndP"
      },
      "source": [
        "\n",
        "As we can notice, it contains 13 values holding the representation of all layers. Thus:\n",
        "\n",
        "- hidden_states[0] contains the representation of all the tokens obtained from the input embedding layer\n",
        "- hidden_states[1] contains the representation of all the tokens obtained from the first encoder layer\n",
        "- hidden_states[2] contains the representation of all the tokens obtained from the second encoder layer\n",
        "\n",
        "Similarly, hidden_states[12] contains the representation of all the tokens obtained from the final encoder layer\n",
        "Let's explore this more. First, let's print the shape of the hidden_states[0] which contains the representation of all the tokens obtained from the input embedding layer :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUT57PxO_ndQ"
      },
      "outputs": [],
      "source": [
        "hidden_states[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPc_0HL3_ndQ"
      },
      "source": [
        "\n",
        "The size [1,7,768] indicates the[batch_size, sequence_length, hidden_size].\n",
        "\n",
        "Now, let's print the shape of hidden_states[1] which contains the representation of all tokens obtained from the first encoder layer :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy_0LBQi_ndR"
      },
      "outputs": [],
      "source": [
        "hidden_states[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "Write down each of the 6 feature vectors from the [table](#table) in this section above only from the `hidden_states` of your model. Define one variable for each."
      ],
      "metadata": {
        "id": "9w5z06xFpIUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CODE HERE"
      ],
      "metadata": {
        "id": "1QNYK_6kqmVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQcW-cj9fowz"
      },
      "source": [
        "# Finetuning BERT for sentiment analysis\n",
        "\n",
        "Let's learn how to finetune the pre-trained BERT for text classification tasks. Say, we are performing sentiment analysis. In the sentiment analysis, our goal is to classify whether a sentence is positive or negative. Suppose, we have a dataset containing sentences along with their labels.\n",
        "\n",
        "Consider a sentence: `I love AI`. First, we tokenize the sentence, add the `[CLS]` token at the beginning, and `[SEP]` token at the end of the sentence. Then, we feed the tokens as an input to the pre-trained BERT and get the embeddings of all the tokens.\n",
        "\n",
        "Next, we ignore the embedding of all other tokens and take only the embedding of `[CLS]` token which is $R_{[CLS]}$. The embedding of the `[CLS]` token will hold the aggregate representation of the sentence. We feed $R_{[CLS]}$ to a classifier (feed-forward network with softmax function) and train the classifier to perform sentiment analysis.\n",
        "\n",
        "Wait! How does it differ from what we saw at the beginning of the section. How finetuning the pre-trained BERT differs from using the pre-trained BERT as a feature extractor?\n",
        "\n",
        "In the first section, we learned that after extracting the embedding $R_{[CLS]}$ of a sentence, we feed the $R_{[CLS]}$ to a classifier and train the classifier to perform classification. Similarly, during finetuning, we feed the embedding of $R_{[CLS]}$ to a classifier and train the classifier to perform classification.\n",
        "\n",
        "The difference is that when we finetune the pre-trained BERT, we can update the weights of the pre-trained BERT along with a classifier. But when we use the pre-trained BERT as a feature extractor, we can update only the weights of a classifier and not the pre-trained BERT.\n",
        "\n",
        "During finetuning, we can adjust the weights of the model in the following two ways:\n",
        "\n",
        "- Update the weights of the pre-trained BERT along with the classification layer\n",
        "- Update only the weights of the classification layer and not the pre-trained BERT. When we do this, it becomes the same as using the pre-trained BERT as a feature extractor\n",
        "\n",
        "The following figure shows how we finetune the pre-trained BERT for the sentiment analysis task:\n",
        "\n",
        "\n",
        "As we can observe from the preceding figure, we feed the tokens to the pre-trained BERT and get the embedding of all the tokens. We take the embedding of `[CLS]` token and feed it to a feedforward network with a softmax function and perform classification.\n",
        "\n",
        "Let's get a better understanding of how finetuning works by getting hands-on with finetuning the pre-trained BERT for sentiment analysis in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph0nLMVFfow0"
      },
      "source": [
        "\n",
        "\n",
        "Import the necessary modules:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xj3OKG3Pfow1"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments\n",
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0hcjGwZfow2"
      },
      "source": [
        "\n",
        "Load the model and dataset. First, let's download and load the dataset using the `datasets` library. We only take the first 20 percent from the training set and the first 5 percent from the test set, otherwise the training will take too long."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "train_set = load_dataset(\"imdb\", split=\"train[:25%]\")\n",
        "test_set = load_dataset(\"imdb\", split=\"test[:10%]\")"
      ],
      "metadata": {
        "id": "4yV3HpnkvM6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGrz_f_cfow-"
      },
      "source": [
        "\n",
        "Let's print the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vd-gEqVcfow_"
      },
      "outputs": [],
      "source": [
        "train_set"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the test set:"
      ],
      "metadata": {
        "id": "2y0VIRPgAuXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_set"
      ],
      "metadata": {
        "id": "5_japhwbAvzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGi62VtxfoxA"
      },
      "source": [
        "\n",
        "Next, let's download and load the pre-trained BERT model. In this example, we use the pre-trained bert-base-uncased model. As we can observe below, since we are performing sequence classification, we use the BertForSequenceClassification class:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNoC_WX2foxB"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW5wD22HfoxC"
      },
      "source": [
        "\n",
        "Next, we download and load the tokenizer which is used for pretraining the bert-base-uncased model.\n",
        "As we can observe, we create the tokenizer using the BertTokenizerFastclass instead of BertTokenizer. The BertTokenizerFast class has many advantages compared to BertTokenizer. We will learn about this in the next section:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szVRFKRpfoxC"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i95cBPtjfoxC"
      },
      "source": [
        "\n",
        "Now that we loaded the dataset and model, next let's preprocess the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poYE4BPPfoxD"
      },
      "source": [
        "## Preprocess the dataset\n",
        "We can preprocess the dataset in a quicker way using our tokenizer. For example, consider the sentence: 'I love AI'.  \n",
        "\n",
        "First, we tokenize the sentence and add the `[CLS]` token at the beginning and `[SEP]` token at the end as shown below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "dHfQQv6pfoxD"
      },
      "source": [
        "```\n",
        "tokens = [ [CLS], I, love, AI, [SEP] ]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHj9t1KrfoxE"
      },
      "source": [
        "\n",
        "Next, we map the tokens to the unique input ids (token ids). Suppose the following are the unique input ids (token ids):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "FzduZUwOfoxE"
      },
      "source": [
        "```\n",
        "input_ids = [101, 1045, 2293, 3000, 102]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJpZf_A5foxE"
      },
      "source": [
        "Then, we need to add the segment ids (token type ids). Wait, what are segment ids? Suppose we have two sentences in the input. In that case, segment ids are used to distinguish one sentence from the other. All the tokens from the first sentence will be mapped to 0 and all the tokens from the second sentence will be mapped to 1. Since here we have only one sentence, all the tokens will be mapped to 0 as shown below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "sLmIG0k3foxF"
      },
      "source": [
        "```\n",
        "token_type_ids = [0, 0, 0, 0, 0]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai4H-92NfoxG"
      },
      "source": [
        "\n",
        "Now, we need to create the attention mask. We know that an attention mask is used to differentiate the actual tokens and `[PAD]` tokens. It will map all the actual tokens to 1 and the `[PAD]` tokens to 0. Suppose, our tokens length should be 5. Now, our tokens list has already 5 tokens. So, we don't have to add `[PAD]` token. Then our attention mask will become:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "aixDEWsTfoxH"
      },
      "source": [
        "```\n",
        "attention_mask = [1, 1, 1, 1, 1]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6ULeBtifoxI"
      },
      "source": [
        "\n",
        "That's it. But instead of doing all the above steps manually, our tokenizer will do these steps for us. We just need to pass the sentence to the tokenizer as shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EujRhGsfoxJ"
      },
      "outputs": [],
      "source": [
        "tokenizer('I love AI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by2HeGksfoxN"
      },
      "source": [
        "\n",
        "With the tokenizer, we can also pass any number of sentences and perform padding dynamically. To do that, we need to set padding to True and also the maximum sequence length. For instance, as shown below, we pass three sentences and we set the maximum sequence length, max_length to 5:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwW-9n96foxN"
      },
      "outputs": [],
      "source": [
        "tokenizer(['I love AI', 'birds fly','snow fall'], padding = True, max_length=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2QyIyEefoxO"
      },
      "source": [
        "\n",
        "That's it, with the tokenizer, we can easily preprocess our dataset. So we define a function called preprocess for processing the dataset as shown below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TYpvRZcnfoxP"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "    return tokenizer(data['text'], padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwV4SLYDfoxP"
      },
      "source": [
        "\n",
        "Now, we preprocess the train and test set using the preprocess function. The dataset loader still shows 0/1 when its done, its a known [bug](https://github.com/huggingface/datasets/issues/5117) in the library. Your dataset is still preprocessed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MUyB2AXKfoxQ"
      },
      "outputs": [],
      "source": [
        "train_set = train_set.map(preprocess, batched=True, batch_size=len(train_set))\n",
        "test_set = test_set.map(preprocess, batched=True, batch_size=len(train_set))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set[0].keys()"
      ],
      "metadata": {
        "id": "9gUgYFlL3XVr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZHwUC-NfoxR"
      },
      "source": [
        "That's it. Now that we have the dataset ready, let's train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv67exnifoxR"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih63Z2wqfoxS"
      },
      "source": [
        "\n",
        "Define the batch size and epoch size.\n",
        "\n",
        "Batch size is how many items we put in per loop. So a batch size of 8 means 8 items per rendering. We need to do this because of the memory allocation of the GPU, its impossible to do the whole dataset at once, because this otherwise wouldn't fit in the GPU.\n",
        "\n",
        "The number of epochs is how many times we loop over the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1NjA0cFGfoxT"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "epochs = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSLvaWr6foxT"
      },
      "source": [
        "\n",
        "Define the warmup steps and weight decay.\n",
        "\n",
        "Warmup steps means that in the beginning of training we set the learning to a really slow amount (slow learning rate). We let our network components kind of get used to our dataset, before we really move a lot in our optimization.\n",
        "\n",
        "We use weight decay to avoid overfitting by adding a penalty to our training function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6bLTXRGQfoxU"
      },
      "outputs": [],
      "source": [
        "warmup_steps = 500\n",
        "weight_decay = 0.01"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brs5HIfufoxU"
      },
      "source": [
        "Lets first define what metrics we want to compute. We deal with binary classification, so it makes sense to include precision, recall, f1 and accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'accuracy': acc, 'f1': f1.item(), 'precision': precision.item(), 'recall': recall.item()}"
      ],
      "metadata": {
        "id": "CIxGbGDzRHWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how you would define your training loop in the `transformers` library: We can put things in like the number of training epochs, batch size, warmup steps and weight decay. Now lets define the training arguments:"
      ],
      "metadata": {
        "id": "rWalfyRTRNL2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06c9nd-DfoxU"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    warmup_steps=warmup_steps,\n",
        "    weight_decay=weight_decay,\n",
        "    logging_dir='./logs',\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOqwu_F8foxV"
      },
      "source": [
        "\n",
        "\n",
        "Now define the trainer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EA70qZzsfoxV"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    compute_metrics=compute_metrics,\n",
        "    train_dataset=train_set,\n",
        "    eval_dataset=test_set\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjOdmJKDfoxV"
      },
      "source": [
        "Start training the model. The training takes about 10 minutes in total!\n",
        "\n",
        "Please make sure you are on GPU, otherwise this takes even longer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce_yJaLwfoxW"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at-qPxJIfoxW"
      },
      "source": [
        "\n",
        "After training we can evaluate the model using the evaluate function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7sE13FDfoxW"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3\n",
        "1. Instead of using the `Trainer` function from the `transfomers` library, now write your own training loop using **only PyTorch**. You should also implement weight decay and use a linear scheduler. Train for 1 epoch.\n",
        "2. Write an evaluation loop on your test data. Implement `f1`, `accuracy`, `precision` and `recall` and evaluate your model."
      ],
      "metadata": {
        "id": "U3bgs1CD5kpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## CODE HERE"
      ],
      "metadata": {
        "id": "Rk8o8UcJ5kKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrlTQ4RsXU-V"
      },
      "source": [
        "# Fine-tuning Pretrained Transformers for Named Entity Recognition\n",
        "\n",
        "In the previous section we worked on sentiment prediction with just two classes using BERT. In this notebook we'll be using BERT-based models for Named Entity Recognition. Again, exactly the same model, but now we train it for a different dataset. Lets see if we can tag some sentences.\n",
        "\n",
        "One thing to notice is that we now have a different class type within Bert that we are going to use. Notice that instead of BertForSequenceClassification we now use BertForTokenClassification. The former we use for predicting labels from a whole piece of text (e.g. sentences). Now we want to classify per token basis.\n",
        "\n",
        "We should reimport the model first, otherwise it is still finetuned on the previous task. That knowledge does not make sense anymore for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS2XMkfxXU-f"
      },
      "source": [
        "from transformers import BertForTokenClassification, AutoTokenizer\n",
        "model = BertForTokenClassification.from_pretrained('bert-base-uncased')\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data\n",
        "Now let's also import a new dataset. This time we are going to work on a PoS dataset. We will be using CoNLL. This dataset contains news stories from Reuters, and contains 3 tasks: part of speech tagging (POS), named entity recognition (NER) and chunking. We import it with the `dataset` library."
      ],
      "metadata": {
        "id": "Eg29QtlmXx2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "id": "Dm6uhuPrXvt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets visualise the structure:"
      ],
      "metadata": {
        "id": "t_UDrabsY4GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "gb2cIFdOY3eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how a token looks like:"
      ],
      "metadata": {
        "id": "glHgqmuuZN8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0][\"tokens\"]"
      ],
      "metadata": {
        "id": "NPl-78tJZNIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how a named entity recognition tag looks like:"
      ],
      "metadata": {
        "id": "BqLl8J2UZP46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0][\"ner_tags\"]"
      ],
      "metadata": {
        "id": "SMlH_cDuZSj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These named entity recognition tags are integers that can be converted with a function back to ones that do have meaning. Let's try it:"
      ],
      "metadata": {
        "id": "BpFvZrMIZXOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_features = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
        "print(ner_features)"
      ],
      "metadata": {
        "id": "xyFiWarTZiyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So converting above pos tags means we just have to do this:"
      ],
      "metadata": {
        "id": "36e38bEiZpOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = dataset[\"train\"][0][\"tokens\"]\n",
        "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
        "line1 = \"\"\n",
        "line2 = \"\"\n",
        "for word, label in zip(words, labels):\n",
        "    full_label = ner_features[label]\n",
        "    max_length = max(len(word), len(full_label))\n",
        "    line1 += word + \" \" * (max_length - len(word) + 1)\n",
        "    line2 += full_label + \" \" * (max_length - len(full_label) + 1)\n",
        "\n",
        "print(line1)\n",
        "print(line2)"
      ],
      "metadata": {
        "id": "N2cw0jfcZr-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets use our tokenizer. We need whole words to perform classification. As we know, a transformers tokenizer splits its content into subwords or bytes. We can solve this with using `tokenizer` as usual and just add `is_split_into_words=True`. For example:\n"
      ],
      "metadata": {
        "id": "bWoWU0JJdZAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(dataset[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
        "inputs.tokens()"
      ],
      "metadata": {
        "id": "WMDR7Fcre0_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we immediately see a problem. As we can see, the tokenizer added the special tokens used by the model (`[CLS]` at the beginning and `[SEP]` at the end) and left most of the words untouched. The word lamb, however, was tokenized into two subwords, la and ##mb. This introduces a mismatch between our inputs and the labels: the list of labels has only 9 elements, whereas our input now has 11 tokens. Accounting for the special tokens is easy (we know they are at the beginning and the end), but we also need to make sure we align all the labels with the proper words.\n",
        "\n"
      ],
      "metadata": {
        "id": "27bd4_uhfYRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.word_ids()"
      ],
      "metadata": {
        "id": "bs96TzPYfWUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the labels, we have the problem that the `CLS` token and `SEP` token are not really pos tags, and since the word ids are splitted up, we need to create a function that can map parts of words to tokens. We remove the two tokens and parts of subwords by changing them in the align function to `-100`:"
      ],
      "metadata": {
        "id": "do2ph-K71tGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        else:\n",
        "            new_labels.append(-100)\n",
        "\n",
        "    return new_labels"
      ],
      "metadata": {
        "id": "BrSRZt_A2BWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
        "word_ids = inputs.word_ids()\n",
        "print(labels)\n",
        "print(align_labels_with_tokens(labels, word_ids))"
      ],
      "metadata": {
        "id": "xMNKmfOT2mHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To preprocess our whole dataset, we need to tokenize all the inputs and apply `align_labels_with_tokens()` on all the labels. To take advantage of the speed of our fast tokenizer, it’s best to tokenize lots of texts at the same time, so we’ll write a function that processes a list of examples and use the `Dataset.map()` method with the option `batched=True`. The only thing that is different from our previous example is that the `word_ids()` function needs to get the index of the example we want the word IDs of when the inputs to the tokenizer are lists of texts (or in our case, list of lists of words), so we add that too:"
      ],
      "metadata": {
        "id": "J0E5xmmM4tV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "vCYleGBF4uu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we haven’t padded our inputs yet; we’ll do that later, when creating the batches with a data collator.\n",
        "\n",
        "We can now apply all that preprocessing in one go on the other splits of our dataset:\n",
        "\n",
        "Please note that the loading function does not fully load to 100%, and the loading bar shows a red color! This is due to a bug in the [datasets](https://github.com/huggingface/datasets/issues/5117) library."
      ],
      "metadata": {
        "id": "rkP1UF_s4y9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = dataset.map(align_labels, batched=True, remove_columns=dataset[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "-6wOBUUh40X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning the model:"
      ],
      "metadata": {
        "id": "585lEPiy_qm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The actual code using the Trainer will be the same as before; the only changes are the way the data is collated into a batch and the metric computation function.\n",
        "\n",
        "With the data collator, our labels should be padded the exact same way as the inputs so that they stay the same size, using -100 as a value so that the corresponding predictions are ignored in the loss computation."
      ],
      "metadata": {
        "id": "R_zlIE-2AJHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
        "print(\"Padded:\")\n",
        "print(batch[\"labels\"])\n",
        "print(\"Not padded:\")\n",
        "for i in range(2):\n",
        "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
      ],
      "metadata": {
        "id": "HDp8FTwS_dqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To have the `Trainer` compute a metric every epoch, we will need to define a `compute_metrics()` function that takes the arrays of predictions and labels, and returns a dictionary with the metric names and values.\n",
        "\n",
        "The traditional framework used to evaluate token classification prediction is [seqeval](https://github.com/chakki-works/seqeval). To use this metric, we first need to install the *seqeval* and the *evaluate* libraries and then load it in:"
      ],
      "metadata": {
        "id": "ScXYHsiKA5kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq seqeval evaluate\n",
        "import evaluate\n",
        "metric = evaluate.load(\"seqeval\")"
      ],
      "metadata": {
        "id": "gc6YNptoAlGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This metric does not behave like the standard accuracy: it will actually take the lists of labels as strings, not integers, so we will need to fully decode the predictions and labels before passing them to the metric. Let’s see how it works. First, we’ll get the labels for our first training example:"
      ],
      "metadata": {
        "id": "KpBNIFA_BggJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset[\"train\"][0][\"ner_tags\"]\n",
        "labels = [ner_features[i] for i in labels]\n",
        "predictions = labels.copy()\n",
        "predictions[2] = \"O\"\n",
        "metric.compute(predictions=[predictions], references=[labels])\n"
      ],
      "metadata": {
        "id": "aVu2TdhTBjbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets now define a `compute_metrics()` function:"
      ],
      "metadata": {
        "id": "gjBVicfSNngc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[ner_features[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [ner_features[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }"
      ],
      "metadata": {
        "id": "kAswlNExKTq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we are working on a token classification problem, we will use the `AutoModelForTokenClassification` class. The main thing to remember when defining this model is to pass along some information on the number of labels we have. The easiest way to do this is to pass that number with the num_labels argument, but if we want a nice inference widget working like the one we saw at the beginning of this section, it’s better to set the correct label correspondences instead.\n",
        "\n",
        "They should be set by two dictionaries, `id2label` and `label2id`, which contain the mappings from ID to label and vice versa:"
      ],
      "metadata": {
        "id": "I8lYwZqkNymm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "id2label = {i: label for i, label in enumerate(ner_features)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")"
      ],
      "metadata": {
        "id": "JoQ3zkRjN4-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets run the model! Make sure you are on your GPU, otherwise it will take ages again."
      ],
      "metadata": {
        "id": "9qD_zj0VRSl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "frktK1AJOJ0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "jAPpdDrrmABj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "Now instead of named entity recognition, try finetuning a `bert-base-uncased` model on **part of speech (POS)** tagging. Use the code above, or base your model from the original source of this section from [chapter 7.2 of the HuggingFace Course](https://huggingface.co/course/chapter7/2). You should still use the same dataset, this time instead of the `ner_tags` you use the `pos_tags`. Evaluate also with the `seqeval` library as shown above. You will get errors like `UserWarning: ... seems not to be ...`, you should ignore these. Ignoring these totally you can do with something like:\n",
        "```python\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "```\n",
        "Train the model for 2 epochs and report the following metrics from the `seqeval` within the `compute_metrics` function on the validation set: `eval_precision`, `eval_recall` `eval_f1` and `eval_accuracy` (like above)."
      ],
      "metadata": {
        "id": "ddAaOVq6Oq-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### CODE HERE"
      ],
      "metadata": {
        "id": "rXuJbd1IQruM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPbF5VjZQryE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQYrtvRmQr15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EvAPS1lTATI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}