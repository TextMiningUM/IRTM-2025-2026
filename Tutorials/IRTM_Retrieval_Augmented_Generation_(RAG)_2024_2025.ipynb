{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
        "\n",
        "# Information Retrieval and Text Mining Course - Retrieval Augmented Generation (RAG) Tutorial\n",
        "Authors: Abderrahmane Issam and Jan Scholtes\n",
        "\n",
        "\n",
        "Version 2024-2025\n",
        "\n",
        "\n",
        "In this notebook we will learn how to implement a Retrieval Augmented Generation (RAG) pipeline and evaluate its performance. In the begining of the tutorial, we demonstrate how to use [Distilabel](https://github.com/argilla-io/distilabel) to annotate Wikipedia documents and use them for fine-tuning ColBERT. [RAGatouille](https://github.com/AnswerDotAI/RAGatouille) offers a very simple API for fine-tuning and using ColBERT, so we will use it in this tutorial as well. For the RAG part, we will be using [DSPy](https://github.com/stanfordnlp/**dspy**), which is a framework for programming language models."
      ],
      "metadata": {
        "id": "Hj3Fg4KwCuYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "bmnTFRU64Wzz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMD8BtmtRavF"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade distilabel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ragatouille"
      ],
      "metadata": {
        "id": "bkNLfayzSjv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dspy"
      ],
      "metadata": {
        "id": "Yiem2Y40dyOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "CI5iVIjlvRt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bm25s PyStemmer"
      ],
      "metadata": {
        "id": "W4IY5hCT8xYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforming Unstructured Data to a Structured Dataset"
      ],
      "metadata": {
        "id": "XG_BCsfLET_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of the tutorial, we will transform Wikipedia documents into a datataset that we can use for fine-tuning a ColBERT. The process starts with retrieving the documents, then chunking them, annotation using a LLM, and finally fine-tuning ColBERT."
      ],
      "metadata": {
        "id": "ihLxDR2d4d4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille.utils import get_wikipedia_page"
      ],
      "metadata": {
        "id": "1cmC3ySXEkwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will retrieve the following 3 Wikipedia pages."
      ],
      "metadata": {
        "id": "x8HDNX00ExsL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille.utils import get_wikipedia_page\n",
        "\n",
        "my_full_corpus = [get_wikipedia_page(\"Hayao_Miyazaki\")]\n",
        "my_full_corpus += [get_wikipedia_page(\"Studio_Ghibli\")]\n",
        "my_full_corpus += [get_wikipedia_page(\"Toei_Animation\")]"
      ],
      "metadata": {
        "id": "HaGlbj_LSJ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Wikipedia document often contains different types of information about a certain topic. It is also more than we can afford to feed an LLM. Although some LLMs support a very large context window, feeding large documents into the model will require more computational resources, furthermore, it might end up confusing the model to feed it with a full documents when only one segment is needed to answer the prompt. \\\\\n",
        "In the following code, we will use Ragatouile `CorpusProcessor` to chunk the copus into multiple segments of 180 tokens. By default it makes sure that the chunks overlap to prevent losing important context."
      ],
      "metadata": {
        "id": "ipvSOxe-E4_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille.data import CorpusProcessor, llama_index_sentence_splitter\n",
        "\n",
        "corpus_processor = CorpusProcessor(document_splitter_fn=llama_index_sentence_splitter)\n",
        "documents = corpus_processor.process_corpus(my_full_corpus, chunk_size=180)\n",
        "\n",
        "len(documents)"
      ],
      "metadata": {
        "id": "OSUd96BYSMc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example document:"
      ],
      "metadata": {
        "id": "rHhDSC6bIPIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents[0]"
      ],
      "metadata": {
        "id": "0BMdbGV9WQEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distilabel expects a HuggingFace dataset with a text column `anchor`."
      ],
      "metadata": {
        "id": "ea7FAZsQIjIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "df = pd.DataFrame.from_dict(documents)\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.rename_column(\"content\", \"anchor\")\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "za_AxecmWgL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use a small LLM locally through the transformers library for our demo purposes, but distilabel can be used with the other paid LLM APIs such as OpenAI or Anthropic."
      ],
      "metadata": {
        "id": "5UIEsPNiJAdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.models import TransformersLLM\n",
        "\n",
        "llm = TransformersLLM(model=\"microsoft/Phi-3-mini-4k-instruct\")"
      ],
      "metadata": {
        "id": "03VFVFu5VYgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`GenerateSentencePair` is a component for generating datasets for embedding models, this includes retrieval, reranking or feature extraction. To fine-tune ColBERT we will only queries, and that is why we set `action` to query. Other supported actions are \"paraphrase\", \"semantically-similar\", \"answer\". In case we need both positive and negative examples, we can set `triplet` to `True`. \\\\\n",
        "\n",
        "Distillabel will create a prompt based on these parameters and extract the LLM answer for us, which in this case is the query. The context is a descrption of our copus that will be included in the prompt as well."
      ],
      "metadata": {
        "id": "sD9Hde4jJftY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps.tasks import GenerateSentencePair\n",
        "\n",
        "context = (\n",
        "\"\"\"\n",
        "The text is a chunk from wikipedia pages that we want to use for fine-tuning a retrieval model.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "with Pipeline(name=\"generate\") as pipeline:\n",
        "    generate_retrieval_pairs = GenerateSentencePair(\n",
        "        name=\"generate_retrieval_pairs\",\n",
        "        triplet=False,\n",
        "        action=\"query\",\n",
        "        llm=llm,\n",
        "        input_batch_size=10,\n",
        "        context=context,\n",
        "    )"
      ],
      "metadata": {
        "id": "ReJATovTVwFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code takes approximately 11 minutes to finish."
      ],
      "metadata": {
        "id": "MJAQhW9boBl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distiset = pipeline.run(dataset=dataset, use_cache=False)"
      ],
      "metadata": {
        "id": "ddEx_8dlXPJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check distilabel_metadata column to see the full prompt and model output for each anchor:"
      ],
      "metadata": {
        "id": "-NKvxFE3avqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = distiset['default']['train'].to_pandas()\n",
        "df"
      ],
      "metadata": {
        "id": "cicj7AY7arO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We instantiate `RAGTrainer`:"
      ],
      "metadata": {
        "id": "mXfZTorKbRBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGTrainer\n",
        "\n",
        "trainer = RAGTrainer(model_name=\"GhibliColBERTv2.0\", pretrained_model_name=\"colbert-ir/colbertv2.0\")"
      ],
      "metadata": {
        "id": "PlzYk0lcZq5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pass query anchor pairs from the dataset to the trainer."
      ],
      "metadata": {
        "id": "Ph8rt8ToUu6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = [[q, doc] for q, doc in zip(df.positive, df.anchor)]\n",
        "\n",
        "trainer.prepare_training_data(\n",
        "        raw_data = pairs,\n",
        "        all_documents = documents,\n",
        "        num_new_negatives = 10,\n",
        "        mine_hard_negatives= True,\n",
        "        )"
      ],
      "metadata": {
        "id": "0YH0Sn3rZt8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1:\n",
        "Explain what `mine_hard_nagtives` does?"
      ],
      "metadata": {
        "id": "oMbJrdwwnLwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here."
      ],
      "metadata": {
        "id": "yIOtc0WNUfo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fine-tune the model for a maximum of 1000 steps. You can change this number or play with other hyperparameters if you like."
      ],
      "metadata": {
        "id": "OGa3qhC4VVHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_colbert_path = trainer.train(maxsteps=1000)\n",
        "finetuned_colbert_path"
      ],
      "metadata": {
        "id": "mXZ1I9WwcrfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is time to compare our fine-tuned model to the original ColBERT model on few queries. This is more of a qualitative analysis to showcase the effect of fine-tuning. In a real-world setup, it is important to have an dedicated test dataset for evaluation. \\\\\n",
        "\n",
        "The fine-tuning we did is often refered to as domain adaptation, where we took a general purpose retrieval model and adapted it to a specific domain. This often leads to better performance on our domain but loses some of the generalization capailities of the original model."
      ],
      "metadata": {
        "id": "SGTjp9csVlyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "colbert = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ],
      "metadata": {
        "id": "LPqcib87oy4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With RAGatouile, we can index our dataset as follows:"
      ],
      "metadata": {
        "id": "c12G70xIXYbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colbert.index(collection=[doc['content'] for doc in documents], document_ids=[\"docno\"+str(i) for i in range(len(documents))], split_documents=False)"
      ],
      "metadata": {
        "id": "SgslZlJWqN5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `search` to query the index:"
      ],
      "metadata": {
        "id": "nAA81SAbYwHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colbert.search(query = \"what's studio ghibli's most famous movie?\", k=3)"
      ],
      "metadata": {
        "id": "jXmQ5O51Yowl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indexing is important for making retrieval more efficient but it is also time consuming. In our case where the dataset is small, indexing doesn't offer an advantage over keeping our encodings in memory. We can use `encode` to skip the indexing part:"
      ],
      "metadata": {
        "id": "OEoij9SGsj3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colbert.encode([x['content'] for x in documents], document_metadatas=[{\"about\": \"ghibli\"} for _ in range(len(documents))])"
      ],
      "metadata": {
        "id": "WsVEuPhEsLhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To search the encoded documents we use `search_encoded_docs` instead of `search`:"
      ],
      "metadata": {
        "id": "ClazgJersiMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colbert.search_encoded_docs(query=\"what's studio ghibli's most famous movie?\", k=3)"
      ],
      "metadata": {
        "id": "WGHJ12Swr1QQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "Use the fine-tuned ColBERT model to encode documents run the previous query. Try mulitple queries with the original and fine-tuned ColBERT models and describe any difference between the two."
      ],
      "metadata": {
        "id": "XxBc_l6bte0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here"
      ],
      "metadata": {
        "id": "zUgF1BNxbeFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3\n",
        "\n",
        "List 3 ways to improve the fine-tuning of the ColBERT model."
      ],
      "metadata": {
        "id": "366Az7JTbdXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here."
      ],
      "metadata": {
        "id": "8WjUKPyZceSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "ax2dnZ_kZR7f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DSPy expects an API that is running the model, and we can achieve this locally using OLlama. Ideally, you want to run this on a server or use paid LLM API (DSPy supports multiple APIs), but for learning purposes we can run OLlama in the background and use it in this notebook:"
      ],
      "metadata": {
        "id": "b3VEkiGRZnK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nohup ollama serve > ollama.log 2>&1 &"
      ],
      "metadata": {
        "id": "6ias-1kwCYcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We run phi-3-instruct. Feel free to try other models supported by Ollama: https://ollama.com/search"
      ],
      "metadata": {
        "id": "fVILZoEYdGhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run phi3:3.8b-instruct &"
      ],
      "metadata": {
        "id": "hlEIZlLzBY3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you encounter any issues with using the model along the way. Then it might be worth it to restart Ollama beginning from `ollama serve`, but first we will need to kill the running process to free the port. You can find the PID and kill the process as follows:\n",
        "```\n",
        "!lsof -i :11434\n",
        "!kill PID     # PID from previous step\n",
        "```\n",
        "\n",
        "After this run `ollama serve` cell followed by `ollama run` and try again."
      ],
      "metadata": {
        "id": "iY-LJeled1Ah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download question--answer pairs from the RAG-QA Arena \"Tech\" dataset:"
      ],
      "metadata": {
        "id": "jJsJvLrudWNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ujson\n",
        "from dspy.utils import download\n",
        "\n",
        "download(\"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_examples.jsonl\")\n",
        "\n",
        "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
        "  data = [ujson.loads(line) for line in f]\n",
        "\n",
        "data[0]"
      ],
      "metadata": {
        "id": "t5habNOsdviV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Example` is DSPy data type to represent items in our data. We can specify the input field using `with_inputs`. In this case the input is the question, and the label is the response."
      ],
      "metadata": {
        "id": "UApvXqz4d9Y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "\n",
        "data = [dspy.Example(**d).with_inputs('question') for d in data]\n",
        "\n",
        "example = data[2]\n",
        "example"
      ],
      "metadata": {
        "id": "oCo-56IMd3ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split our dataset as follows. We will only use 20 examples for evaluation (devset) since it is time consuming to generate using the LLM as well as use it for evaluation."
      ],
      "metadata": {
        "id": "yHkJnlQReiUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.Random(123).shuffle(data)\n",
        "trainset, devset, testset = data[:200], data[200:220], data[220:400]\n",
        "\n",
        "len(trainset), len(devset), len(testset)"
      ],
      "metadata": {
        "id": "tQx2VriHeeXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We connect DSPy to the running Ollama API as follows:"
      ],
      "metadata": {
        "id": "7B7ak0QwgXZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "lm = dspy.LM('ollama_chat/phi3:3.8b-instruct', api_base='http://localhost:11434', api_key='')\n",
        "dspy.configure(lm=lm)"
      ],
      "metadata": {
        "id": "yg3Etna3C2jZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a ChainOfThought module which will use the LM we provide and instruct it to reason before generating the answer. In the Prediction output below, we can see the reasoning generated by the model, and the final answer:"
      ],
      "metadata": {
        "id": "1RP9EAuIgdJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cot = dspy.ChainOfThought('question -> response')\n",
        "cot(question=\"should curly braces appear on their own line?\")"
      ],
      "metadata": {
        "id": "Hokh0W29C_a7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another question :)"
      ],
      "metadata": {
        "id": "J5Uf8Rc_fKph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cot(question=\"how to install python on mac?\")"
      ],
      "metadata": {
        "id": "SDH-S2_meuAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Semantic F1 is a metric that attemtpts to capture the following: How well does the system response cover all key facts in the gold response? (Recall) And the other way around, how well is the system response not saying things that aren't in the gold response? (Precision). \\\\\n",
        "The semantic part comes from the fact that we are using an LLM to measure this. We will be using the same LLM we are using for generation."
      ],
      "metadata": {
        "id": "zOaoGXnmBxi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dspy.evaluate import SemanticF1\n",
        "\n",
        "# Instantiate the metric.\n",
        "metric = SemanticF1(decompositional=True)\n",
        "\n",
        "# Produce a prediction from our `cot` module\n",
        "example = data[2]\n",
        "pred = cot(**example.inputs())\n",
        "\n",
        "# Compute the metric score for the prediction.\n",
        "score = metric(example, pred)\n",
        "\n",
        "print(f\"Question: \\t {example.question}\\n\")\n",
        "print(f\"Gold Response: \\t {example.response}\\n\")\n",
        "print(f\"Predicted Response: \\t {pred.response}\\n\")\n",
        "print(f\"Semantic F1 Score: {score:.2f}\")\n"
      ],
      "metadata": {
        "id": "bnPr-UzIeifg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following is how we would evaluate our model using DSPy. We should take the results with a grain of salt because the dataset is tiny."
      ],
      "metadata": {
        "id": "5GdRhSV_otHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=1,\n",
        "                         display_progress=True, display_table=2, provide_traceback=True)\n",
        "\n",
        "evaluate(cot)\n"
      ],
      "metadata": {
        "id": "_J6jmiDyc5Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will implement it a RAG pipeline that starts with retrieving 2 documents as context, then generating the answer using our Chain Of Thought model:"
      ],
      "metadata": {
        "id": "1nZyH-S_goAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "class RAG(dspy.Module):\n",
        "  def __init__(self, ir_model, documents, topk=2):\n",
        "    super().__init__()\n",
        "    self.ir_model = RAGPretrainedModel.from_pretrained(ir_model)\n",
        "    self.ir_model.encode(documents)\n",
        "    self.generate_answer = dspy.ChainOfThought('context, question -> response')\n",
        "    self.topk = topk\n",
        "    self.documents = documents\n",
        "\n",
        "  def forward(self, question):\n",
        "    context = self.ir_model.search_encoded_docs(query=question, k=self.topk)\n",
        "    context = [doc['content'] for doc in context]\n",
        "    prediction = self.generate_answer(context=context, question=question)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "o5dKIo_4ZvYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using all the documents in our dataset for retrieval:"
      ],
      "metadata": {
        "id": "-4MGyNnGhCur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = [doc[\"response\"] for doc in data]\n",
        "rag = RAG(\"colbert-ir/colbertv2.0\", docs)"
      ],
      "metadata": {
        "id": "RWHy6xEHK0wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We try a query:"
      ],
      "metadata": {
        "id": "ntsEE0nfhEmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag(question=\"how to install python on mac?\")"
      ],
      "metadata": {
        "id": "0NfaCb62SKiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally evaluate the model. We can see that we got ~1 point improvement by using RAG. But again since we are using 20 examples for validation, the results are not conclusive:"
      ],
      "metadata": {
        "id": "oK2IF9nXhI0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=1,\n",
        "                         display_progress=True, display_table=2, provide_traceback=True)\n",
        "\n",
        "evaluate(rag)\n"
      ],
      "metadata": {
        "id": "yburPof5TN-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "\n",
        "Fine-tune ColBERT on the train set and use in the rag pipeline above. explain why it does or doesn't improve the results."
      ],
      "metadata": {
        "id": "_OOp2efHZ2I_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here."
      ],
      "metadata": {
        "id": "pSWiUaeR3gF-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try using BM25 for retrieval instead of ColBERT which is slower. We will use `bm25s` to create an index from our documents as follow:"
      ],
      "metadata": {
        "id": "zZf0Lx6i3-bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bm25s\n",
        "import Stemmer\n",
        "\n",
        "stemmer = Stemmer.Stemmer(\"english\")\n",
        "corpus_tokens = bm25s.tokenize(docs, stopwords=\"en\", stemmer=stemmer)\n",
        "\n",
        "retriever = bm25s.BM25(k1=0.9, b=0.4)\n",
        "retriever.index(corpus_tokens)"
      ],
      "metadata": {
        "id": "zTcWBu4Jax05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can retrieve documents from the index we created as folows:"
      ],
      "metadata": {
        "id": "2wU1j2kcibax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = bm25s.tokenize(\"should curly braces appear on their own line?\", stopwords=\"en\", stemmer=stemmer, show_progress=False)\n",
        "results, scores = retriever.retrieve(tokens, k=2, n_threads=1, show_progress=False)\n",
        "run = [docs[doc] for doc, score in zip(results[0], scores[0])]\n",
        "run[0]"
      ],
      "metadata": {
        "id": "kQEdvn4chzVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "class RAG_bm25(dspy.Module):\n",
        "  def __init__(self, documents, topk=2):\n",
        "    super().__init__()\n",
        "    self.stemmer = Stemmer.Stemmer(\"english\")\n",
        "    corpus_tokens = bm25s.tokenize(documents, stopwords=\"en\", stemmer=self.stemmer)\n",
        "    self.retriever = bm25s.BM25(k1=0.9, b=0.4)\n",
        "    self.retriever.index(corpus_tokens)\n",
        "\n",
        "    self.generate_answer = dspy.ChainOfThought('context, question -> response')\n",
        "    self.topk = topk\n",
        "    self.documents = documents\n",
        "\n",
        "  def bm25_search(self, question: str) -> list[str]:\n",
        "    tokens = bm25s.tokenize(question, stopwords=\"en\", stemmer=self.stemmer, show_progress=False)\n",
        "    results, scores = self.retriever.retrieve(tokens, k=self.topk, n_threads=1, show_progress=False)\n",
        "    run = [docs[doc] for doc, score in zip(results[0], scores[0])]\n",
        "    return run\n",
        "\n",
        "  def forward(self, question):\n",
        "    context = self.bm25_search(question)\n",
        "    prediction = self.generate_answer(context=context, question=question)\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "D5qIXU2_rSlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_bm25 = RAG_bm25(docs)"
      ],
      "metadata": {
        "id": "2cfimx50sLs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=1,\n",
        "                         display_progress=True, display_table=2, provide_traceback=True)\n",
        "\n",
        "evaluate(rag_bm25)"
      ],
      "metadata": {
        "id": "j1M-z95EsROM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exericse 5\n",
        "Implement a reranking into the RAG pipeline. Start by retrieving 10 documents using BM25 then rerank them using ColBERT and return 2 documents as context. Evaluate the model and compare it against the other results."
      ],
      "metadata": {
        "id": "EwH-uytBsnxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here."
      ],
      "metadata": {
        "id": "qasRyblI3c82"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "unyteXsX3eDT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}