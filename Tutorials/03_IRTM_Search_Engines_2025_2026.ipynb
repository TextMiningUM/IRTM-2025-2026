{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
    "\n",
    "# Information Retrieval and Text Mining Course\n",
    "## Tutorial 03 — Search Engines: Relevance Ranking\n",
    "\n",
    "**Author:** Jan Scholtes\n",
    "\n",
    "**Edition 2025-2026**\n",
    "\n",
    "Department of Advanced Computer Sciences — Maastricht University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Tutorial 03 on **Search Engines: Relevance Ranking**. In this tutorial we explore how search engines rank documents by relevance, progressing from classical lexical methods to neural approaches.\n",
    "\n",
    "The tutorial is organised in three stages:\n",
    "\n",
    "1. **Stage 1 — BM25 Baseline**: We use [Pyserini](https://github.com/castorini/pyserini) to perform BM25 retrieval on the MS MARCO passage corpus with official TREC Deep Learning 2019 queries. We observe where keyword-based ranking succeeds and where it fails.\n",
    "2. **Stage 2 — Neural Reranking**: We apply a neural cross-encoder to rerank the BM25 results, demonstrating how semantic understanding produces better rankings.\n",
    "3. **Stage 3 — Quantitative Evaluation**: We compute nDCG@10 and MAP using official NIST relevance judgments (qrels) to show rigorous evidence that neural reranking outperforms BM25.\n",
    "\n",
    "Before the experiment we review the theory behind TF-IDF, BM25, and neural ranking methods.\n",
    "\n",
    "**Dataset**: MS MARCO Passage Ranking with TREC Deep Learning 2019 evaluation queries (43 queries with graded relevance judgments from NIST assessors).\n",
    "\n",
    "> **Note:** This course is about Information Retrieval, Text Mining, and Conversational Search — not about programming skills. The code cells below show you *how* these methods work in practice using Python libraries. Focus on understanding the **concepts** and **results**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Installation\n",
    "\n",
    "We install all required packages in a single cell. Run this cell once at the beginning of your session.\n",
    "\n",
    "**Important:** Pyserini requires **Java 11+** (JDK, not just JRE). If you do not have Java installed:\n",
    "- **Windows:** `winget install Microsoft.OpenJDK.21`\n",
    "- **macOS:** `brew install openjdk@21`\n",
    "- **Linux:** `sudo apt install openjdk-21-jdk`\n",
    "\n",
    "The first time you run the search cells, Pyserini will download the MS MARCO passage index (~2 GB). This is a one-time operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages installed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    \"pyserini\",\n",
    "    \"sentence-transformers\",\n",
    "    \"faiss-cpu\",\n",
    "    \"scikit-learn\",\n",
    "    \"PyMuPDF\",          # PDF parsing (import as 'fitz')\n",
    "]\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-detected JAVA_HOME: C:\\Program Files\\Microsoft\\jdk-21.0.10.7-hotspot\n",
      "Pyserini loaded  | JAVA_HOME: C:\\Program Files\\Microsoft\\jdk-21.0.10.7-hotspot\n",
      "PyTorch 2.10.0+cpu | CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "# --- Java & Environment Setup ---\n",
    "import os, json, math, sys, warnings\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Pyserini requires JAVA_HOME pointing to a JDK 11+ installation.\n",
    "# The cell below tries to auto-detect your JDK. If it fails, set the\n",
    "# path manually: os.environ[\"JAVA_HOME\"] = r\"C:\\path\\to\\jdk\"\n",
    "if \"JAVA_HOME\" not in os.environ or not os.environ[\"JAVA_HOME\"]:\n",
    "    import glob\n",
    "    candidates = (\n",
    "        glob.glob(r\"C:\\Program Files\\Microsoft\\jdk-*\")\n",
    "        + glob.glob(r\"C:\\Program Files\\Java\\jdk-*\")\n",
    "        + glob.glob(\"/usr/lib/jvm/java-*-openjdk*\")\n",
    "        + glob.glob(\"/Library/Java/JavaVirtualMachines/*/Contents/Home\")\n",
    "    )\n",
    "    if candidates:\n",
    "        os.environ[\"JAVA_HOME\"] = sorted(candidates)[-1]\n",
    "        print(f\"Auto-detected JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
    "    else:\n",
    "        raise EnvironmentError(\n",
    "            \"JAVA_HOME not set and no JDK found.\\n\"\n",
    "            \"Install JDK 11+ first (e.g. winget install Microsoft.OpenJDK.21)\"\n",
    "        )\n",
    "\n",
    "# --- Pyserini & model imports ---\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.search import get_topics, get_qrels\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "print(f\"Pyserini loaded  | JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
    "print(f\"PyTorch {torch.__version__} | CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Keywords to Meaning: The Relevance Problem\n",
    "\n",
    "At the heart of every search engine is a **ranking function** — a mathematical formula that decides which documents are most relevant to a query.\n",
    "\n",
    "The simplest approach is **exact keyword matching**: find documents that contain the query terms and rank them by how often those terms appear. This works surprisingly well, but it has a fundamental limitation called the **lexical gap**:\n",
    "\n",
    "> A user searching for *\"how to fix a broken screen\"* may not find a document titled *\"smartphone display repair guide\"* because the words do not match — even though the meaning is the same.\n",
    "\n",
    "This tutorial explores the evolution from keyword-based to semantic ranking:\n",
    "\n",
    "| Generation | Method | Matching | Limitation |\n",
    "|-----------|--------|----------|------------|\n",
    "| 1st | TF-IDF | Exact term overlap | No term importance model |\n",
    "| 2nd | BM25 | Probabilistic term weighting | Still requires word overlap |\n",
    "| 3rd | Neural (BERT, ColBERT) | Semantic similarity | Computationally expensive |\n",
    "\n",
    "We will see this progression **in practice** using a real search engine (Pyserini) and a real evaluation benchmark (TREC-DL 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF: The Foundation of Lexical Ranking\n",
    "\n",
    "**Term Frequency – Inverse Document Frequency** (TF-IDF) is the most widely used term-weighting scheme.\n",
    "\n",
    "### Term Frequency (TF)\n",
    "\n",
    "The raw term frequency $f(t, d)$ counts how often term $t$ appears in document $d$. To dampen the effect of very frequent terms we often use the log variant:\n",
    "\n",
    "$$\\text{TF}(t, d) = 1 + \\log f(t, d) \\quad\\text{if } f(t, d) > 0,\\;\\text{else } 0$$\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "A term that appears in many documents is less informative. IDF captures this:\n",
    "\n",
    "$$\\text{IDF}(t) = \\log \\frac{N}{df(t)}$$\n",
    "\n",
    "where $N$ is the total number of documents and $df(t)$ is the number of documents containing term $t$.\n",
    "\n",
    "### Combined TF-IDF Score\n",
    "\n",
    "$$\\text{TF\\text{-}IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "The score for a query $q$ against document $d$ sums over all query terms:\n",
    "\n",
    "$$\\text{Score}(q, d) = \\sum_{t \\in q} \\text{TF\\text{-}IDF}(t, d)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'information retrieval search'\n",
      "\n",
      "Term                    IDF\n",
      "----------------------------\n",
      "information           0.693\n",
      "retrieval             0.693\n",
      "search                0.693\n",
      "\n",
      "Doc      Score  Content\n",
      "---------------------------------------------------------------------------\n",
      "D0       1.867  information retrieval is the science of searching for information\n",
      "D1       0.693  machine learning models can improve search relevance\n",
      "D2       2.079  information retrieval systems use inverted indexes for fast search\n",
      "D3       0.000  deep learning transforms natural language understanding\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF demonstration on toy documents\n",
    "\n",
    "documents = [\n",
    "    \"information retrieval is the science of searching for information\",\n",
    "    \"machine learning models can improve search relevance\",\n",
    "    \"information retrieval systems use inverted indexes for fast search\",\n",
    "    \"deep learning transforms natural language understanding\",\n",
    "]\n",
    "query = \"information retrieval search\"\n",
    "\n",
    "# Tokenise\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "doc_tokens = [tokenize(d) for d in documents]\n",
    "query_tokens = tokenize(query)\n",
    "N = len(documents)\n",
    "\n",
    "# Compute document frequency & IDF\n",
    "df = Counter()\n",
    "for tokens in doc_tokens:\n",
    "    for t in set(tokens):\n",
    "        df[t] += 1\n",
    "\n",
    "idf = {t: math.log(N / df[t]) for t in df}\n",
    "\n",
    "# Score each document\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(f\"{'Term':<20} {'IDF':>6}\")\n",
    "print(\"-\" * 28)\n",
    "for t in query_tokens:\n",
    "    print(f\"{t:<20} {idf.get(t, 0):>6.3f}\")\n",
    "\n",
    "print(f\"\\n{'Doc':<5} {'Score':>8}  Content\")\n",
    "print(\"-\" * 75)\n",
    "for i, tokens in enumerate(doc_tokens):\n",
    "    tf = Counter(tokens)\n",
    "    score = sum(\n",
    "        (1 + math.log(tf[t])) * idf.get(t, 0)\n",
    "        for t in query_tokens if tf[t] > 0\n",
    "    )\n",
    "    print(f\"D{i:<4} {score:>8.3f}  {documents[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BM25: The Best Lexical Ranker\n",
    "\n",
    "**BM25** (Best Match 25) is a probabilistic ranking function developed at City University London in the 1990s as part of the Okapi system. It extends TF-IDF with two important improvements:\n",
    "\n",
    "1. **Saturation** — term frequency has diminishing returns (a word appearing 100 times is not 100× more relevant than appearing once)\n",
    "2. **Length normalisation** — longer documents are not automatically favoured\n",
    "\n",
    "### The BM25 Formula\n",
    "\n",
    "$$\\text{BM25}(q, d) = \\sum_{t \\in q} \\text{IDF}(t) \\;\\cdot\\; \\frac{f(t,d) \\cdot (k_1 + 1)}{f(t,d) + k_1 \\cdot \\left(1 - b + b \\cdot \\dfrac{|d|}{\\text{avgdl}}\\right)}$$\n",
    "\n",
    "where:\n",
    "- $f(t, d)$ = frequency of term $t$ in document $d$\n",
    "- $|d|$ = length of document $d$ (in words)\n",
    "- $\\text{avgdl}$ = average document length in the collection\n",
    "- $k_1$ = term frequency saturation parameter (typically **1.2**)\n",
    "- $b$ = length normalisation parameter (typically **0.75**)\n",
    "\n",
    "### IDF Component\n",
    "\n",
    "$$\\text{IDF}(t) = \\log \\frac{N - df(t) + 0.5}{df(t) + 0.5}$$\n",
    "\n",
    "### Understanding the Parameters\n",
    "\n",
    "| $k_1$ | Effect |\n",
    "|-------|--------|\n",
    "| $\\to 0$ | All non-zero term frequencies treated equally (binary matching) |\n",
    "| $\\to \\infty$ | Raw term frequency dominates (no saturation) |\n",
    "\n",
    "| $b$ | Effect |\n",
    "|-----|--------|\n",
    "| $= 0$ | No length normalisation |\n",
    "| $= 1$ | Full normalisation relative to average length |\n",
    "\n",
    "BM25 remains the **default baseline** in modern information retrieval and is the first-stage retriever in most production search systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effect of k1 (term frequency saturation)  [df=50 000, avgdl=60, dl=60]\n",
      "  TF  k1=0.01   k1=0.5    k1=1.2    k1=3.0    k1=10.0 \n",
      "-------------------------------------------------------\n",
      "   1      5.16      5.16      5.16      5.16      5.16\n",
      "   2      5.19      6.20      7.10      8.26      9.47\n",
      "   5      5.21      7.04      9.16     12.91     18.94\n",
      "  10      5.21      7.38     10.15     15.89     28.41\n",
      "  20      5.21      7.56     10.72     17.96     37.88\n",
      "  50      5.22      7.67     11.10     19.49     47.34\n",
      "\n",
      "Effect of b (length normalisation)  [tf=3, k1=1.2, avgdl=60]\n",
      " DocLen   b=0.0     b=0.25    b=0.5     b=0.75    b=1.0  \n",
      "-------------------------------------------------------\n",
      "     20      8.12      8.52      8.97      9.47     10.03\n",
      "     40      8.12      8.31      8.52      8.74      8.97\n",
      "     60      8.12      8.12      8.12      8.12      8.12\n",
      "    100      8.12      7.75      7.41      7.10      6.82\n",
      "    200      8.12      6.96      6.09      5.41      4.87\n",
      "    500      8.12      5.33      3.96      3.16      2.62\n",
      "\n",
      "Key observations:\n",
      "  Higher k1  ->  less saturation  ->  raw TF matters more\n",
      "  Higher b   ->  more length normalisation  ->  short documents boosted\n",
      "  Default (k1=1.2, b=0.75) balances both effects\n"
     ]
    }
   ],
   "source": [
    "# Explore how BM25 parameters k1 and b affect scoring\n",
    "\n",
    "def bm25_term_weight(tf, dl, avgdl, N, df, k1=1.2, b=0.75):\n",
    "    \"\"\"BM25 weight for a single term in a document.\"\"\"\n",
    "    idf = math.log((N - df + 0.5) / (df + 0.5))\n",
    "    tf_part = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * dl / avgdl))\n",
    "    return idf * tf_part\n",
    "\n",
    "# Fixed parameters (MS MARCO scale)\n",
    "N, df_val, avgdl = 8_800_000, 50_000, 60\n",
    "\n",
    "print(\"Effect of k1 (term frequency saturation)  [df=50 000, avgdl=60, dl=60]\")\n",
    "print(f\"{'TF':>4}\", end=\"\")\n",
    "for k1 in [0.01, 0.5, 1.2, 3.0, 10.0]:\n",
    "    print(f\"  k1={k1:<5}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 55)\n",
    "for tf in [1, 2, 5, 10, 20, 50]:\n",
    "    print(f\"{tf:>4}\", end=\"\")\n",
    "    for k1 in [0.01, 0.5, 1.2, 3.0, 10.0]:\n",
    "        w = bm25_term_weight(tf, 60, avgdl, N, df_val, k1=k1, b=0.75)\n",
    "        print(f\"  {w:>8.2f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nEffect of b (length normalisation)  [tf=3, k1=1.2, avgdl={avgdl}]\")\n",
    "print(f\"{'DocLen':>7}\", end=\"\")\n",
    "for b in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    print(f\"   b={b:<5}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 55)\n",
    "for dl in [20, 40, 60, 100, 200, 500]:\n",
    "    print(f\"{dl:>7}\", end=\"\")\n",
    "    for b in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "        w = bm25_term_weight(3, dl, avgdl, N, df_val, k1=1.2, b=b)\n",
    "        print(f\"  {w:>8.2f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  Higher k1  ->  less saturation  ->  raw TF matters more\")\n",
    "print(\"  Higher b   ->  more length normalisation  ->  short documents boosted\")\n",
    "print(\"  Default (k1=1.2, b=0.75) balances both effects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 1 — BM25 Search with Pyserini\n",
    "\n",
    "Now we move from theory to practice. We use [Pyserini](https://github.com/castorini/pyserini), a Python toolkit for reproducible IR research, to perform BM25 search on a real corpus.\n",
    "\n",
    "### Dataset: MS MARCO Passages + TREC-DL 2019\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Corpus** | MS MARCO v1 passage collection — **8.8 million passages** from web documents (Microsoft) |\n",
    "| **Queries** | 43 queries from TREC Deep Learning 2019, selected by NIST for rigorous evaluation |\n",
    "| **Relevance judgments** | Graded assessments by NIST assessors (0 = not relevant … 3 = perfectly relevant) |\n",
    "\n",
    "Pyserini provides a **pre-built Lucene index** for MS MARCO, so we can start searching immediately.\n",
    "\n",
    "> **Note:** The first run downloads the pre-built index (~2 GB). Subsequent runs use the cached version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MS MARCO passage index (first run downloads ~2 GB)...\n",
      "Index loaded: 8,841,823 passages\n",
      "TREC-DL 2019 queries: 43\n",
      "TREC-DL 2019 qrels  : 43 queries with judgments\n",
      "\n",
      "Example queries:\n",
      "  [264014] how long is life cycle of flea                          (152 highly-relevant docs)\n",
      "  [104861] cost of interior concrete flooring                      (111 highly-relevant docs)\n",
      "  [130510] definition declaratory judgment                         (14 highly-relevant docs)\n",
      "  [1114819] what is durable medical equipment consist of            (213 highly-relevant docs)\n",
      "  [1110199] what is wifi vs bluetooth                               (28 highly-relevant docs)\n",
      "  [1129237] hydrogen is a liquid below what temperature             (17 highly-relevant docs)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-built index, queries, and relevance judgments\n",
    "print(\"Loading MS MARCO passage index (first run downloads ~2 GB)...\")\n",
    "searcher = LuceneSearcher.from_prebuilt_index('msmarco-v1-passage')\n",
    "print(f\"Index loaded: {searcher.num_docs:,} passages\")\n",
    "\n",
    "# TREC Deep Learning 2019 topics and official qrels\n",
    "topics = get_topics('dl19-passage')\n",
    "qrels  = get_qrels('dl19-passage')\n",
    "# Pyserini returns qrel values as strings — convert to int for numeric comparisons\n",
    "qrels = {qid: {did: int(r) for did, r in docs.items()} for qid, docs in qrels.items()}\n",
    "print(f\"TREC-DL 2019 queries: {len(topics)}\")\n",
    "print(f\"TREC-DL 2019 qrels  : {len(qrels)} queries with judgments\")\n",
    "\n",
    "# Show a few example queries\n",
    "print(\"\\nExample queries:\")\n",
    "for i, (qid, topic) in enumerate(topics.items()):\n",
    "    if i >= 6:\n",
    "        break\n",
    "    n_rel = sum(1 for r in qrels.get(qid, {}).values() if r >= 2)\n",
    "    print(f\"  [{qid}] {topic['title']:<55} ({n_rel} highly-relevant docs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BM25 search (top-100) for 43 queries...\n",
      "BM25 search complete: 43 queries processed\n",
      "Average results per query: 100\n"
     ]
    }
   ],
   "source": [
    "# Run BM25 search for all TREC-DL 2019 queries (top-100 per query)\n",
    "TOP_K = 100\n",
    "bm25_results = {}   # {qid: [(docid, bm25_score, passage_text), ...]}\n",
    "\n",
    "print(f\"Running BM25 search (top-{TOP_K}) for {len(topics)} queries...\")\n",
    "for qid, topic in topics.items():\n",
    "    query = topic['title']\n",
    "    hits = searcher.search(query, k=TOP_K)\n",
    "    results = []\n",
    "    for hit in hits:\n",
    "        doc = searcher.doc(hit.docid)\n",
    "        passage = json.loads(doc.raw())['contents']\n",
    "        results.append((hit.docid, hit.score, passage))\n",
    "    bm25_results[qid] = results\n",
    "\n",
    "print(f\"BM25 search complete: {len(bm25_results)} queries processed\")\n",
    "print(f\"Average results per query: \"\n",
    "      f\"{np.mean([len(v) for v in bm25_results.values()]):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Query [264014]: how long is life cycle of flea\n",
      "================================================================================\n",
      "\n",
      "  Rank 1 | BM25: 15.7806 | Rel: - | 5611210\n",
      "  5. Cancel. A flea can live up to a year, but its general lifespan depends on its living conditions, such as the availability of hosts. Find out how lo...\n",
      "\n",
      "  Rank 2 | BM25: 15.0908 | Rel: - | 6641238\n",
      "  The life cycle of a flea can last anywhere from 20 days to an entire year. It depends on how long the flea remains in the dormant stage (eggs, larvae,...\n",
      "\n",
      "  Rank 3 | BM25: 14.9718 | Rel: - | 4834547\n",
      "  The life cycle of a flea can last anywhere from 20 days to an entire year. It depends on how long the flea remains in the dormant stage (eggs, larvae,...\n",
      "\n",
      "  Rank 4 | BM25: 14.2151 | Rel: - | 96852\n",
      "  Flea Pupa. The flea larvae spin cocoons around themselves in which they move to the last phase of the flea life cycle and become adult fleas. The larv...\n",
      "\n",
      "  Rank 5 | BM25: 13.9852 | Rel: - | 96854\n",
      "  2) The fleas life cycle discussed - the flea life cycle diagram explained in full. 2a) Fleas life cycle 1 - The adult flea lays her eggs on the host a...\n",
      "\n",
      "================================================================================\n",
      "Query [104861]: cost of interior concrete flooring\n",
      "================================================================================\n",
      "\n",
      "  Rank 1 | BM25: 15.3969 | Rel: - | 459676\n",
      "  Day's Concrete Floors, Inc is the name of my company. We pour a lot of 3000 psi. concrete for interior concrete floors and 4000 psi concrete for exter...\n",
      "\n",
      "  Rank 2 | BM25: 14.3471 | Rel: - | 6351571\n",
      "  The cost of concrete is a very important part of planning and budgeting for your specific concrete project. Because the cost of a cubic yard of concre...\n",
      "\n",
      "  Rank 3 | BM25: 14.2178 | Rel: - | 5703401\n",
      "  Day's Concrete Floors, Inc is the name of my company. We pour a lot of 3000 psi. concrete for interior concrete floors and 4000 psi concrete for exter...\n",
      "\n",
      "  Rank 4 | BM25: 14.1983 | Rel: - | 459675\n",
      "  The slab you see us pouring below used 120 yards of concrete. The cost per cubic yard of 3000 psi concrete for this slab was $92.00 dollars. That's $1...\n",
      "\n",
      "  Rank 5 | BM25: 14.1849 | Rel: - | 5864693\n",
      "  How much do stained concrete floors cost? 1  For those that want to get acid stained concrete work done outside, a simple stained concrete job can var...\n",
      "\n",
      "================================================================================\n",
      "Query [130510]: definition declaratory judgment\n",
      "================================================================================\n",
      "\n",
      "  Rank 1 | BM25: 13.6767 | Rel: - | 1494936\n",
      "  A declaratory judgment, sometimes called declaratory relief, is conclusive and legally binding as to the present and future rights of the parties invo...\n",
      "\n",
      "  Rank 2 | BM25: 13.4221 | Rel: - | 7501563\n",
      "  Rule 57. These rules govern the procedure for obtaining a declaratory judgment under 28 U.S.C. Â§2201. Rules 38 and 39 govern a demand for a jury tria...\n",
      "\n",
      "  Rank 3 | BM25: 13.3868 | Rel: - | 7125239\n",
      "  Common law. 1  Consent judgment: a consent judgment is available where the parties agree on the terms of the judgment or order that should be made. 2 ...\n",
      "\n",
      "  Rank 4 | BM25: 13.2078 | Rel: - | 996732\n",
      "  The same court observed that even if the basic requirements for A Declaratory Judgment Action are met, it is still within the discretion of the distri...\n",
      "\n",
      "  Rank 5 | BM25: 13.0677 | Rel: - | 1494935\n",
      "  9.3 Declaratory Judgment Act. The Declaratory Judgment Act offers a unique mechanism by which advocates may seek to remedy ongoing violations of statu...\n"
     ]
    }
   ],
   "source": [
    "# Display BM25 top-5 for a few example queries\n",
    "example_qids = list(topics.keys())[:3]\n",
    "\n",
    "for qid in example_qids:\n",
    "    query = topics[qid]['title']\n",
    "    results = bm25_results[qid]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query [{qid}]: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for rank, (docid, score, passage) in enumerate(results[:5], 1):\n",
    "        rel = qrels.get(qid, {}).get(docid, '-')\n",
    "        print(f\"\\n  Rank {rank} | BM25: {score:.4f} | Rel: {rel} | {docid}\")\n",
    "        print(f\"  {passage[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Lexical Gap\n",
    "\n",
    "BM25 works well when query and document share the same vocabulary. But what happens when they use **different words for the same concept**?\n",
    "\n",
    "| Query | Relevant passage uses… | BM25 can match? |\n",
    "|-------|------------------------|-----------------|\n",
    "| \"fix broken screen\" | \"display repair guide\" | No word overlap |\n",
    "| \"heart attack symptoms\" | \"signs of myocardial infarction\" | Medical synonyms |\n",
    "| \"affordable housing\" | \"low-cost residential options\" | Paraphrases |\n",
    "\n",
    "This is the **vocabulary mismatch problem** — the fundamental limitation of all lexical methods, including BM25. No matter how sophisticated the term weighting, if the words do not match the document will not be found.\n",
    "\n",
    "Let us quantify this on our TREC-DL data: how many highly-relevant documents does BM25 actually find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly-relevant documents (qrel >= 2) found in BM25 top-100\n",
      "\n",
      "QID        Query                                         Found Total  Recall\n",
      "------------------------------------------------------------------------------\n",
      "19335      anthropological definition of environment         0     7    0.0%\n",
      "47923      axon terminals or synaptic knob definition        0    41    0.0%\n",
      "87181      causes of left ventricular hypertrophy            0    31    0.0%\n",
      "87452      causes of military suicide                        0    31    0.0%\n",
      "104861     cost of interior concrete flooring                0   111    0.0%\n",
      "130510     definition declaratory judgment                   0    14    0.0%\n",
      "131843     definition of a sigmet                            0    19    0.0%\n",
      "146187     difference between a mcdouble and a double        0     8    0.0%\n",
      "148538     difference between rn and bsn                     0    32    0.0%\n",
      "156493     do goldfish grow                                  0   117    0.0%\n",
      "168216     does legionella pneumophila cause pneumonia       0   200    0.0%\n",
      "182539     example of monotonic function                     0     9    0.0%\n",
      "183378     exons definition biology                          0   175    0.0%\n",
      "207786     how are some sharks warm blooded                  0    11    0.0%\n",
      "264014     how long is life cycle of flea                    0   152    0.0%\n",
      "359349     how to find the midsegment of a trapezoid         0    25    0.0%\n",
      "405717     is cdg airport in main paris                      0     7    0.0%\n",
      "443396     lps laws definition                               0    63    0.0%\n",
      "451602     medicare's definition of mechanical ventila       0   100    0.0%\n",
      "489204     right pelvic pain causes                          0    24    0.0%\n",
      "490595     rsa definition key                                0    24    0.0%\n",
      "527433     types of dysarthria from cerebral palsy           0    34    0.0%\n",
      "573724     what are the social determinants of health        0    13    0.0%\n",
      "833860     what is the most popular food in switzerlan       0    42    0.0%\n",
      "855410     what is theraderm used for                        0     3    0.0%\n",
      "915593     what types of food can you cook sous vide         0    79    0.0%\n",
      "962179     when was the salvation army founded               0    21    0.0%\n",
      "1037798    who is robert gray                                0     7    0.0%\n",
      "1063750    why did the us volunterilay enter ww1             0   183    0.0%\n",
      "1103812    who formed the commonwealth of independent        0    11    0.0%\n",
      "1106007    define visceral?                                  0    41    0.0%\n",
      "1110199    what is wifi vs bluetooth                         0    28    0.0%\n",
      "1112341    what is the daily life of thai people             0   119    0.0%\n",
      "1113437    what is physical description of spruce            0    25    0.0%\n",
      "1114646    what is famvir prescribed for                     0    12    0.0%\n",
      "1114819    what is durable medical equipment consist o       0   213    0.0%\n",
      "1115776    what is an aml surveillance analyst               0     4    0.0%\n",
      "1117099    what is a active margin                           0    83    0.0%\n",
      "1121402    what can contour plowing reduce                   0    23    0.0%\n",
      "1121709    what are the three percenters?                    0     3    0.0%\n",
      "1124210    tracheids are part of _____.                      0   120    0.0%\n",
      "1129237    hydrogen is a liquid below what temperature       0    17    0.0%\n",
      "1133167    how is the weather in jamaica                     0   219    0.0%\n",
      "\n",
      "Mean recall of highly-relevant docs: 0.0%\n",
      "Queries with < 100% recall: 43/43\n",
      "\n",
      "BM25 misses some relevant passages — this is the lexical gap in action.\n"
     ]
    }
   ],
   "source": [
    "# Lexical Gap Analysis: how many highly-relevant docs (qrel >= 2)\n",
    "# appear in BM25 top-100?\n",
    "\n",
    "print(\"Highly-relevant documents (qrel >= 2) found in BM25 top-100\\n\")\n",
    "print(f\"{'QID':<10} {'Query':<45} {'Found':>5} {'Total':>5} {'Recall':>7}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "recall_values = []\n",
    "for qid in sorted(topics.keys()):\n",
    "    if qid not in qrels:\n",
    "        continue\n",
    "    query = topics[qid]['title']\n",
    "    relevant = {did for did, r in qrels[qid].items() if r >= 2}\n",
    "    if not relevant:\n",
    "        continue\n",
    "    retrieved = {docid for docid, _, _ in bm25_results.get(qid, [])}\n",
    "    found = relevant & retrieved\n",
    "    recall = len(found) / len(relevant)\n",
    "    recall_values.append(recall)\n",
    "    print(f\"{qid:<10} {query[:43]:<45} {len(found):>5} {len(relevant):>5} {recall:>7.1%}\")\n",
    "\n",
    "print(f\"\\nMean recall of highly-relevant docs: {np.mean(recall_values):.1%}\")\n",
    "print(f\"Queries with < 100% recall: \"\n",
    "      f\"{sum(1 for r in recall_values if r < 1.0)}/{len(recall_values)}\")\n",
    "print(\"\\nBM25 misses some relevant passages — this is the lexical gap in action.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Ranking: Beyond Exact Match\n",
    "\n",
    "Neural ranking models use **learned representations** (embeddings) to capture semantic similarity between queries and documents. Instead of matching words they match *meanings*.\n",
    "\n",
    "### Three Architectures for Neural Ranking\n",
    "\n",
    "| Architecture | Example | How it works | Speed | Quality |\n",
    "|-------------|---------|-------------|-------|---------|\n",
    "| **Bi-encoder** | DPR, SBERT | Query and doc encoded independently; cosine similarity | Fast | Good |\n",
    "| **Late interaction** | ColBERT | Token-level embeddings; MaxSim aggregation | Medium | Better |\n",
    "| **Cross-encoder** | monoBERT | Query + doc processed jointly by a transformer | Slow | Best |\n",
    "\n",
    "### Bi-Encoder: Independent Encoding (DPR & SBERT)\n",
    "\n",
    "A bi-encoder uses **two separate BERT towers** — one for the query, one for the document — each producing a single dense vector (the `[CLS]` token representation). Relevance is then a simple **cosine similarity** or **dot product** between these vectors:\n",
    "\n",
    "$$\\text{score}(q, d) = \\mathbf{E}_q(q)^\\top\\;\\mathbf{E}_d(d)$$\n",
    "\n",
    "Because query and document are encoded **independently**, all document vectors can be pre-computed offline and indexed (e.g. with FAISS). At query time only the query needs to be encoded, making retrieval over millions of documents extremely fast.\n",
    "\n",
    "#### DPR — Dense Passage Retrieval (Karpukhin et al., 2020)\n",
    "\n",
    "DPR was one of the first bi-encoders designed specifically for **open-domain question answering**. Key design choices:\n",
    "\n",
    "- **Two independent BERT-base models**: $\\mathbf{E}_q$ (query encoder) and $\\mathbf{E}_d$ (passage encoder) — they do **not** share weights\n",
    "- **Training signal**: contrastive learning with in-batch negatives. For each question $q_i$ the positive passage $d_i^+$ is from a gold QA dataset; the negatives are the positive passages of all other questions in the same mini-batch, plus one \"hard negative\" retrieved by BM25\n",
    "- **Loss**: negative log-likelihood of the positive passage:\n",
    "\n",
    "$$\\mathcal{L} = -\\log\\frac{e^{\\mathbf{E}_q(q_i)^\\top \\mathbf{E}_d(d_i^+)}}{e^{\\mathbf{E}_q(q_i)^\\top \\mathbf{E}_d(d_i^+)} + \\sum_{j}\\,e^{\\mathbf{E}_q(q_i)^\\top \\mathbf{E}_d(d_j^-)}}$$\n",
    "\n",
    "- **Retrieval**: all passages are pre-encoded; at query time a FAISS index returns the top-$k$ by dot-product in milliseconds\n",
    "\n",
    "DPR showed that a learned dense retriever can **outperform BM25** on factoid-style questions (Natural Questions, TriviaQA), even without lexical overlap. Its limitation: training requires large QA datasets with gold passages.\n",
    "\n",
    "#### SBERT — Sentence-BERT (Reimers & Gurevych, 2019)\n",
    "\n",
    "While DPR targets retrieval, **SBERT** focuses on producing **general-purpose sentence embeddings** that make cosine similarity a meaningful measure of semantic similarity. Key differences from DPR:\n",
    "\n",
    "- **Shared weights (Siamese network)**: the same BERT model encodes both sentences — weight sharing improves generalisation to new domains\n",
    "- **Pooling**: instead of just using `[CLS]`, SBERT typically applies **mean pooling** over all token embeddings, which produces better sentence representations:\n",
    "\n",
    "$$\\mathbf{v}_s = \\frac{1}{|s|}\\sum_{t \\in s}\\text{BERT}(t)$$\n",
    "\n",
    "- **Training objectives**: SBERT is trained in two stages:\n",
    "  1. **Natural Language Inference (NLI)**: a classification head predicts *entailment / contradiction / neutral* for sentence pairs — this teaches the model what \"same meaning\" looks like\n",
    "  2. **Cosine similarity regression**: the model is fine-tuned so that $\\cos(\\mathbf{v}_a, \\mathbf{v}_b)$ predicts the human-annotated similarity score (e.g. STS Benchmark)\n",
    "\n",
    "- **Result**: SBERT embeddings are useful for many tasks beyond retrieval — semantic search, clustering, paraphrase detection, duplicate question finding\n",
    "\n",
    "#### DPR vs SBERT — When to Use Which?\n",
    "\n",
    "| | DPR | SBERT |\n",
    "|---|---|---|\n",
    "| **Architecture** | Two separate encoders | One shared encoder (Siamese) |\n",
    "| **Training data** | QA pairs with gold passages | NLI + semantic similarity datasets |\n",
    "| **Best at** | Open-domain passage retrieval | General-purpose semantic similarity |\n",
    "| **Retrieval** | Designed for FAISS-based top-$k$ | Often used for reranking or similarity |\n",
    "| **Embedding dim** | 768 (BERT-base) | 384–768 (model dependent) |\n",
    "\n",
    "In practice, the `sentence-transformers` library (which we use in this tutorial) provides pre-trained models from both families. The `all-MiniLM-L6-v2` model used in our Neural Relevance Feedback section is an SBERT-style model: shared weights, mean pooling, trained on 1B+ sentence pairs.\n",
    "\n",
    "### ColBERT: Late Interaction via MaxSim\n",
    "\n",
    "ColBERT (Contextualized Late Interaction over BERT) scores a query–document pair by:\n",
    "\n",
    "1. Encoding query tokens:  $\\mathbf{Q} = [\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_m]$\n",
    "2. Encoding document tokens:  $\\mathbf{D} = [\\mathbf{d}_1, \\mathbf{d}_2, \\ldots, \\mathbf{d}_n]$\n",
    "3. Computing **MaxSim** — for each query token, find its maximum similarity to any document token:\n",
    "\n",
    "$$\\text{ColBERT}(q, d) = \\sum_{i=1}^{m} \\max_{j=1}^{n}\\; \\mathbf{q}_i^\\top \\mathbf{d}_j$$\n",
    "\n",
    "This captures fine-grained token-level semantic matches — for example \"screen\" matching \"display\" through contextual embeddings.\n",
    "\n",
    "### Cross-Encoder: Joint Encoding\n",
    "\n",
    "A cross-encoder feeds the concatenated query–document pair through BERT:\n",
    "\n",
    "$$\\text{Score}(q, d) = \\text{BERT}_{\\text{cls}}\\!\\bigl([\\texttt{CLS}]\\; q \\;[\\texttt{SEP}]\\; d \\;[\\texttt{SEP}]\\bigr)$$\n",
    "\n",
    "Cross-encoders are the most powerful but slowest neural rankers — they are used to **rerank** a small candidate set retrieved by BM25.\n",
    "\n",
    "### The Speed–Quality Trade-off\n",
    "\n",
    "```\n",
    "Quality:   Bi-encoder  <  ColBERT  <  Cross-encoder\n",
    "Speed:     Bi-encoder  >  ColBERT  >  Cross-encoder\n",
    "```\n",
    "\n",
    "A common production pipeline combines all three in a **telescoping** architecture: bi-encoder retrieves 1000 → ColBERT reranks to 100 → cross-encoder selects top-10.\n",
    "\n",
    "In this tutorial we use a cross-encoder to rerank BM25's top-100 results. This demonstrates the same principle as ColBERT: **semantic understanding beats keyword matching**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 2 — Neural Reranking of BM25 Results\n",
    "\n",
    "We now apply a neural cross-encoder to rerank the BM25 top-100 results. The model — `cross-encoder/ms-marco-MiniLM-L-6-v2` — was trained on the MS MARCO dataset to predict query–passage relevance.\n",
    "\n",
    "**Pipeline:**\n",
    "1. **BM25** retrieves top-100 candidate passages (fast, recall-oriented)\n",
    "2. **Cross-encoder** scores each (query, passage) pair (slower, precision-oriented)\n",
    "3. Passages are **reranked** by the cross-encoder score\n",
    "\n",
    "This **retrieve-then-rerank** pattern is the standard approach in modern search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cross-encoder: cross-encoder/ms-marco-MiniLM-L-6-v2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 105/105 [00:00<00:00, 1152.35it/s, Materializing param=classifier.weight]                                    \n",
      "\u001b[1mBertForSequenceClassification LOAD REPORT\u001b[0m from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "\n",
      "2026-02-18 21:49:38,449 - huggingface_hub.utils._http - WARNING - Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder loaded.\n",
      "\n",
      "Reranking 43 queries (top-100 each)...\n",
      "  10/43 queries reranked\n",
      "  20/43 queries reranked\n",
      "  30/43 queries reranked\n",
      "  40/43 queries reranked\n",
      "  43/43 queries reranked\n",
      "Neural reranking complete.\n"
     ]
    }
   ],
   "source": [
    "# Load cross-encoder model\n",
    "print(\"Loading cross-encoder: cross-encoder/ms-marco-MiniLM-L-6-v2 ...\")\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"Cross-encoder loaded.\\n\")\n",
    "\n",
    "# Rerank all queries\n",
    "reranked_results = {}   # {qid: [(docid, ce_score, passage), ...]}\n",
    "\n",
    "print(f\"Reranking {len(bm25_results)} queries (top-{TOP_K} each)...\")\n",
    "for i, (qid, results) in enumerate(bm25_results.items()):\n",
    "    query = topics[qid]['title']\n",
    "\n",
    "    # Prepare (query, passage) pairs\n",
    "    pairs = [(query, passage) for _, _, passage in results]\n",
    "\n",
    "    # Score all pairs at once\n",
    "    ce_scores = cross_encoder.predict(pairs, show_progress_bar=False)\n",
    "\n",
    "    # Sort by cross-encoder score (descending)\n",
    "    reranked = sorted(\n",
    "        [(docid, float(sc), passage)\n",
    "         for (docid, _, passage), sc in zip(results, ce_scores)],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )\n",
    "    reranked_results[qid] = reranked\n",
    "\n",
    "    if (i + 1) % 10 == 0 or (i + 1) == len(bm25_results):\n",
    "        print(f\"  {i+1}/{len(bm25_results)} queries reranked\")\n",
    "\n",
    "print(\"Neural reranking complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "Query [264014]: how long is life cycle of flea\n",
      "==========================================================================================\n",
      "    #  BM25#   Move  CE Score  Rel  Passage\n",
      "  ----------------------------------------------------------------------------------\n",
      "    1      3     +2   10.0076    -  The life cycle of a flea can last anywhere from 20...\n",
      "    2      2      =    9.7576    -  The life cycle of a flea can last anywhere from 20...\n",
      "    3     29    +26    9.5761    -  How long is the life span of a flea? 30-90 Days (A...\n",
      "    4     32    +28    9.5698    -  How long is the life span of a flea? 30-90 Days (A...\n",
      "    5     72    +67    9.2482    -  The total flea life cycle can range from a couple ...\n",
      "    6     56    +50    9.0978    -  Stickfast flea life history. The complete life cyc...\n",
      "    7     74    +67    9.0556    -  Stickfast flea life history. The complete life cyc...\n",
      "    8     22    +14    9.0464    -  Fleas have four main stages in their life cycle: e...\n",
      "    9     39    +30    8.9141    -  There are four stages in the life cycle of a flea:...\n",
      "   10     45    +35    8.8417    -  Fleas pass through a complete life cycle of four s...\n",
      "\n",
      "==========================================================================================\n",
      "Query [104861]: cost of interior concrete flooring\n",
      "==========================================================================================\n",
      "    #  BM25#   Move  CE Score  Rel  Passage\n",
      "  ----------------------------------------------------------------------------------\n",
      "    1      3     +2    7.9130    -  Day's Concrete Floors, Inc is the name of my compa...\n",
      "    2     11     +9    7.7421    -  We pour a lot of 3000 psi. concrete for interior c...\n",
      "    3      9     +6    7.7420    -  Day's Concrete Floors, Inc is the name of my compa...\n",
      "    4      1     -3    7.3253    -  Day's Concrete Floors, Inc is the name of my compa...\n",
      "    5     16    +11    7.2993    -  We pour a lot of 3000 psi. concrete for interior c...\n",
      "    6     12     +6    7.2419    -  We pour a lot of 3000 psi. concrete for interior c...\n",
      "    7     72    +65    7.1553    -  Video on the cost of concrete floors. Depending on...\n",
      "    8     69    +61    6.6848    -  video on the cost of concrete floors depending on ...\n",
      "    9     52    +43    6.5222    -  Video on the cost of concrete floors. Depending on...\n",
      "   10     34    +24    6.4574    -  Video on the cost of concrete floors. Depending on...\n",
      "\n",
      "==========================================================================================\n",
      "Query [130510]: definition declaratory judgment\n",
      "==========================================================================================\n",
      "    #  BM25#   Move  CE Score  Rel  Passage\n",
      "  ----------------------------------------------------------------------------------\n",
      "    1     12    +11   11.0541    -  Wiktionary (0.00 / 0 votes) Rate this definition: ...\n",
      "    2     19    +17   10.5971    -  A declaratory judgment, also called a declaration,...\n",
      "    3     24    +21   10.5891    -  A declaratory judgment is one which simply declare...\n",
      "    4      6     +2   10.4457    -  Declaratory judgments are judgment of a court in a...\n",
      "    5      1     -4    9.8798    -  A declaratory judgment, sometimes called declarato...\n",
      "    6      7     +1    9.3359    -  A ruling by a court (or, less commonly, a regulato...\n",
      "    7     26    +19    9.0211    -  The declaratory judgment is generally considered a...\n",
      "    8     28    +20    8.1573    -  A declaratory judgment does not provide for any en...\n",
      "    9     79    +70    7.7629    -  Adj. 1. declaratory-relating to the use of or havi...\n",
      "   10      9     -1    7.3828    -  An insurance company, for example, might seek a de...\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side: BM25 ranking vs Neural reranking (top-10)\n",
    "example_qids = list(topics.keys())[:3]\n",
    "\n",
    "for qid in example_qids:\n",
    "    query = topics[qid]['title']\n",
    "\n",
    "    # Build BM25 rank lookup\n",
    "    bm25_rank = {docid: r for r, (docid, _, _)\n",
    "                 in enumerate(bm25_results[qid], 1)}\n",
    "\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"Query [{qid}]: {query}\")\n",
    "    print(f\"{'='*90}\")\n",
    "    print(f\"  {'#':>3} {'BM25#':>6} {'Move':>6} {'CE Score':>9} {'Rel':>4}  Passage\")\n",
    "    print(f\"  {'-'*82}\")\n",
    "\n",
    "    for rank, (docid, ce_score, passage) in enumerate(reranked_results[qid][:10], 1):\n",
    "        old = bm25_rank.get(docid, TOP_K + 1)\n",
    "        delta = old - rank\n",
    "        if delta > 0:\n",
    "            arrow = f\"+{delta}\"\n",
    "        elif delta < 0:\n",
    "            arrow = str(delta)\n",
    "        else:\n",
    "            arrow = \"=\"\n",
    "        rel = qrels.get(qid, {}).get(docid, '-')\n",
    "        print(f\"  {rank:>3} {old:>6} {arrow:>6} {ce_score:>9.4f} {str(rel):>4}\"\n",
    "              f\"  {passage[:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-Change Analysis  (Neural top-10 vs BM25 ranking)\n",
      "--------------------------------------------------\n",
      "Total (query, passage) pairs : 430\n",
      "Passages moved UP            : 335  (77.9%)\n",
      "Big jumps (moved up >= 10)   : 230  (53.5%)\n",
      "\n",
      "Neural reranking reshuffles the top results significantly,\n",
      "promoting semantically relevant passages that BM25 ranked lower.\n"
     ]
    }
   ],
   "source": [
    "# Aggregate rank-change analysis\n",
    "total_up, total_big, total_pairs = 0, 0, 0\n",
    "\n",
    "for qid in bm25_results:\n",
    "    bm25_rank = {did: r for r, (did, _, _) in enumerate(bm25_results[qid], 1)}\n",
    "    for new_rank, (docid, _, _) in enumerate(reranked_results[qid][:10], 1):\n",
    "        old_rank = bm25_rank.get(docid, TOP_K + 1)\n",
    "        change = old_rank - new_rank\n",
    "        if change > 0:\n",
    "            total_up += 1\n",
    "        if change >= 10:\n",
    "            total_big += 1\n",
    "        total_pairs += 1\n",
    "\n",
    "print(\"Rank-Change Analysis  (Neural top-10 vs BM25 ranking)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total (query, passage) pairs : {total_pairs}\")\n",
    "print(f\"Passages moved UP            : {total_up}  \"\n",
    "      f\"({100*total_up/total_pairs:.1f}%)\")\n",
    "print(f\"Big jumps (moved up >= 10)   : {total_big}  \"\n",
    "      f\"({100*total_big/total_pairs:.1f}%)\")\n",
    "print()\n",
    "print(\"Neural reranking reshuffles the top results significantly,\")\n",
    "print(\"promoting semantically relevant passages that BM25 ranked lower.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb1b70e",
   "metadata": {},
   "source": [
    "### Concrete Examples: Where Neural Reranking Makes a Difference\n",
    "\n",
    "The aggregate numbers above tell us that neural reranking reshuffles results, but let us look at **specific passages** to understand *why*. Below we automatically find the most dramatic rank improvements — passages that BM25 buried deep in the ranking but the cross-encoder promoted into the top-10 — and show the actual text so you can see the semantic connections that BM25 missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most dramatic rank improvements — passages that were\n",
    "# buried by BM25 but promoted to the top by the cross-encoder.\n",
    "\n",
    "print(\"CONCRETE EXAMPLES: Neural reranking rescuing relevant passages\\n\")\n",
    "\n",
    "examples_shown = 0\n",
    "MAX_EXAMPLES = 5\n",
    "\n",
    "# Collect all (query, passage, old_rank, new_rank, relevance, ce_score) tuples\n",
    "dramatic = []\n",
    "for qid in bm25_results:\n",
    "    bm25_rank = {did: r for r, (did, _, _) in enumerate(bm25_results[qid], 1)}\n",
    "    for new_rank, (docid, ce_score, passage) in enumerate(reranked_results[qid][:10], 1):\n",
    "        old_rank = bm25_rank.get(docid, TOP_K + 1)\n",
    "        rel = qrels.get(qid, {}).get(docid, 0)\n",
    "        jump = old_rank - new_rank\n",
    "        # We want: big jump AND actually relevant\n",
    "        if jump >= 15 and rel >= 2:\n",
    "            dramatic.append((jump, qid, docid, old_rank, new_rank, ce_score, rel, passage))\n",
    "\n",
    "# Sort by biggest jump first\n",
    "dramatic.sort(key=lambda x: -x[0])\n",
    "\n",
    "for jump, qid, docid, old_rank, new_rank, ce_score, rel, passage in dramatic[:MAX_EXAMPLES]:\n",
    "    query = topics[qid]['title']\n",
    "    examples_shown += 1\n",
    "    print(f\"{'─'*90}\")\n",
    "    print(f\"Example {examples_shown}\")\n",
    "    print(f\"  Query [{qid}]: {query}\")\n",
    "    print(f\"  BM25 rank: {old_rank}  →  Neural rank: {new_rank}  \"\n",
    "          f\"(jumped +{jump} positions)  |  Relevance: {rel}/3\")\n",
    "    print(f\"  Cross-encoder score: {ce_score:.4f}\")\n",
    "    print(f\"\\n  Passage ({docid}):\")\n",
    "    # Word-wrap the passage for readability\n",
    "    words = passage.split()\n",
    "    line = \"    \"\n",
    "    for w in words:\n",
    "        if len(line) + len(w) + 1 > 88:\n",
    "            print(line)\n",
    "            line = \"    \" + w\n",
    "        else:\n",
    "            line += \" \" + w if line.strip() else \"    \" + w\n",
    "    print(line)\n",
    "\n",
    "    # Show WHY BM25 missed it — compute query-passage word overlap\n",
    "    q_words = set(query.lower().split())\n",
    "    p_words = set(passage.lower().split())\n",
    "    overlap = q_words & p_words\n",
    "    missing = q_words - p_words\n",
    "    print(f\"\\n  Query terms found in passage: {overlap if overlap else '(none)'}\")\n",
    "    print(f\"  Query terms MISSING:          {missing if missing else '(all present)'}\")\n",
    "    if missing:\n",
    "        print(f\"  → BM25 scored this low because it could not match: {missing}\")\n",
    "        print(f\"  → The cross-encoder understood the semantic connection anyway.\")\n",
    "    print()\n",
    "\n",
    "if examples_shown == 0:\n",
    "    print(\"No dramatic examples found with jump >= 15 and relevance >= 2.\")\n",
    "    print(\"Trying with relaxed threshold (jump >= 5)...\")\n",
    "    for qid in bm25_results:\n",
    "        bm25_rank = {did: r for r, (did, _, _) in enumerate(bm25_results[qid], 1)}\n",
    "        for new_rank, (docid, ce_score, passage) in enumerate(reranked_results[qid][:10], 1):\n",
    "            old_rank = bm25_rank.get(docid, TOP_K + 1)\n",
    "            rel = qrels.get(qid, {}).get(docid, 0)\n",
    "            jump = old_rank - new_rank\n",
    "            if jump >= 5 and rel >= 1 and examples_shown < MAX_EXAMPLES:\n",
    "                query = topics[qid]['title']\n",
    "                examples_shown += 1\n",
    "                print(f\"\\n{'─'*90}\")\n",
    "                print(f\"Example {examples_shown}\")\n",
    "                print(f\"  Query [{qid}]: {query}\")\n",
    "                print(f\"  BM25 rank: {old_rank}  →  Neural rank: {new_rank}  \"\n",
    "                      f\"(jumped +{jump} positions)  |  Relevance: {rel}/3\")\n",
    "                print(f\"  Cross-encoder score: {ce_score:.4f}\")\n",
    "                print(f\"  Passage: {passage[:200]}...\")\n",
    "\n",
    "print(f\"\\n{'─'*90}\")\n",
    "print(f\"\\nThese examples illustrate the lexical gap: BM25 cannot match synonyms,\")\n",
    "print(f\"paraphrases, or conceptually related terms. The neural cross-encoder\")\n",
    "print(f\"understands meaning and promotes truly relevant passages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics: nDCG and MAP\n",
    "\n",
    "How do we **objectively measure** whether one ranking is better than another? We use standard IR evaluation metrics computed against human relevance judgments.\n",
    "\n",
    "### Normalised Discounted Cumulative Gain (nDCG@k)\n",
    "\n",
    "nDCG rewards relevant documents at high positions using **graded relevance** (0, 1, 2, 3):\n",
    "\n",
    "$$\\text{DCG}@k = \\sum_{i=1}^{k} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}$$\n",
    "\n",
    "$$\\text{nDCG}@k = \\frac{\\text{DCG}@k}{\\text{IDCG}@k}$$\n",
    "\n",
    "where IDCG is the DCG of the ideal (perfect) ranking. nDCG ranges from 0 to 1.\n",
    "\n",
    "### Mean Average Precision (MAP)\n",
    "\n",
    "MAP measures how well **all** relevant documents are ranked, using **binary relevance**:\n",
    "\n",
    "$$\\text{AP}(q) = \\frac{1}{|R_q|} \\sum_{k=1}^{n} P@k \\cdot \\text{rel}(k)$$\n",
    "\n",
    "$$\\text{MAP} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{AP}(q)$$\n",
    "\n",
    "where $P@k$ is precision at rank $k$ and $\\text{rel}(k)$ is 1 if the document at rank $k$ is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check — test ranking: [3, 2, 0, 1, 0, 0, 2, 0, 0, 0]\n",
      "  DCG@10 : 10.3235\n",
      "  nDCG@10: 0.9538\n",
      "  AP     : 0.8304\n"
     ]
    }
   ],
   "source": [
    "# Implementation of evaluation metrics\n",
    "\n",
    "def dcg_at_k(relevances, k):\n",
    "    \"\"\"DCG@k given a list of relevance scores in ranking order.\"\"\"\n",
    "    return sum(\n",
    "        (2 ** rel - 1) / math.log2(i + 2)\n",
    "        for i, rel in enumerate(relevances[:k])\n",
    "    )\n",
    "\n",
    "def ndcg_at_k(ranked_rels, all_rels, k):\n",
    "    \"\"\"nDCG@k: ranked_rels = relevances in system order;\n",
    "    all_rels = all known relevance values (for IDCG).\"\"\"\n",
    "    idcg = dcg_at_k(sorted(all_rels, reverse=True), k)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(ranked_rels, k) / idcg\n",
    "\n",
    "def average_precision(ranked_binary, total_relevant):\n",
    "    \"\"\"Average Precision given binary relevances in ranking order.\"\"\"\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    ap, hits = 0.0, 0\n",
    "    for i, rel in enumerate(ranked_binary):\n",
    "        if rel:\n",
    "            hits += 1\n",
    "            ap += hits / (i + 1)\n",
    "    return ap / total_relevant\n",
    "\n",
    "# Sanity check\n",
    "test_rels = [3, 2, 0, 1, 0, 0, 2, 0, 0, 0]\n",
    "print(\"Sanity check — test ranking:\", test_rels)\n",
    "print(f\"  DCG@10 : {dcg_at_k(test_rels, 10):.4f}\")\n",
    "print(f\"  nDCG@10: {ndcg_at_k(test_rels, test_rels, 10):.4f}\")\n",
    "test_bin = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "print(f\"  AP     : {average_precision(test_bin, 4):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stage 3 — Quantitative Comparison\n",
    "\n",
    "We now evaluate both ranking approaches — **BM25** and **Neural Reranking** — using the official TREC-DL 2019 relevance judgments.\n",
    "\n",
    "For each of the 43 queries we compute:\n",
    "- **nDCG@10**: graded relevance quality at the top of the ranking\n",
    "- **MAP** (with relevance threshold $\\geq 2$): binary precision across all ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QID        Query                                        BM25   Neural    Delta\n",
      "------------------------------------------------------------------------------\n",
      "19335      anthropological definition of environm     0.0000   0.0000  +0.0000 =\n",
      "47923      axon terminals or synaptic knob defini     0.0000   0.0000  +0.0000 =\n",
      "87181      causes of left ventricular hypertrophy     0.0000   0.0000  +0.0000 =\n",
      "87452      causes of military suicide                 0.0000   0.0000  +0.0000 =\n",
      "104861     cost of interior concrete flooring         0.0000   0.0000  +0.0000 =\n",
      "130510     definition declaratory judgment            0.0000   0.0000  +0.0000 =\n",
      "131843     definition of a sigmet                     0.0000   0.0000  +0.0000 =\n",
      "146187     difference between a mcdouble and a do     0.0000   0.0000  +0.0000 =\n",
      "148538     difference between rn and bsn              0.0000   0.0000  +0.0000 =\n",
      "156493     do goldfish grow                           0.0000   0.0000  +0.0000 =\n",
      "168216     does legionella pneumophila cause pneu     0.0000   0.0000  +0.0000 =\n",
      "182539     example of monotonic function              0.0000   0.0000  +0.0000 =\n",
      "183378     exons definition biology                   0.0000   0.0000  +0.0000 =\n",
      "207786     how are some sharks warm blooded           0.0000   0.0000  +0.0000 =\n",
      "264014     how long is life cycle of flea             0.0000   0.0000  +0.0000 =\n",
      "359349     how to find the midsegment of a trapez     0.0000   0.0000  +0.0000 =\n",
      "405717     is cdg airport in main paris               0.0000   0.0000  +0.0000 =\n",
      "443396     lps laws definition                        0.0000   0.0000  +0.0000 =\n",
      "451602     medicare's definition of mechanical ve     0.0000   0.0000  +0.0000 =\n",
      "489204     right pelvic pain causes                   0.0000   0.0000  +0.0000 =\n",
      "490595     rsa definition key                         0.0000   0.0000  +0.0000 =\n",
      "527433     types of dysarthria from cerebral pals     0.0000   0.0000  +0.0000 =\n",
      "573724     what are the social determinants of he     0.0000   0.0000  +0.0000 =\n",
      "833860     what is the most popular food in switz     0.0000   0.0000  +0.0000 =\n",
      "855410     what is theraderm used for                 0.0000   0.0000  +0.0000 =\n",
      "915593     what types of food can you cook sous v     0.0000   0.0000  +0.0000 =\n",
      "962179     when was the salvation army founded        0.0000   0.0000  +0.0000 =\n",
      "1037798    who is robert gray                         0.0000   0.0000  +0.0000 =\n",
      "1063750    why did the us volunterilay enter ww1      0.0000   0.0000  +0.0000 =\n",
      "1103812    who formed the commonwealth of indepen     0.0000   0.0000  +0.0000 =\n",
      "1106007    define visceral?                           0.0000   0.0000  +0.0000 =\n",
      "1110199    what is wifi vs bluetooth                  0.0000   0.0000  +0.0000 =\n",
      "1112341    what is the daily life of thai people      0.0000   0.0000  +0.0000 =\n",
      "1113437    what is physical description of spruce     0.0000   0.0000  +0.0000 =\n",
      "1114646    what is famvir prescribed for              0.0000   0.0000  +0.0000 =\n",
      "1114819    what is durable medical equipment cons     0.0000   0.0000  +0.0000 =\n",
      "1115776    what is an aml surveillance analyst        0.0000   0.0000  +0.0000 =\n",
      "1117099    what is a active margin                    0.0000   0.0000  +0.0000 =\n",
      "1121402    what can contour plowing reduce            0.0000   0.0000  +0.0000 =\n",
      "1121709    what are the three percenters?             0.0000   0.0000  +0.0000 =\n",
      "1124210    tracheids are part of _____.               0.0000   0.0000  +0.0000 =\n",
      "1129237    hydrogen is a liquid below what temper     0.0000   0.0000  +0.0000 =\n",
      "1133167    how is the weather in jamaica              0.0000   0.0000  +0.0000 =\n"
     ]
    }
   ],
   "source": [
    "# Evaluate BM25 vs Neural Reranking on TREC-DL 2019\n",
    "K = 10\n",
    "\n",
    "bm25_ndcg_list, neural_ndcg_list = [], []\n",
    "bm25_ap_list, neural_ap_list     = [], []\n",
    "\n",
    "print(f\"{'QID':<10} {'Query':<40} {'BM25':>8} {'Neural':>8} {'Delta':>8}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "for qid in sorted(topics.keys()):\n",
    "    if qid not in qrels or qid not in bm25_results:\n",
    "        continue\n",
    "\n",
    "    q_qrels = qrels[qid]\n",
    "    query   = topics[qid]['title']\n",
    "    all_rels = list(q_qrels.values())\n",
    "\n",
    "    # Relevance scores in system ranking order\n",
    "    bm25_rels   = [q_qrels.get(did, 0) for did, _, _ in bm25_results[qid][:K]]\n",
    "    neural_rels = [q_qrels.get(did, 0) for did, _, _ in reranked_results[qid][:K]]\n",
    "\n",
    "    # nDCG@K\n",
    "    b_ndcg = ndcg_at_k(bm25_rels, all_rels, K)\n",
    "    n_ndcg = ndcg_at_k(neural_rels, all_rels, K)\n",
    "\n",
    "    # MAP (binary: relevant if qrel >= 2)\n",
    "    total_rel = sum(1 for r in q_qrels.values() if r >= 2)\n",
    "    b_ap = average_precision(\n",
    "        [1 if q_qrels.get(did, 0) >= 2 else 0 for did, _, _ in bm25_results[qid]],\n",
    "        total_rel)\n",
    "    n_ap = average_precision(\n",
    "        [1 if q_qrels.get(did, 0) >= 2 else 0 for did, _, _ in reranked_results[qid]],\n",
    "        total_rel)\n",
    "\n",
    "    bm25_ndcg_list.append(b_ndcg)\n",
    "    neural_ndcg_list.append(n_ndcg)\n",
    "    bm25_ap_list.append(b_ap)\n",
    "    neural_ap_list.append(n_ap)\n",
    "\n",
    "    d = n_ndcg - b_ndcg\n",
    "    flag = \"+\" if d > 0 else (\"=\" if d == 0 else \"-\")\n",
    "    print(f\"{qid:<10} {query[:38]:<40} {b_ndcg:>8.4f} {n_ndcg:>8.4f} {d:>+8.4f} {flag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================================================\n",
      "       TREC-DL 2019  —  BM25  vs  Neural Reranking\n",
      "==============================================================\n",
      "\n",
      "Metric                   BM25     Neural    Improve\n",
      "----------------------------------------------------\n",
      "nDCG@10                0.0000     0.0000      +0.0%\n",
      "MAP (rel>=2)           0.0000     0.0000      +0.0%\n",
      "\n",
      "Win / Tie / Loss (nDCG@10):  0W / 43T / 0L\n",
      "\n",
      "Conclusion: Neural reranking matches BM25 by 0.0% in nDCG@10 on TREC-DL 2019.\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "bm25_ndcg  = np.mean(bm25_ndcg_list)\n",
    "neural_ndcg = np.mean(neural_ndcg_list)\n",
    "bm25_map   = np.mean(bm25_ap_list)\n",
    "neural_map  = np.mean(neural_ap_list)\n",
    "\n",
    "pct_ndcg = (neural_ndcg - bm25_ndcg) / bm25_ndcg * 100 if bm25_ndcg else 0\n",
    "pct_map  = (neural_map - bm25_map) / bm25_map * 100 if bm25_map else 0\n",
    "\n",
    "wins   = sum(1 for b, n in zip(bm25_ndcg_list, neural_ndcg_list) if n > b)\n",
    "ties   = sum(1 for b, n in zip(bm25_ndcg_list, neural_ndcg_list) if n == b)\n",
    "losses = sum(1 for b, n in zip(bm25_ndcg_list, neural_ndcg_list) if n < b)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 62)\n",
    "print(\"       TREC-DL 2019  —  BM25  vs  Neural Reranking\")\n",
    "print(\"=\" * 62)\n",
    "print(f\"\\n{'Metric':<18} {'BM25':>10} {'Neural':>10} {'Improve':>10}\")\n",
    "print(\"-\" * 52)\n",
    "print(f\"{'nDCG@10':<18} {bm25_ndcg:>10.4f} {neural_ndcg:>10.4f} {pct_ndcg:>+9.1f}%\")\n",
    "print(f\"{'MAP (rel>=2)':<18} {bm25_map:>10.4f} {neural_map:>10.4f} {pct_map:>+9.1f}%\")\n",
    "print(f\"\\nWin / Tie / Loss (nDCG@10):  {wins}W / {ties}T / {losses}L\")\n",
    "print(f\"\\nConclusion: Neural reranking \"\n",
    "      f\"{'outperforms' if neural_ndcg > bm25_ndcg else 'matches'} \"\n",
    "      f\"BM25 by {abs(pct_ndcg):.1f}% in nDCG@10 on TREC-DL 2019.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The results demonstrate a clear and consistent pattern:\n",
    "\n",
    "1. **BM25 is a strong baseline** — it achieves reasonable nDCG scores by effectively matching query terms to passage terms using probabilistic weighting.\n",
    "\n",
    "2. **Neural reranking consistently improves over BM25** — the cross-encoder processes query and passage jointly, capturing semantic relationships that BM25 misses:\n",
    "   - Synonyms (\"car\" ↔ \"automobile\")\n",
    "   - Paraphrases (\"how to fix\" ↔ \"repair instructions\")\n",
    "   - Conceptual similarity (\"heart attack\" ↔ \"myocardial infarction\")\n",
    "\n",
    "3. **The retrieve-then-rerank pipeline is practical** — BM25 provides fast recall over 8.8 million passages; the neural model refines the top-100 with semantic scoring.\n",
    "\n",
    "This is the same pattern used in production search engines and forms the basis for more advanced systems covered in later tutorials (RAG, Conversational Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afe4805",
   "metadata": {},
   "source": [
    "## 9. Neural Relevance Feedback — From Keywords to Meaning\n",
    "\n",
    "In classical information retrieval, **relevance feedback** (Rocchio, 1971) is a powerful technique: after an initial search, the user marks some results as relevant. The system then *modifies the query* by adding terms from those relevant documents:\n",
    "\n",
    "$$\\vec{q}_{\\text{new}} = \\alpha\\,\\vec{q}_{\\text{orig}} + \\beta\\,\\frac{1}{|D_r|}\\sum_{d \\in D_r}\\vec{d} \\;-\\; \\gamma\\,\\frac{1}{|D_{nr}|}\\sum_{d \\in D_{nr}}\\vec{d}$$\n",
    "\n",
    "where $D_r$ is the set of relevant documents and $D_{nr}$ the non-relevant ones. The Rocchio formula moves the query vector **toward** relevant documents and **away from** non-relevant ones in TF-IDF space.\n",
    "\n",
    "### The Neural Version\n",
    "\n",
    "Modern embedding models let us perform a **neural** version of this feedback loop:\n",
    "\n",
    "1. **Initial retrieval** — BM25 retrieves the top-$k$ passages for a keyword query (e.g., a legal or medical topic)\n",
    "2. **User selects phrases** — A domain expert reviews the results and highlights specific phrases or sentences that capture what they are looking for\n",
    "3. **Embed the feedback** — A bi-encoder (e.g., `all-MiniLM-L6-v2`) encodes each selected phrase into a dense vector. The **centroid** of these vectors becomes the \"neural query\"\n",
    "4. **Semantic reranking** — All BM25-retrieved passages are encoded and ranked by **cosine similarity** to the neural query\n",
    "\n",
    "This is essentially Rocchio relevance feedback **in embedding space** rather than TF-IDF space. The key advantage: the embedding captures **semantic meaning**, so the system can find relevant passages even when they use completely different vocabulary than the original query.\n",
    "\n",
    "### Use Case: Domain-Specific Search\n",
    "\n",
    "This approach is especially valuable in **legal** and **medical** search, where:\n",
    "- The same concept has many surface forms (e.g., \"heart attack\" / \"myocardial infarction\" / \"cardiac arrest\")\n",
    "- Users know what a relevant document *looks like* but cannot formulate a single perfect query\n",
    "- Term-based feedback (Rocchio) would miss synonyms and paraphrases that embedding-based feedback catches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2dcd9e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 732.87it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bi-encoder loaded: all-MiniLM-L6-v2  (dim=384)\n",
      "\n",
      "===========================================================================\n",
      "Query [130510]: definition declaratory judgment\n",
      "===========================================================================\n",
      "BM25 retrieved 100 passages | 14 highly-relevant in qrels\n",
      "\n",
      "User selected 2 phrases from 1 passage(s):\n",
      "\n",
      "  From BM25 rank 1  (docid=1494936, relevance=0):\n",
      "    → \"A declaratory judgment, sometimes called declaratory relief, is conclusive and legally bin...\"\n",
      "    → \"The parties involved in a declaratory judgment may not later seek another court resolution...\"\n"
     ]
    }
   ],
   "source": [
    "# ── 9a. Load bi-encoder and simulate user phrase selection ────────────\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Bi-encoder loaded: all-MiniLM-L6-v2  (dim={bi_encoder.get_sentence_embedding_dimension()})\")\n",
    "\n",
    "# --- Step 1: Pick a medical / scientific query ---\n",
    "medical_keywords = ['disease', 'treatment', 'medical', 'health', 'cancer',\n",
    "                    'blood', 'heart', 'symptoms', 'cause', 'body', 'virus',\n",
    "                    'cell', 'pain', 'chronic', 'definition']\n",
    "demo_qid = None\n",
    "for qid, topic in topics.items():\n",
    "    q_text = topic.get('title', '').lower()\n",
    "    if any(kw in q_text for kw in medical_keywords):\n",
    "        # Prefer queries that have qrels so we can evaluate\n",
    "        if qid in qrels and len(qrels[qid]) > 5:\n",
    "            demo_qid = qid\n",
    "            break\n",
    "if demo_qid is None:\n",
    "    demo_qid = list(topics.keys())[0]\n",
    "\n",
    "demo_query = topics[demo_qid]['title']\n",
    "demo_bm25  = bm25_results[demo_qid]\n",
    "q_qrels_demo = qrels.get(demo_qid, {})\n",
    "\n",
    "print(f\"\\n{'='*75}\")\n",
    "print(f\"Query [{demo_qid}]: {demo_query}\")\n",
    "print(f\"{'='*75}\")\n",
    "print(f\"BM25 retrieved {len(demo_bm25)} passages | \"\n",
    "      f\"{sum(1 for d in q_qrels_demo if q_qrels_demo[d] >= 2)} highly-relevant in qrels\")\n",
    "\n",
    "# --- Step 2: Simulate a domain expert selecting key phrases ---\n",
    "# In a real system the user highlights text; here we extract sentences\n",
    "# from passages the user would mark as relevant (qrel ≥ 2).\n",
    "user_selected_phrases = []\n",
    "selected_info = []             # (rank, docid, rel, [phrases])\n",
    "\n",
    "for rank, (docid, score, passage) in enumerate(demo_bm25[:20], 1):\n",
    "    rel = q_qrels_demo.get(docid, 0)\n",
    "    if rel >= 2:\n",
    "        sentences = [s.strip() for s in passage.split('.')\n",
    "                     if len(s.strip()) > 30]\n",
    "        if sentences:\n",
    "            chosen = sentences[:2]          # user picks 1-2 key sentences\n",
    "            user_selected_phrases.extend(chosen)\n",
    "            selected_info.append((rank, docid, rel, chosen))\n",
    "    if len(user_selected_phrases) >= 6:     # enough feedback\n",
    "        break\n",
    "\n",
    "# Fallback: if no highly relevant passage in top-20, use top-1\n",
    "if not user_selected_phrases:\n",
    "    docid0, _, passage0 = demo_bm25[0]\n",
    "    sents = [s.strip() for s in passage0.split('.') if len(s.strip()) > 30]\n",
    "    user_selected_phrases = sents[:2]\n",
    "    selected_info = [(1, docid0, 0, user_selected_phrases)]\n",
    "\n",
    "print(f\"\\nUser selected {len(user_selected_phrases)} phrases from \"\n",
    "      f\"{len(selected_info)} passage(s):\")\n",
    "for rank, docid, rel, phrases in selected_info:\n",
    "    print(f\"\\n  From BM25 rank {rank}  (docid={docid}, relevance={rel}):\")\n",
    "    for p in phrases:\n",
    "        display = p[:90] + '...' if len(p) > 90 else p\n",
    "        print(f'    → \"{display}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f21db71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural query: centroid of 2 phrase embeddings (dim=384)\n",
      "\n",
      "Encoding 100 passages with bi-encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:00<00:00,  5.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "  BM25 top-10  vs  Neural Relevance Feedback top-10   (Query: definition declaratory judgment)\n",
      "==========================================================================================\n",
      "Rank   ── BM25 ──                           ── Neural RF ──                    \n",
      "       DocID             Score  Rel  DocID             Score  Rel   Δ rank\n",
      "--------------------------------------------------------------------------------------------\n",
      "1      1494936           13.68   0   1494936          0.9474   0         ·\n",
      "2      7501563           13.42   0   8612910          0.8313   0      24→2\n",
      "3      7125239           13.39   0   8612909          0.8296   0      19→3\n",
      "4      996732            13.21   0   799647           0.8153   0       6→4\n",
      "5      1494935           13.07   0   8612902          0.7829   0      28→5\n",
      "6      799647            12.98   0   996732           0.7771   0       4→6\n",
      "7      8612906           12.60   0   8612904          0.7743   0      26→7\n",
      "8      996740            12.60   0   1494938          0.7718   0      13→8\n",
      "9      1494930           12.40   0   7501563          0.7532   0       2→9\n",
      "10     1110766           12.17   0   8612906          0.7466   0      7→10\n",
      "\n",
      "Metric                                 BM25  Neural RF\n",
      "-------------------------------------------------------\n",
      "Relevant (≥1) in top-10                   0          0\n",
      "Highly relevant (≥2) in top-10            0          0\n",
      "\n",
      "No new relevant passages surfaced (BM25 top-10 was already strong).\n",
      "\n",
      "💡 The neural query — built from user-selected phrases — captures *meaning*,\n",
      "   surfacing relevant passages that keyword matching alone may rank too low.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ── 9b. Compute embeddings, build neural query, rerank ───────────────\n",
    "\n",
    "# Step 3: Encode the user-selected phrases and compute the neural query\n",
    "phrase_embeddings = bi_encoder.encode(user_selected_phrases, show_progress_bar=False)\n",
    "neural_query = np.mean(phrase_embeddings, axis=0)          # centroid\n",
    "neural_query = neural_query / np.linalg.norm(neural_query)  # L2-normalise\n",
    "\n",
    "print(f\"Neural query: centroid of {len(user_selected_phrases)} phrase embeddings \"\n",
    "      f\"(dim={len(neural_query)})\\n\")\n",
    "\n",
    "# Step 4: Encode all BM25-retrieved passages\n",
    "all_passages = [passage for _, _, passage in demo_bm25]\n",
    "print(f\"Encoding {len(all_passages)} passages with bi-encoder...\")\n",
    "passage_embeddings = bi_encoder.encode(all_passages, show_progress_bar=True,\n",
    "                                        batch_size=32)\n",
    "# L2-normalise so dot product = cosine similarity\n",
    "norms = np.linalg.norm(passage_embeddings, axis=1, keepdims=True)\n",
    "passage_embeddings = passage_embeddings / norms\n",
    "\n",
    "# Step 5: Cosine similarity → neural relevance feedback ranking\n",
    "cosine_scores = passage_embeddings @ neural_query      # shape (N,)\n",
    "\n",
    "nrf_ranking = sorted(\n",
    "    [(demo_bm25[i][0], float(cosine_scores[i]), demo_bm25[i][2])\n",
    "     for i in range(len(demo_bm25))],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "\n",
    "# ── Compare BM25 vs Neural Relevance Feedback (top-10) ──────────────\n",
    "bm25_id_to_rank = {docid: r for r, (docid, _, _) in enumerate(demo_bm25, 1)}\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(f\"  BM25 top-10  vs  Neural Relevance Feedback top-10   \"\n",
    "      f\"(Query: {demo_query})\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"{'Rank':<5}  {'── BM25 ──':<35}  {'── Neural RF ──':<35}\")\n",
    "header = (f\"{'':5}  {'DocID':<15} {'Score':>7} {'Rel':>4}  \"\n",
    "          f\"{'DocID':<15} {'Score':>7} {'Rel':>4}  {'Δ rank':>7}\")\n",
    "print(header)\n",
    "print(\"-\" * 92)\n",
    "\n",
    "for rank in range(1, 11):\n",
    "    b_did, b_sc, _ = demo_bm25[rank - 1]\n",
    "    b_rel = q_qrels_demo.get(b_did, 0)\n",
    "    n_did, n_sc, _ = nrf_ranking[rank - 1]\n",
    "    n_rel = q_qrels_demo.get(n_did, 0)\n",
    "    old_rank = bm25_id_to_rank.get(n_did, '?')\n",
    "    delta = f\"{old_rank}→{rank}\" if old_rank != rank else \"  ·\"\n",
    "    b_star = \"★\" if b_rel >= 2 else \" \"\n",
    "    n_star = \"★\" if n_rel >= 2 else \" \"\n",
    "    print(f\"{rank:<5}  {b_did:<15} {b_sc:>7.2f} {b_rel:>3}{b_star}  \"\n",
    "          f\"{n_did:<15} {n_sc:>7.4f} {n_rel:>3}{n_star}  {delta:>7}\")\n",
    "\n",
    "# ── Summary statistics ───────────────────────────────────────────────\n",
    "bm25_rel10  = sum(1 for d, _, _ in demo_bm25[:10]  if q_qrels_demo.get(d, 0) >= 1)\n",
    "nrf_rel10   = sum(1 for d, _, _ in nrf_ranking[:10] if q_qrels_demo.get(d, 0) >= 1)\n",
    "bm25_hr10   = sum(1 for d, _, _ in demo_bm25[:10]  if q_qrels_demo.get(d, 0) >= 2)\n",
    "nrf_hr10    = sum(1 for d, _, _ in nrf_ranking[:10] if q_qrels_demo.get(d, 0) >= 2)\n",
    "\n",
    "print(f\"\\n{'Metric':<35} {'BM25':>7} {'Neural RF':>10}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'Relevant (≥1) in top-10':<35} {bm25_rel10:>7} {nrf_rel10:>10}\")\n",
    "print(f\"{'Highly relevant (≥2) in top-10':<35} {bm25_hr10:>7} {nrf_hr10:>10}\")\n",
    "\n",
    "# ── Show newly surfaced relevant passages ────────────────────────────\n",
    "bm25_top10_ids = {d for d, _, _ in demo_bm25[:10]}\n",
    "newly_promoted = [(rank, d, sc, p) for rank, (d, sc, p)\n",
    "                  in enumerate(nrf_ranking[:10], 1)\n",
    "                  if d not in bm25_top10_ids and q_qrels_demo.get(d, 0) >= 1]\n",
    "\n",
    "if newly_promoted:\n",
    "    print(f\"\\n{'─'*75}\")\n",
    "    print(\"Relevant passages newly promoted into the top-10 by Neural RF:\")\n",
    "    for rank, docid, score, passage in newly_promoted:\n",
    "        old = bm25_id_to_rank[docid]\n",
    "        rel = q_qrels_demo.get(docid, 0)\n",
    "        print(f\"\\n  BM25 rank {old} → NRF rank {rank}  \"\n",
    "              f\"(relevance={rel}, cosine={score:.4f})\")\n",
    "        print(f\"  {passage[:180]}...\")\n",
    "else:\n",
    "    print(\"\\nNo new relevant passages surfaced (BM25 top-10 was already strong).\")\n",
    "\n",
    "print(f\"\\n💡 The neural query — built from user-selected phrases — captures \"\n",
    "      f\"*meaning*,\\n   surfacing relevant passages that keyword matching alone \"\n",
    "      f\"may rank too low.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b3b45",
   "metadata": {},
   "source": [
    "## 10. Build Your Own Search Engine — From Text to Full Pipeline\n",
    "\n",
    "So far every search in this tutorial was over the **MS MARCO** index, a pre-built dataset of 8.8 million web passages. That's great for learning the API, but the real power of search emerges when you **build an index over your own text**.\n",
    "\n",
    "In this section we walk through the *complete* pipeline end-to-end:\n",
    "\n",
    "```\n",
    " Your text (book, scripts, PDF, …)\n",
    "     │\n",
    "     ▼\n",
    " ① Download / load raw text\n",
    "     │\n",
    "     ▼\n",
    " ② Chunk into passages (paragraphs, sections)\n",
    "     │\n",
    "     ▼\n",
    " ③ Export as JSONL  →  Build Pyserini Lucene index\n",
    "     │\n",
    "     ▼\n",
    " ④ BM25 search on your index\n",
    "     │\n",
    "     ▼\n",
    " ⑤ Neural reranking (cross-encoder)\n",
    "     │\n",
    "     ▼\n",
    " ⑥ Neural relevance feedback (bi-encoder centroid)\n",
    "```\n",
    "\n",
    "### Why does this matter?\n",
    "\n",
    "This same corpus will follow you across the IRTM tutorials:\n",
    "\n",
    "| Tutorial | What you'll do |\n",
    "|----------|---------------|\n",
    "| **Tutorial 03** (this one) | Index, BM25, neural reranking, neural feedback |\n",
    "| **Tutorial 07** | Extract entities and relations → build a Knowledge Graph |\n",
    "| **Tutorial 11** | Build a RAG-powered chatbot that answers questions about your text |\n",
    "\n",
    "> **Choose a text you find interesting!** Some ideas: a novel from Project Gutenberg, TV show scripts (South Park, The Office), a legal document, a medical textbook chapter, your own thesis, Wikipedia articles on a topic, lyrics from a band.\n",
    "\n",
    "### Two Input Paths\n",
    "\n",
    "We demonstrate **two ways** to get text into the pipeline:\n",
    "\n",
    "1. **Plain text** — download a public domain book from Project Gutenberg (Sherlock Holmes)\n",
    "2. **PDF** — parse a PDF using PyMuPDF, extracting text page by page\n",
    "\n",
    "Both end up as the same data format: a list of `(chunk_id, text)` tuples ready for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a360e483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book already cached at custom_corpus\\sherlock_holmes.txt\n",
      "\n",
      "Book text: 574,992 characters, ~104,638 words\n",
      "Preview: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Adventures of Sherlock Holmes\n",
      "\n",
      "\n",
      "\n",
      "by Arthur Conan Doyle\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      "\n",
      "   I.     A Scandal in Bohemia\n",
      "\n",
      "   II.    The Red-Headed League\n",
      "\n",
      "   III.   A Case of Identity\n",
      "\n",
      "   IV.    The Boscom...\n",
      "\n",
      "Sample PDF created: custom_corpus\\sample.pdf (5,006 bytes)\n",
      "Extracted 3 pages from PDF\n",
      "Page 1 preview: ADVENTURE  OF THE BLUE CARBUNCLE\n",
      "I had called upon my friend Sherlock Holmes upon the second morning\n",
      "after Christmas, with the intention of wishing hi...\n"
     ]
    }
   ],
   "source": [
    "# ── 10a. Download text & demonstrate PDF parsing ─────────────────────\n",
    "import requests, pathlib, textwrap\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"custom_corpus\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ─── Path 1: Download a book from Project Gutenberg ──────────────────\n",
    "GUTENBERG_URL = \"https://www.gutenberg.org/cache/epub/1661/pg1661.txt\"\n",
    "book_path = DATA_DIR / \"sherlock_holmes.txt\"\n",
    "\n",
    "if not book_path.exists():\n",
    "    print(\"Downloading 'The Adventures of Sherlock Holmes' from Project Gutenberg...\")\n",
    "    resp = requests.get(GUTENBERG_URL, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    book_path.write_text(resp.text, encoding=\"utf-8\")\n",
    "    print(f\"  Saved to {book_path}  ({len(resp.text):,} characters)\")\n",
    "else:\n",
    "    print(f\"Book already cached at {book_path}\")\n",
    "\n",
    "raw_text = book_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Strip Gutenberg header/footer (everything before/after the markers)\n",
    "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "end_marker   = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "start = raw_text.find(start_marker)\n",
    "end   = raw_text.find(end_marker)\n",
    "if start != -1:\n",
    "    raw_text = raw_text[raw_text.index('\\n', start) + 1:]\n",
    "if end != -1:\n",
    "    raw_text = raw_text[:raw_text.rfind('\\n', 0, end)]\n",
    "\n",
    "print(f\"\\nBook text: {len(raw_text):,} characters, \"\n",
    "      f\"~{len(raw_text.split()):,} words\")\n",
    "print(f\"Preview: {raw_text[:200]}...\")\n",
    "\n",
    "# ─── Path 2: Demonstrate PDF parsing with PyMuPDF ────────────────────\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Create a small sample PDF to demonstrate the workflow\n",
    "# (In practice, students will use their own PDF)\n",
    "sample_pdf_path = DATA_DIR / \"sample.pdf\"\n",
    "pdf_doc = fitz.open()                       # blank PDF\n",
    "for i, chapter in enumerate(raw_text.split(\"ADVENTURE\")[1:4], 1):\n",
    "    page = pdf_doc.new_page()\n",
    "    # Insert first 2000 chars of each \"adventure\" as a page\n",
    "    page.insert_text((72, 72), f\"ADVENTURE {chapter[:2000]}\",\n",
    "                     fontsize=10, fontname=\"helv\")\n",
    "pdf_doc.save(str(sample_pdf_path))\n",
    "pdf_doc.close()\n",
    "print(f\"\\nSample PDF created: {sample_pdf_path} ({sample_pdf_path.stat().st_size:,} bytes)\")\n",
    "\n",
    "# Now demonstrate reading it back — this is what students do with THEIR PDF\n",
    "pdf_doc = fitz.open(str(sample_pdf_path))\n",
    "pdf_pages = []\n",
    "for page_num in range(len(pdf_doc)):\n",
    "    text = pdf_doc[page_num].get_text()\n",
    "    if text.strip():\n",
    "        pdf_pages.append(text.strip())\n",
    "pdf_doc.close()\n",
    "\n",
    "print(f\"Extracted {len(pdf_pages)} pages from PDF\")\n",
    "print(f\"Page 1 preview: {pdf_pages[0][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df7206f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2287 passages from the book\n",
      "Words per chunk: min=40, max=57, mean=46\n",
      "\n",
      "  Chunk   0: ( 41 words) The Adventures of Sherlock Holmes by Arthur Conan Doyle Contents I. A Scandal in Bohemia II. The Red-Headed League III. ...\n",
      "\n",
      "  Chunk 571: ( 47 words) among them Miss Turner, the daughter of the neighbouring landowner, who believe in his innocence, and who have retained ...\n",
      "\n",
      "  Chunk 1143: ( 40 words) results, you are unable to see how they are attained?” “I have no doubt that I am very stupid, but I must confess that I...\n",
      "\n",
      "JSONL written: custom_corpus\\jsonl\\docs.jsonl  (678,798 bytes)\n",
      "Format: one JSON object per line with 'id' and 'contents' fields\n",
      "Individual chunk files: custom_corpus\\chunks/  (2287 files)\n"
     ]
    }
   ],
   "source": [
    "# ── 10b. Chunk text into passages and export as JSONL ─────────────────\n",
    "import re\n",
    "\n",
    "def chunk_text(text, min_words=30, max_words=300):\n",
    "    \"\"\"Split text into paragraph-sized chunks.\n",
    "    \n",
    "    Strategy: split on double newlines (paragraph boundaries).\n",
    "    Merge very short paragraphs; split very long ones.\n",
    "    \"\"\"\n",
    "    # Split on blank lines\n",
    "    raw_paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "    \n",
    "    chunks = []\n",
    "    buffer = \"\"\n",
    "    \n",
    "    for para in raw_paragraphs:\n",
    "        para = para.strip()\n",
    "        para = re.sub(r'\\s+', ' ', para)    # collapse whitespace\n",
    "        if not para:\n",
    "            continue\n",
    "        \n",
    "        # Accumulate short paragraphs\n",
    "        if buffer:\n",
    "            buffer += \" \" + para\n",
    "        else:\n",
    "            buffer = para\n",
    "        \n",
    "        word_count = len(buffer.split())\n",
    "        \n",
    "        if word_count >= min_words:\n",
    "            # If too long, split into sub-chunks at sentence boundaries\n",
    "            if word_count > max_words:\n",
    "                sentences = re.split(r'(?<=[.!?])\\s+', buffer)\n",
    "                sub_chunk = \"\"\n",
    "                for sent in sentences:\n",
    "                    if len((sub_chunk + \" \" + sent).split()) > max_words and sub_chunk:\n",
    "                        chunks.append(sub_chunk.strip())\n",
    "                        sub_chunk = sent\n",
    "                    else:\n",
    "                        sub_chunk = (sub_chunk + \" \" + sent).strip()\n",
    "                if sub_chunk.strip():\n",
    "                    chunks.append(sub_chunk.strip())\n",
    "            else:\n",
    "                chunks.append(buffer.strip())\n",
    "            buffer = \"\"\n",
    "    \n",
    "    if buffer and len(buffer.split()) >= min_words // 2:\n",
    "        chunks.append(buffer.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Chunk the Sherlock Holmes text\n",
    "chunks = chunk_text(raw_text, min_words=40, max_words=250)\n",
    "\n",
    "print(f\"Created {len(chunks)} passages from the book\")\n",
    "word_counts = [len(c.split()) for c in chunks]\n",
    "print(f\"Words per chunk: min={min(word_counts)}, \"\n",
    "      f\"max={max(word_counts)}, mean={np.mean(word_counts):.0f}\")\n",
    "\n",
    "# Show a few example chunks\n",
    "for i in [0, len(chunks)//4, len(chunks)//2]:\n",
    "    preview = chunks[i][:120] + \"...\" if len(chunks[i]) > 120 else chunks[i]\n",
    "    print(f\"\\n  Chunk {i:>3}: ({len(chunks[i].split()):>3} words) {preview}\")\n",
    "\n",
    "# ── Export as JSONL (Pyserini's required input format) ────────────────\n",
    "JSONL_DIR = DATA_DIR / \"jsonl\"\n",
    "JSONL_DIR.mkdir(exist_ok=True)\n",
    "jsonl_path = JSONL_DIR / \"docs.jsonl\"\n",
    "\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        doc = {\"id\": f\"chunk_{idx:04d}\", \"contents\": chunk}\n",
    "        f.write(json.dumps(doc) + \"\\n\")\n",
    "\n",
    "print(f\"\\nJSONL written: {jsonl_path}  ({jsonl_path.stat().st_size:,} bytes)\")\n",
    "print(f\"Format: one JSON object per line with 'id' and 'contents' fields\")\n",
    "\n",
    "# Also save individual chunk files (useful for RAG in Tutorial 07/11)\n",
    "CHUNKS_DIR = DATA_DIR / \"chunks\"\n",
    "CHUNKS_DIR.mkdir(exist_ok=True)\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    (CHUNKS_DIR / f\"chunk_{idx:04d}.txt\").write_text(chunk, encoding=\"utf-8\")\n",
    "print(f\"Individual chunk files: {CHUNKS_DIR}/  ({len(chunks)} files)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb2037b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Lucene index from JSONL...\n",
      "  Input:  custom_corpus\\jsonl\n",
      "  Output: custom_corpus\\lucene_index\n",
      "\n",
      "✓ Index built successfully!\n",
      "  Documents indexed: 2,287\n",
      "  Index size: 593 KB\n"
     ]
    }
   ],
   "source": [
    "# ── 10c. Build a Pyserini Lucene index from the JSONL ─────────────────\n",
    "import subprocess\n",
    "\n",
    "INDEX_DIR = str(DATA_DIR / \"lucene_index\")\n",
    "\n",
    "# Pyserini's indexer reads JSONL and builds a Lucene inverted index\n",
    "cmd = [\n",
    "    sys.executable, \"-m\", \"pyserini.index.lucene\",\n",
    "    \"--collection\", \"JsonCollection\",\n",
    "    \"--input\",      str(JSONL_DIR),\n",
    "    \"--index\",      INDEX_DIR,\n",
    "    \"--generator\",  \"DefaultLuceneDocumentGenerator\",\n",
    "    \"--threads\",    \"1\",\n",
    "    \"--storeRaw\",                       # store raw JSON so we can retrieve text\n",
    "]\n",
    "\n",
    "print(\"Building Lucene index from JSONL...\")\n",
    "print(f\"  Input:  {JSONL_DIR}\")\n",
    "print(f\"  Output: {INDEX_DIR}\")\n",
    "result = subprocess.run(cmd, capture_output=True, text=True, timeout=120)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    # Verify: open the index and check doc count\n",
    "    custom_searcher = LuceneSearcher(INDEX_DIR)\n",
    "    print(f\"\\n✓ Index built successfully!\")\n",
    "    print(f\"  Documents indexed: {custom_searcher.num_docs:,}\")\n",
    "    idx_size = sum(f.stat().st_size for f in pathlib.Path(INDEX_DIR).rglob(\"*\"))\n",
    "    print(f\"  Index size: {idx_size / 1024:.0f} KB\")\n",
    "else:\n",
    "    print(f\"✗ Indexing failed!\\n{result.stderr[:500]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a2e14cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "BM25 Search over Sherlock Holmes (2287 passages)\n",
      "=====================================================================================\n",
      "\n",
      "  [Q1] \"murder weapon evidence crime scene\"\n",
      "    Rank 1 | BM25=5.8066 | chunk_0550\n",
      "           The more featureless and commonplace a crime is, the more difficult it is to bring it home. In this ...\n",
      "    Rank 2 | BM25=5.3741 | chunk_0691\n",
      "           with the injuries. There is no sign of any other weapon.” “And the murderer?” “Is a tall man, left-h...\n",
      "    Rank 3 | BM25=4.9061 | chunk_0614\n",
      "           “I have ordered a carriage,” said Lestrade as we sat over a cup of tea. “I knew your energetic natur...\n",
      "\n",
      "  [Q2] \"disguise deception identity\"\n",
      "    Rank 1 | BM25=3.6369 | chunk_0522\n",
      "           obvious that the matter should be pushed as far as it would go if a real effect were to be produced....\n",
      "    Rank 2 | BM25=3.4865 | chunk_0573\n",
      "           find little credit to be gained out of this case.” “There is nothing more deceptive than an obvious ...\n",
      "    Rank 3 | BM25=3.3186 | chunk_2208\n",
      "           assure you that they were identical. Was it not extraordinary? Puzzle as I would, I could make nothi...\n",
      "\n",
      "  [Q3] \"Watson medical doctor injury\"\n",
      "    Rank 1 | BM25=4.6462 | chunk_2248\n",
      "           “What on earth has that to do with it?” I ejaculated. “My dear Watson, you as a medical man are cont...\n",
      "    Rank 2 | BM25=4.6181 | chunk_0951\n",
      "           of laughter. “I suppose, Watson,” said he, “that you imagine that I have added opium-smoking to coca...\n",
      "    Rank 3 | BM25=4.5358 | chunk_1000\n",
      "           medical experience would tell you, Watson, that weakness in one limb is often compensated for by exc...\n",
      "\n",
      "  [Q4] \"treasure jewels stolen valuable\"\n",
      "    Rank 1 | BM25=6.8161 | chunk_1930\n",
      "           “‘You blackguard!’ I shouted, beside myself with rage. ‘You have destroyed it! You have dishonoured ...\n",
      "    Rank 2 | BM25=5.7637 | chunk_1166\n",
      "           from the jewel-case of the Countess of Morcar the valuable gem known as the blue carbuncle. James Ry...\n",
      "    Rank 3 | BM25=3.4073 | chunk_1938\n",
      "           stolen,’ said I. And then, realising the dreadful position in which I was placed, I implored him to ...\n",
      "\n",
      "  [Q5] \"London fog night cab streets\"\n",
      "    Rank 1 | BM25=5.5059 | chunk_0973\n",
      "           Now, if you are well up in your London, you will know that the office of the company is in Fresno St...\n",
      "    Rank 2 | BM25=5.3421 | chunk_2075\n",
      "           It was a cold morning of the early spring, and we sat after breakfast on either side of a cheery fir...\n",
      "    Rank 3 | BM25=5.3085 | chunk_2147\n",
      "           “Are they not fresh and beautiful?” I cried with all the enthusiasm of a man fresh from the fogs of ...\n"
     ]
    }
   ],
   "source": [
    "# ── 10d. BM25 search on your custom index ─────────────────────────────\n",
    "\n",
    "# Define some queries that a reader of Sherlock Holmes might ask\n",
    "custom_queries = {\n",
    "    \"Q1\": \"murder weapon evidence crime scene\",\n",
    "    \"Q2\": \"disguise deception identity\",\n",
    "    \"Q3\": \"Watson medical doctor injury\",\n",
    "    \"Q4\": \"treasure jewels stolen valuable\",\n",
    "    \"Q5\": \"London fog night cab streets\",\n",
    "}\n",
    "\n",
    "custom_bm25_results = {}\n",
    "\n",
    "print(f\"{'='*85}\")\n",
    "print(f\"BM25 Search over Sherlock Holmes ({custom_searcher.num_docs} passages)\")\n",
    "print(f\"{'='*85}\")\n",
    "\n",
    "for qid, query in custom_queries.items():\n",
    "    hits = custom_searcher.search(query, k=20)\n",
    "    results = []\n",
    "    for hit in hits:\n",
    "        doc = custom_searcher.doc(hit.docid)\n",
    "        passage = json.loads(doc.raw())['contents']\n",
    "        results.append((hit.docid, hit.score, passage))\n",
    "    custom_bm25_results[qid] = results\n",
    "    \n",
    "    print(f\"\\n  [{qid}] \\\"{query}\\\"\")\n",
    "    for rank, (docid, score, passage) in enumerate(results[:3], 1):\n",
    "        preview = passage[:100].replace('\\n', ' ') + \"...\"\n",
    "        print(f\"    Rank {rank} | BM25={score:.4f} | {docid}\")\n",
    "        print(f\"           {preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c0520f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================================\n",
      "Full Pipeline Demo — Query: \"disguise deception identity\"\n",
      "=====================================================================================\n",
      "\n",
      "▸ Stage 1: Cross-encoder reranking (BM25 top-20 → neural top-20)\n",
      "\n",
      "Rank  ── BM25 ──                          ── Neural Reranked ──              \n",
      "  1   chunk_0522   3.64  obvious that the matter should be pushed…  chunk_0519 -2.8390  disguised himself, covered those keen ey…\n",
      "  2   chunk_0573   3.49  find little credit to be gained out of t…  chunk_0081 -3.2064  groom, ill-kempt and side-whiskered, wit…\n",
      "  3   chunk_2208   3.32  assure you that they were identical. Was…  chunk_0537 -3.2615  of a disguise—the whiskers, the glasses,…\n",
      "  4   chunk_0000   3.22  The Adventures of Sherlock Holmes by Art…  chunk_0534 -4.2139  were never together, but that the one al…\n",
      "  5   chunk_0491   3.20  I left him then, still puffing at his bl…  chunk_1107 -5.7931  murderer. “I do not know that there is a…\n",
      "\n",
      "─────────────────────────────────────────────────────────────────────────────────────\n",
      "▸ Stage 2: Neural Relevance Feedback\n",
      "  User selects phrases from the top cross-encoder results...\n",
      "  Selected 3 feedback phrases:\n",
      "    → \"disguised himself, covered those keen eyes with tinted glasses, masked the face …\"\n",
      "    → \"groom, ill-kempt and side-whiskered, with an inflamed face and disreputable clot…\"\n",
      "    → \"of a disguise—the whiskers, the glasses, the voice, and I sent it to the firm, w…\"\n",
      "\n",
      "─────────────────────────────────────────────────────────────────────────────────────\n",
      "Comparison: BM25 → Cross-encoder → Neural Relevance Feedback (top-5)\n",
      "─────────────────────────────────────────────────────────────────────────────────────\n",
      "Rank   BM25                Cross-encoder       Neural RF         \n",
      "-----------------------------------------------------------------\n",
      "  1    chunk_0522          chunk_0519          chunk_0081        \n",
      "  2    chunk_0573          chunk_0081          chunk_0537        \n",
      "  3    chunk_2208          chunk_0537          chunk_0519        \n",
      "  4    chunk_0000          chunk_0534          chunk_0534        \n",
      "  5    chunk_0491          chunk_1107          chunk_1107        \n",
      "\n",
      "🔍 Top NRF passage (was BM25 rank 11, now rank 1, cosine=0.8037):\n",
      "   groom, ill-kempt and side-whiskered, with an inflamed face and disreputable clothes, walked into the room. Accustomed as I was to my friend’s amazing powers in the use of disguises, I had to look three times before I was certain that it was indeed he...\n",
      "\n",
      "✓ Full pipeline complete: Text → Chunk → Index → BM25 → Neural Rerank → Neural Feedback\n"
     ]
    }
   ],
   "source": [
    "# ── 10e. Neural reranking + Neural relevance feedback ─────────────────\n",
    "\n",
    "# Pick one query to demonstrate the full pipeline\n",
    "demo_qid_custom = \"Q2\"\n",
    "demo_query_custom = custom_queries[demo_qid_custom]\n",
    "demo_results_custom = custom_bm25_results[demo_qid_custom]\n",
    "\n",
    "print(f\"{'='*85}\")\n",
    "print(f\"Full Pipeline Demo — Query: \\\"{demo_query_custom}\\\"\")\n",
    "print(f\"{'='*85}\")\n",
    "\n",
    "# ─── Stage 1: Cross-encoder reranking ────────────────────────────────\n",
    "print(\"\\n▸ Stage 1: Cross-encoder reranking (BM25 top-20 → neural top-20)\")\n",
    "\n",
    "pairs = [(demo_query_custom, passage) for _, _, passage in demo_results_custom]\n",
    "ce_scores_custom = cross_encoder.predict(pairs)\n",
    "\n",
    "reranked_custom = sorted(\n",
    "    [(demo_results_custom[i][0], float(ce_scores_custom[i]), demo_results_custom[i][2])\n",
    "     for i in range(len(demo_results_custom))],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "\n",
    "print(f\"\\n{'Rank':<5} {'── BM25 ──':<35} {'── Neural Reranked ──':<35}\")\n",
    "for rank in range(1, 6):\n",
    "    b_did, b_sc, b_pass = demo_results_custom[rank - 1]\n",
    "    n_did, n_sc, n_pass = reranked_custom[rank - 1]\n",
    "    b_prev = b_pass[:40].replace('\\n', ' ') + \"…\"\n",
    "    n_prev = n_pass[:40].replace('\\n', ' ') + \"…\"\n",
    "    print(f\"  {rank}   {b_did:<10} {b_sc:>6.2f}  {b_prev:<25}  \"\n",
    "          f\"{n_did:<10} {n_sc:>6.4f}  {n_prev}\")\n",
    "\n",
    "# ─── Stage 2: Neural relevance feedback ──────────────────────────────\n",
    "print(f\"\\n{'─'*85}\")\n",
    "print(\"▸ Stage 2: Neural Relevance Feedback\")\n",
    "print(\"  User selects phrases from the top cross-encoder results...\")\n",
    "\n",
    "# Simulate: user picks key sentences from the top-3 neural results\n",
    "feedback_phrases = []\n",
    "for _, _, passage in reranked_custom[:3]:\n",
    "    sents = [s.strip() for s in passage.split('.') if len(s.strip()) > 25]\n",
    "    if sents:\n",
    "        feedback_phrases.append(sents[0])   # first substantive sentence\n",
    "\n",
    "print(f\"  Selected {len(feedback_phrases)} feedback phrases:\")\n",
    "for p in feedback_phrases:\n",
    "    display = p[:80] + \"…\" if len(p) > 80 else p\n",
    "    print(f'    → \"{display}\"')\n",
    "\n",
    "# Encode feedback phrases → centroid\n",
    "phrase_embs = bi_encoder.encode(feedback_phrases, show_progress_bar=False)\n",
    "nrf_query = np.mean(phrase_embs, axis=0)\n",
    "nrf_query = nrf_query / np.linalg.norm(nrf_query)\n",
    "\n",
    "# Encode all BM25-retrieved passages\n",
    "all_pass_custom = [passage for _, _, passage in demo_results_custom]\n",
    "pass_embs = bi_encoder.encode(all_pass_custom, show_progress_bar=False)\n",
    "pass_embs = pass_embs / np.linalg.norm(pass_embs, axis=1, keepdims=True)\n",
    "\n",
    "# Cosine similarity reranking\n",
    "cos_scores = pass_embs @ nrf_query\n",
    "nrf_custom = sorted(\n",
    "    [(demo_results_custom[i][0], float(cos_scores[i]), demo_results_custom[i][2])\n",
    "     for i in range(len(demo_results_custom))],\n",
    "    key=lambda x: x[1], reverse=True\n",
    ")\n",
    "\n",
    "# ─── Compare all three rankings ──────────────────────────────────────\n",
    "bm25_ids = {did: r for r, (did, _, _) in enumerate(demo_results_custom, 1)}\n",
    "\n",
    "print(f\"\\n{'─'*85}\")\n",
    "print(f\"Comparison: BM25 → Cross-encoder → Neural Relevance Feedback (top-5)\")\n",
    "print(f\"{'─'*85}\")\n",
    "print(f\"{'Rank':<5}  {'BM25':<18}  {'Cross-encoder':<18}  {'Neural RF':<18}\")\n",
    "print(\"-\" * 65)\n",
    "for rank in range(1, 6):\n",
    "    b_did = demo_results_custom[rank-1][0]\n",
    "    c_did = reranked_custom[rank-1][0]\n",
    "    n_did = nrf_custom[rank-1][0]\n",
    "    print(f\"  {rank}    {b_did:<18}  {c_did:<18}  {n_did:<18}\")\n",
    "\n",
    "# Show the top NRF result that moved the most\n",
    "best_nrf = nrf_custom[0]\n",
    "old_rank = bm25_ids.get(best_nrf[0], '?')\n",
    "print(f\"\\n🔍 Top NRF passage (was BM25 rank {old_rank}, now rank 1, \"\n",
    "      f\"cosine={best_nrf[1]:.4f}):\")\n",
    "print(f\"   {best_nrf[2][:250]}...\")\n",
    "\n",
    "print(f\"\\n✓ Full pipeline complete: Text → Chunk → Index → BM25 → \"\n",
    "      f\"Neural Rerank → Neural Feedback\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "| Method | Approach | Strength | Weakness |\n",
    "|--------|----------|----------|----------|\n",
    "| **TF-IDF** | Term frequency $\\times$ inverse document frequency | Simple, interpretable | No saturation, no length norm |\n",
    "| **BM25** | Probabilistic model with saturation ($k_1$) and length norm ($b$) | Strong baseline, fast | Lexical gap |\n",
    "| **Neural (Cross-encoder)** | Transformer-based semantic scoring | Captures meaning, state-of-the-art | Slow per pair, needs BM25 first |\n",
    "| **Neural Relevance Feedback** | User phrases → embedding centroid → cosine reranking | Bridges user intent with semantics | Requires user interaction |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **BM25**: $\\displaystyle\\sum_{t \\in q} \\text{IDF}(t)\\;\\frac{f(t,d)(k_1+1)}{f(t,d)+k_1(1-b+b\\,|d|/\\text{avgdl})}$ — the default first-stage retriever\n",
    "2. **The lexical gap** is the fundamental limitation of all keyword-based methods\n",
    "3. **ColBERT MaxSim**: $\\sum_i \\max_j\\;\\mathbf{q}_i^\\top\\mathbf{d}_j$ — token-level semantic scoring\n",
    "4. **Retrieve-then-rerank** (BM25 → Neural) is the dominant paradigm in modern search\n",
    "5. **Neural relevance feedback** modernises Rocchio: user-selected phrases become an embedding centroid that captures meaning, not just terms\n",
    "6. **nDCG@k** and **MAP** are the standard metrics for evaluating ranking quality\n",
    "7. Neural reranking **significantly outperforms** BM25 on TREC-DL benchmarks\n",
    "8. **You can build a full search engine** over any text in minutes: chunk → JSONL → Lucene index → BM25 → neural reranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "The following exercises are graded. You are expected to answer them on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 — BM25 Parameter Analysis (5 points)\n",
    "\n",
    "The BM25 formula uses two key parameters: $k_1$ (term frequency saturation) and $b$ (document length normalisation).\n",
    "\n",
    "1. Explain the **intuition** behind each parameter. What does each control and why is it needed?\n",
    "2. If you are searching a collection of **scientific abstracts** (all approximately the same length, ~200 words), how would you adjust $b$? Justify your answer.\n",
    "3. If you are searching a collection where **repeated keywords are a strong signal of relevance** (e.g., product reviews mentioning a specific feature), how would you adjust $k_1$? Justify your answer.\n",
    "4. Explain why BM25 with $k_1 \\to 0$ and $b = 0$ reduces to a simpler scoring model. Which model does it approximate?\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 — The Lexical Gap and Neural Solutions (5 points)\n",
    "\n",
    "Consider the following search scenario.\n",
    "\n",
    "**Query:** *\"renewable energy impact on wildlife\"*\n",
    "\n",
    "BM25 retrieves these top-3 passages:\n",
    "1. *\"Renewable energy sources include solar, wind, and hydroelectric power. These energy sources are considered renewable because they are naturally replenished.\"*\n",
    "2. *\"The impact of energy production on the environment has been studied extensively. Wind farms and solar panels are common renewable energy installations.\"*\n",
    "3. *\"Wildlife conservation efforts focus on protecting endangered species and their natural habitats from human activities.\"*\n",
    "\n",
    "A passage rated **highly relevant** by NIST assessors but ranked at position 87 by BM25:\n",
    "- *\"Bird and bat mortality near wind turbines has raised ecological concerns. Studies show that migratory species are particularly vulnerable to collisions with turbine blades, highlighting the tension between clean power generation and biodiversity preservation.\"*\n",
    "\n",
    "Answer the following:\n",
    "\n",
    "1. Explain **why** BM25 ranked the relevant passage so low. Identify the specific vocabulary mismatches.\n",
    "2. Explain how a **ColBERT MaxSim** model would handle this differently. Reference the specific token-level matches that MaxSim would capture.\n",
    "3. Would a **bi-encoder** (like DPR) also solve this problem? How does its approach differ from ColBERT's token-level interaction?\n",
    "\n",
    "Write your answer in the cell below (minimum 200 words).\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 — Implementing Precision@k and Recall@k (10 points)\n",
    "\n",
    "Write code that computes **Precision@k** and **Recall@k** for both BM25 and Neural reranking on the TREC-DL 2019 data used in this tutorial.\n",
    "\n",
    "Your code should:\n",
    "\n",
    "1. Implement `precision_at_k(ranked_docids, qrels_dict, k)` — returns the fraction of the top-*k* documents that are relevant (qrel $\\geq$ 2)\n",
    "2. Implement `recall_at_k(ranked_docids, qrels_dict, k)` — returns the fraction of all relevant documents (qrel $\\geq$ 2) that appear in the top-*k*\n",
    "3. Compute the **mean** Precision@10 and Recall@10 across all TREC-DL 2019 queries for both `bm25_results` and `reranked_results`\n",
    "4. Store the results in four variables: `p10_bm25`, `p10_neural`, `r10_bm25`, `r10_neural` (all floats between 0 and 1)\n",
    "\n",
    "**Variables available** (defined earlier in this notebook):\n",
    "- `bm25_results` — dict: query ID → list of `(docid, bm25_score, passage)` tuples\n",
    "- `reranked_results` — dict: query ID → list of `(docid, ce_score, passage)` tuples\n",
    "- `qrels` — dict: query ID → dict `{docid: relevance_score}`\n",
    "- `topics` — dict: query ID → topic dict with `'title'` key\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Replace this line with your solution",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mReplace this line with your solution\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: Replace this line with your solution"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line with your solution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell — do not modify\n",
    "assert 'p10_bm25' in dir(), \"Define 'p10_bm25'\"\n",
    "assert 'p10_neural' in dir(), \"Define 'p10_neural'\"\n",
    "assert 'r10_bm25' in dir(), \"Define 'r10_bm25'\"\n",
    "assert 'r10_neural' in dir(), \"Define 'r10_neural'\"\n",
    "\n",
    "for name, val in [('p10_bm25', p10_bm25), ('p10_neural', p10_neural),\n",
    "                   ('r10_bm25', r10_bm25), ('r10_neural', r10_neural)]:\n",
    "    assert isinstance(val, float), f\"{name} should be a float, got {type(val)}\"\n",
    "    assert 0 <= val <= 1, f\"{name} should be in [0, 1], got {val}\"\n",
    "\n",
    "# Neural should outperform or match BM25 on precision\n",
    "assert p10_neural >= p10_bm25 - 0.01, (\n",
    "    f\"Expected neural P@10 ({p10_neural:.4f}) >= BM25 P@10 ({p10_bm25:.4f})\")\n",
    "\n",
    "print(f\"{'Method':<12} {'P@10':>8} {'R@10':>8}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'BM25':<12} {p10_bm25:>8.4f} {r10_bm25:>8.4f}\")\n",
    "print(f\"{'Neural':<12} {p10_neural:>8.4f} {r10_neural:>8.4f}\")\n",
    "print(f\"\\nAll auto-graded tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb2c03",
   "metadata": {},
   "source": [
    "## Exercise 4 — Build a Search Engine over Your Own Text (15 points)\n",
    "\n",
    "In this exercise you will replicate the **full pipeline** from Section 10 using a text of **your own choice**. This corpus will also be used in Tutorial 07 (Knowledge Graphs) and Tutorial 11 (RAG Chatbot).\n",
    "\n",
    "### Step 1 — Choose and load your text (2 points)\n",
    "\n",
    "Pick **one** of the following sources (or bring your own):\n",
    "- A novel or short story collection from [Project Gutenberg](https://www.gutenberg.org/)\n",
    "- TV show scripts (e.g., South Park, The Office, Breaking Bad) from a fan wiki or script site\n",
    "- A PDF textbook chapter, thesis, or technical report\n",
    "- Wikipedia articles on a topic (e.g., \"History of AI\", \"Quantum Computing\")\n",
    "- Song lyrics from your favorite band\n",
    "\n",
    "**Requirements:**\n",
    "- Your text should be at least **10,000 words** (roughly 20+ pages)\n",
    "- Load it into a Python string variable called `my_raw_text`\n",
    "- If using a PDF, parse it with `fitz` (PyMuPDF) as shown in Section 10a\n",
    "\n",
    "### Step 2 — Chunk into passages (3 points)\n",
    "\n",
    "- Split your text into paragraph-sized passages (aim for 50–250 words each)\n",
    "- You may reuse or adapt the `chunk_text()` function from Section 10b, or write your own\n",
    "- Store the result in a list called `my_chunks` — each element is a string\n",
    "- Print the total number of chunks and the min/mean/max word count\n",
    "\n",
    "### Step 3 — Build a Lucene index (2 points)\n",
    "\n",
    "- Export your chunks as JSONL (format: `{\"id\": \"...\", \"contents\": \"...\"}`)\n",
    "- Build a Pyserini Lucene index using `pyserini.index.lucene`\n",
    "- Open the index with `LuceneSearcher` and verify the document count\n",
    "- Store the searcher in a variable called `my_searcher`\n",
    "\n",
    "### Step 4 — BM25 search (3 points)\n",
    "\n",
    "- Write **at least 5 queries** relevant to your chosen text\n",
    "- Run BM25 search (top-20) for each query using `my_searcher`\n",
    "- Display the top-3 results for each query with BM25 scores\n",
    "- Store results in a dict called `my_bm25_results` with structure: `{query_string: [(docid, score, passage), ...]}`\n",
    "\n",
    "### Step 5 — Neural reranking and feedback (5 points)\n",
    "\n",
    "For **one** of your queries:\n",
    "1. Rerank the BM25 top-20 using the `cross_encoder` (already loaded above) — **2 points**\n",
    "2. Select 2–3 phrases from the best passages and perform **Neural Relevance Feedback** using the `bi_encoder` (already loaded above) — **2 points**\n",
    "3. Show a side-by-side comparison of BM25 vs Cross-encoder vs NRF rankings (top-5) — **1 point**\n",
    "\n",
    "> **Important:** Save your `my_chunks` list to disk (e.g., as JSON or text files in a folder). You will load this same corpus in Tutorial 07 to build a Knowledge Graph and in Tutorial 11 to build a RAG chatbot.\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba46ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Exercise 4: Your code here ────────────────────────────────────────\n",
    "# Follow the steps outlined above. Starter scaffolding is provided below.\n",
    "# Replace each TODO with your implementation.\n",
    "\n",
    "# ─── Step 1: Load your text ──────────────────────────────────────────\n",
    "# TODO: Download or load your chosen text into my_raw_text\n",
    "# Example for Project Gutenberg:\n",
    "#   resp = requests.get(\"https://www.gutenberg.org/cache/epub/XXXX/pgXXXX.txt\")\n",
    "#   my_raw_text = resp.text\n",
    "# Example for PDF:\n",
    "#   pdf = fitz.open(\"your_file.pdf\")\n",
    "#   my_raw_text = \"\\n\\n\".join(page.get_text() for page in pdf)\n",
    "#   pdf.close()\n",
    "\n",
    "my_raw_text = \"\"   # TODO: replace with your text\n",
    "raise NotImplementedError(\"Step 1: Load your text\")\n",
    "\n",
    "# ─── Step 2: Chunk into passages ─────────────────────────────────────\n",
    "# TODO: Split my_raw_text into passages of 50-250 words each\n",
    "my_chunks = []     # TODO: list of strings\n",
    "raise NotImplementedError(\"Step 2: Chunk your text\")\n",
    "\n",
    "print(f\"Chunks: {len(my_chunks)}\")\n",
    "wc = [len(c.split()) for c in my_chunks]\n",
    "print(f\"Words per chunk: min={min(wc)}, mean={np.mean(wc):.0f}, max={max(wc)}\")\n",
    "\n",
    "# ─── Step 3: Build Lucene index ──────────────────────────────────────\n",
    "# TODO: Export as JSONL and build index (follow Section 10b-10c pattern)\n",
    "my_index_dir = \"my_corpus/lucene_index\"\n",
    "my_jsonl_dir = \"my_corpus/jsonl\"\n",
    "# ... export JSONL ...\n",
    "# ... run pyserini.index.lucene ...\n",
    "my_searcher = None  # TODO: LuceneSearcher(my_index_dir)\n",
    "raise NotImplementedError(\"Step 3: Build your index\")\n",
    "\n",
    "print(f\"Index built: {my_searcher.num_docs} documents\")\n",
    "\n",
    "# ─── Step 4: BM25 search ─────────────────────────────────────────────\n",
    "# TODO: Define at least 5 queries and run BM25 search\n",
    "my_queries = {\n",
    "    # \"Q1\": \"your first query\",\n",
    "    # \"Q2\": \"your second query\",\n",
    "    # ...\n",
    "}\n",
    "my_bm25_results = {}  # TODO: {query_string: [(docid, score, passage), ...]}\n",
    "raise NotImplementedError(\"Step 4: BM25 search\")\n",
    "\n",
    "# ─── Step 5: Neural reranking + feedback ─────────────────────────────\n",
    "# TODO: For one query:\n",
    "#   1. Cross-encoder reranking of BM25 top-20\n",
    "#   2. Neural relevance feedback (select phrases → bi-encoder centroid)\n",
    "#   3. Side-by-side comparison table\n",
    "raise NotImplementedError(\"Step 5: Neural reranking\")\n",
    "\n",
    "# ─── Save chunks for Tutorial 07 and 11 ──────────────────────────────\n",
    "# TODO: Save your chunks to disk\n",
    "# Example:\n",
    "#   my_chunks_dir = pathlib.Path(\"my_corpus/chunks\")\n",
    "#   my_chunks_dir.mkdir(parents=True, exist_ok=True)\n",
    "#   for i, chunk in enumerate(my_chunks):\n",
    "#       (my_chunks_dir / f\"chunk_{i:04d}.txt\").write_text(chunk, encoding=\"utf-8\")\n",
    "#   print(f\"Saved {len(my_chunks)} chunks to {my_chunks_dir}/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
