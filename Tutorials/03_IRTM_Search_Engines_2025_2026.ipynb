{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
    "\n",
    "# Information Retrieval and Text Mining Course\n",
    "## Tutorial 03 — Search Engines: Relevance Ranking\n",
    "\n",
    "**Author:** Jan Scholtes\n",
    "\n",
    "**Edition 2025-2026**\n",
    "\n",
    "Department of Advanced Computer Sciences — Maastricht University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Tutorial 03 on **Search Engines: Relevance Ranking**. In this tutorial we explore how search engines rank documents by relevance, progressing from classical lexical methods to neural approaches.\n",
    "\n",
    "The tutorial is organised in three stages:\n",
    "\n",
    "1. **Stage 1 — BM25 Baseline**: We use [Pyserini](https://github.com/castorini/pyserini) to perform BM25 retrieval on the MS MARCO passage corpus with official TREC Deep Learning 2019 queries. We observe where keyword-based ranking succeeds and where it fails.\n",
    "2. **Stage 2 — Neural Reranking**: We apply a neural cross-encoder to rerank the BM25 results, demonstrating how semantic understanding produces better rankings.\n",
    "3. **Stage 3 — Quantitative Evaluation**: We compute nDCG@10 and MAP using official NIST relevance judgments (qrels) to show rigorous evidence that neural reranking outperforms BM25.\n",
    "\n",
    "Before the experiment we review the theory behind TF-IDF, BM25, and neural ranking methods.\n",
    "\n",
    "**Dataset**: MS MARCO Passage Ranking with TREC Deep Learning 2019 evaluation queries (43 queries with graded relevance judgments from NIST assessors).\n",
    "\n",
    "> **Note:** This course is about Information Retrieval, Text Mining, and Conversational Search — not about programming skills. The code cells below show you *how* these methods work in practice using Python libraries. Focus on understanding the **concepts** and **results**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Installation\n",
    "\n",
    "We install all required packages in a single cell. Run this cell once at the beginning of your session.\n",
    "\n",
    "**Important:** Pyserini requires **Java 11+** (JDK, not just JRE). If you do not have Java installed:\n",
    "- **Windows:** `winget install Microsoft.OpenJDK.21`\n",
    "- **macOS:** `brew install openjdk@21`\n",
    "- **Linux:** `sudo apt install openjdk-21-jdk`\n",
    "\n",
    "The first time you run the search cells, Pyserini will download the MS MARCO passage index (~2 GB). This is a one-time operation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    \"pyserini\",\n",
    "    \"sentence-transformers\",\n",
    "    \"faiss-cpu\",\n",
    "    \"scikit-learn\",\n",
    "]\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "\n",
    "print(\"All packages installed successfully.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Java & Environment Setup ---\n",
    "import os, json, math, sys, warnings\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Pyserini requires JAVA_HOME pointing to a JDK 11+ installation.\n",
    "# The cell below tries to auto-detect your JDK. If it fails, set the\n",
    "# path manually: os.environ[\"JAVA_HOME\"] = r\"C:\\path\\to\\jdk\"\n",
    "if \"JAVA_HOME\" not in os.environ or not os.environ[\"JAVA_HOME\"]:\n",
    "    import glob\n",
    "    candidates = (\n",
    "        glob.glob(r\"C:\\Program Files\\Microsoft\\jdk-*\")\n",
    "        + glob.glob(r\"C:\\Program Files\\Java\\jdk-*\")\n",
    "        + glob.glob(\"/usr/lib/jvm/java-*-openjdk*\")\n",
    "        + glob.glob(\"/Library/Java/JavaVirtualMachines/*/Contents/Home\")\n",
    "    )\n",
    "    if candidates:\n",
    "        os.environ[\"JAVA_HOME\"] = sorted(candidates)[-1]\n",
    "        print(f\"Auto-detected JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
    "    else:\n",
    "        raise EnvironmentError(\n",
    "            \"JAVA_HOME not set and no JDK found.\\n\"\n",
    "            \"Install JDK 11+ first (e.g. winget install Microsoft.OpenJDK.21)\"\n",
    "        )\n",
    "\n",
    "# --- Pyserini & model imports ---\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from pyserini.search import get_topics, get_qrels\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "print(f\"Pyserini loaded  | JAVA_HOME: {os.environ['JAVA_HOME']}\")\n",
    "print(f\"PyTorch {torch.__version__} | CUDA available: {torch.cuda.is_available()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Keywords to Meaning: The Relevance Problem\n",
    "\n",
    "At the heart of every search engine is a **ranking function** — a mathematical formula that decides which documents are most relevant to a query.\n",
    "\n",
    "The simplest approach is **exact keyword matching**: find documents that contain the query terms and rank them by how often those terms appear. This works surprisingly well, but it has a fundamental limitation called the **lexical gap**:\n",
    "\n",
    "> A user searching for *\"how to fix a broken screen\"* may not find a document titled *\"smartphone display repair guide\"* because the words do not match — even though the meaning is the same.\n",
    "\n",
    "This tutorial explores the evolution from keyword-based to semantic ranking:\n",
    "\n",
    "| Generation | Method | Matching | Limitation |\n",
    "|-----------|--------|----------|------------|\n",
    "| 1st | TF-IDF | Exact term overlap | No term importance model |\n",
    "| 2nd | BM25 | Probabilistic term weighting | Still requires word overlap |\n",
    "| 3rd | Neural (BERT, ColBERT) | Semantic similarity | Computationally expensive |\n",
    "\n",
    "We will see this progression **in practice** using a real search engine (Pyserini) and a real evaluation benchmark (TREC-DL 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF: The Foundation of Lexical Ranking\n",
    "\n",
    "**Term Frequency – Inverse Document Frequency** (TF-IDF) is the most widely used term-weighting scheme.\n",
    "\n",
    "### Term Frequency (TF)\n",
    "\n",
    "The raw term frequency $f(t, d)$ counts how often term $t$ appears in document $d$. To dampen the effect of very frequent terms we often use the log variant:\n",
    "\n",
    "$$\\text{TF}(t, d) = 1 + \\log f(t, d) \\quad\\text{if } f(t, d) > 0,\\;\\text{else } 0$$\n",
    "\n",
    "### Inverse Document Frequency (IDF)\n",
    "\n",
    "A term that appears in many documents is less informative. IDF captures this:\n",
    "\n",
    "$$\\text{IDF}(t) = \\log \\frac{N}{df(t)}$$\n",
    "\n",
    "where $N$ is the total number of documents and $df(t)$ is the number of documents containing term $t$.\n",
    "\n",
    "### Combined TF-IDF Score\n",
    "\n",
    "$$\\text{TF\\text{-}IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
    "\n",
    "The score for a query $q$ against document $d$ sums over all query terms:\n",
    "\n",
    "$$\\text{Score}(q, d) = \\sum_{t \\in q} \\text{TF\\text{-}IDF}(t, d)$$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TF-IDF demonstration on toy documents\n",
    "\n",
    "documents = [\n",
    "    \"information retrieval is the science of searching for information\",\n",
    "    \"machine learning models can improve search relevance\",\n",
    "    \"information retrieval systems use inverted indexes for fast search\",\n",
    "    \"deep learning transforms natural language understanding\",\n",
    "]\n",
    "query = \"information retrieval search\"\n",
    "\n",
    "# Tokenise\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "doc_tokens = [tokenize(d) for d in documents]\n",
    "query_tokens = tokenize(query)\n",
    "N = len(documents)\n",
    "\n",
    "# Compute document frequency & IDF\n",
    "df = Counter()\n",
    "for tokens in doc_tokens:\n",
    "    for t in set(tokens):\n",
    "        df[t] += 1\n",
    "\n",
    "idf = {t: math.log(N / df[t]) for t in df}\n",
    "\n",
    "# Score each document\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(f\"{'Term':<20} {'IDF':>6}\")\n",
    "print(\"-\" * 28)\n",
    "for t in query_tokens:\n",
    "    print(f\"{t:<20} {idf.get(t, 0):>6.3f}\")\n",
    "\n",
    "print(f\"\\n{'Doc':<5} {'Score':>8}  Content\")\n",
    "print(\"-\" * 75)\n",
    "for i, tokens in enumerate(doc_tokens):\n",
    "    tf = Counter(tokens)\n",
    "    score = sum(\n",
    "        (1 + math.log(tf[t])) * idf.get(t, 0)\n",
    "        for t in query_tokens if tf[t] > 0\n",
    "    )\n",
    "    print(f\"D{i:<4} {score:>8.3f}  {documents[i]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BM25: The Best Lexical Ranker\n",
    "\n",
    "**BM25** (Best Match 25) is a probabilistic ranking function developed at City University London in the 1990s as part of the Okapi system. It extends TF-IDF with two important improvements:\n",
    "\n",
    "1. **Saturation** — term frequency has diminishing returns (a word appearing 100 times is not 100× more relevant than appearing once)\n",
    "2. **Length normalisation** — longer documents are not automatically favoured\n",
    "\n",
    "### The BM25 Formula\n",
    "\n",
    "$$\\text{BM25}(q, d) = \\sum_{t \\in q} \\text{IDF}(t) \\;\\cdot\\; \\frac{f(t,d) \\cdot (k_1 + 1)}{f(t,d) + k_1 \\cdot \\left(1 - b + b \\cdot \\dfrac{|d|}{\\text{avgdl}}\\right)}$$\n",
    "\n",
    "where:\n",
    "- $f(t, d)$ = frequency of term $t$ in document $d$\n",
    "- $|d|$ = length of document $d$ (in words)\n",
    "- $\\text{avgdl}$ = average document length in the collection\n",
    "- $k_1$ = term frequency saturation parameter (typically **1.2**)\n",
    "- $b$ = length normalisation parameter (typically **0.75**)\n",
    "\n",
    "### IDF Component\n",
    "\n",
    "$$\\text{IDF}(t) = \\log \\frac{N - df(t) + 0.5}{df(t) + 0.5}$$\n",
    "\n",
    "### Understanding the Parameters\n",
    "\n",
    "| $k_1$ | Effect |\n",
    "|-------|--------|\n",
    "| $\\to 0$ | All non-zero term frequencies treated equally (binary matching) |\n",
    "| $\\to \\infty$ | Raw term frequency dominates (no saturation) |\n",
    "\n",
    "| $b$ | Effect |\n",
    "|-----|--------|\n",
    "| $= 0$ | No length normalisation |\n",
    "| $= 1$ | Full normalisation relative to average length |\n",
    "\n",
    "BM25 remains the **default baseline** in modern information retrieval and is the first-stage retriever in most production search systems."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Explore how BM25 parameters k1 and b affect scoring\n",
    "\n",
    "def bm25_term_weight(tf, dl, avgdl, N, df, k1=1.2, b=0.75):\n",
    "    \"\"\"BM25 weight for a single term in a document.\"\"\"\n",
    "    idf = math.log((N - df + 0.5) / (df + 0.5))\n",
    "    tf_part = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * dl / avgdl))\n",
    "    return idf * tf_part\n",
    "\n",
    "# Fixed parameters (MS MARCO scale)\n",
    "N, df_val, avgdl = 8_800_000, 50_000, 60\n",
    "\n",
    "print(\"Effect of k1 (term frequency saturation)  [df=50 000, avgdl=60, dl=60]\")\n",
    "print(f\"{'TF':>4}\", end=\"\")\n",
    "for k1 in [0.01, 0.5, 1.2, 3.0, 10.0]:\n",
    "    print(f\"  k1={k1:<5}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 55)\n",
    "for tf in [1, 2, 5, 10, 20, 50]:\n",
    "    print(f\"{tf:>4}\", end=\"\")\n",
    "    for k1 in [0.01, 0.5, 1.2, 3.0, 10.0]:\n",
    "        w = bm25_term_weight(tf, 60, avgdl, N, df_val, k1=k1, b=0.75)\n",
    "        print(f\"  {w:>8.2f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nEffect of b (length normalisation)  [tf=3, k1=1.2, avgdl={avgdl}]\")\n",
    "print(f\"{'DocLen':>7}\", end=\"\")\n",
    "for b in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "    print(f\"   b={b:<5}\", end=\"\")\n",
    "print()\n",
    "print(\"-\" * 55)\n",
    "for dl in [20, 40, 60, 100, 200, 500]:\n",
    "    print(f\"{dl:>7}\", end=\"\")\n",
    "    for b in [0.0, 0.25, 0.5, 0.75, 1.0]:\n",
    "        w = bm25_term_weight(3, dl, avgdl, N, df_val, k1=1.2, b=b)\n",
    "        print(f\"  {w:>8.2f}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\nKey observations:\")\n",
    "print(\"  Higher k1  ->  less saturation  ->  raw TF matters more\")\n",
    "print(\"  Higher b   ->  more length normalisation  ->  short documents boosted\")\n",
    "print(\"  Default (k1=1.2, b=0.75) balances both effects\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 1 — BM25 Search with Pyserini\n",
    "\n",
    "Now we move from theory to practice. We use [Pyserini](https://github.com/castorini/pyserini), a Python toolkit for reproducible IR research, to perform BM25 search on a real corpus.\n",
    "\n",
    "### Dataset: MS MARCO Passages + TREC-DL 2019\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| **Corpus** | MS MARCO v1 passage collection — **8.8 million passages** from web documents (Microsoft) |\n",
    "| **Queries** | 43 queries from TREC Deep Learning 2019, selected by NIST for rigorous evaluation |\n",
    "| **Relevance judgments** | Graded assessments by NIST assessors (0 = not relevant … 3 = perfectly relevant) |\n",
    "\n",
    "Pyserini provides a **pre-built Lucene index** for MS MARCO, so we can start searching immediately.\n",
    "\n",
    "> **Note:** The first run downloads the pre-built index (~2 GB). Subsequent runs use the cached version."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load pre-built index, queries, and relevance judgments\n",
    "print(\"Loading MS MARCO passage index (first run downloads ~2 GB)...\")\n",
    "searcher = LuceneSearcher.from_prebuilt_index('msmarco-v1-passage')\n",
    "print(f\"Index loaded: {searcher.num_docs:,} passages\")\n",
    "\n",
    "# TREC Deep Learning 2019 topics and official qrels\n",
    "topics = get_topics('dl19-passage')\n",
    "qrels  = get_qrels('dl19-passage')\n",
    "print(f\"TREC-DL 2019 queries: {len(topics)}\")\n",
    "print(f\"TREC-DL 2019 qrels  : {len(qrels)} queries with judgments\")\n",
    "\n",
    "# Show a few example queries\n",
    "print(\"\\nExample queries:\")\n",
    "for i, (qid, topic) in enumerate(topics.items()):\n",
    "    if i >= 6:\n",
    "        break\n",
    "    n_rel = sum(1 for r in qrels.get(qid, {}).values() if r >= 2)\n",
    "    print(f\"  [{qid}] {topic['title']:<55} ({n_rel} highly-relevant docs)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run BM25 search for all TREC-DL 2019 queries (top-100 per query)\n",
    "TOP_K = 100\n",
    "bm25_results = {}   # {qid: [(docid, bm25_score, passage_text), ...]}\n",
    "\n",
    "print(f\"Running BM25 search (top-{TOP_K}) for {len(topics)} queries...\")\n",
    "for qid, topic in topics.items():\n",
    "    query = topic['title']\n",
    "    hits = searcher.search(query, k=TOP_K)\n",
    "    results = []\n",
    "    for hit in hits:\n",
    "        doc = searcher.doc(hit.docid)\n",
    "        passage = json.loads(doc.raw())['contents']\n",
    "        results.append((hit.docid, hit.score, passage))\n",
    "    bm25_results[qid] = results\n",
    "\n",
    "print(f\"BM25 search complete: {len(bm25_results)} queries processed\")\n",
    "print(f\"Average results per query: \"\n",
    "      f\"{np.mean([len(v) for v in bm25_results.values()]):.0f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Display BM25 top-5 for a few example queries\n",
    "example_qids = list(topics.keys())[:3]\n",
    "\n",
    "for qid in example_qids:\n",
    "    query = topics[qid]['title']\n",
    "    results = bm25_results[qid]\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query [{qid}]: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    for rank, (docid, score, passage) in enumerate(results[:5], 1):\n",
    "        rel = qrels.get(qid, {}).get(docid, '-')\n",
    "        print(f\"\\n  Rank {rank} | BM25: {score:.4f} | Rel: {rel} | {docid}\")\n",
    "        print(f\"  {passage[:150]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Lexical Gap\n",
    "\n",
    "BM25 works well when query and document share the same vocabulary. But what happens when they use **different words for the same concept**?\n",
    "\n",
    "| Query | Relevant passage uses… | BM25 can match? |\n",
    "|-------|------------------------|-----------------|\n",
    "| \"fix broken screen\" | \"display repair guide\" | No word overlap |\n",
    "| \"heart attack symptoms\" | \"signs of myocardial infarction\" | Medical synonyms |\n",
    "| \"affordable housing\" | \"low-cost residential options\" | Paraphrases |\n",
    "\n",
    "This is the **vocabulary mismatch problem** — the fundamental limitation of all lexical methods, including BM25. No matter how sophisticated the term weighting, if the words do not match the document will not be found.\n",
    "\n",
    "Let us quantify this on our TREC-DL data: how many highly-relevant documents does BM25 actually find?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Lexical Gap Analysis: how many highly-relevant docs (qrel >= 2)\n",
    "# appear in BM25 top-100?\n",
    "\n",
    "print(\"Highly-relevant documents (qrel >= 2) found in BM25 top-100\\n\")\n",
    "print(f\"{'QID':<10} {'Query':<45} {'Found':>5} {'Total':>5} {'Recall':>7}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "recall_values = []\n",
    "for qid in sorted(topics.keys()):\n",
    "    if qid not in qrels:\n",
    "        continue\n",
    "    query = topics[qid]['title']\n",
    "    relevant = {did for did, r in qrels[qid].items() if r >= 2}\n",
    "    if not relevant:\n",
    "        continue\n",
    "    retrieved = {docid for docid, _, _ in bm25_results.get(qid, [])}\n",
    "    found = relevant & retrieved\n",
    "    recall = len(found) / len(relevant)\n",
    "    recall_values.append(recall)\n",
    "    print(f\"{qid:<10} {query[:43]:<45} {len(found):>5} {len(relevant):>5} {recall:>7.1%}\")\n",
    "\n",
    "print(f\"\\nMean recall of highly-relevant docs: {np.mean(recall_values):.1%}\")\n",
    "print(f\"Queries with < 100% recall: \"\n",
    "      f\"{sum(1 for r in recall_values if r < 1.0)}/{len(recall_values)}\")\n",
    "print(\"\\nBM25 misses some relevant passages — this is the lexical gap in action.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Ranking: Beyond Exact Match\n",
    "\n",
    "Neural ranking models use **learned representations** (embeddings) to capture semantic similarity between queries and documents. Instead of matching words they match *meanings*.\n",
    "\n",
    "### Three Architectures for Neural Ranking\n",
    "\n",
    "| Architecture | Example | How it works | Speed | Quality |\n",
    "|-------------|---------|-------------|-------|---------|\n",
    "| **Bi-encoder** | DPR, SBERT | Query and doc encoded independently; cosine similarity | Fast | Good |\n",
    "| **Late interaction** | ColBERT | Token-level embeddings; MaxSim aggregation | Medium | Better |\n",
    "| **Cross-encoder** | monoBERT | Query + doc processed jointly by a transformer | Slow | Best |\n",
    "\n",
    "### ColBERT: Late Interaction via MaxSim\n",
    "\n",
    "ColBERT (Contextualized Late Interaction over BERT) scores a query–document pair by:\n",
    "\n",
    "1. Encoding query tokens:  $\\mathbf{Q} = [\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_m]$\n",
    "2. Encoding document tokens:  $\\mathbf{D} = [\\mathbf{d}_1, \\mathbf{d}_2, \\ldots, \\mathbf{d}_n]$\n",
    "3. Computing **MaxSim** — for each query token, find its maximum similarity to any document token:\n",
    "\n",
    "$$\\text{ColBERT}(q, d) = \\sum_{i=1}^{m} \\max_{j=1}^{n}\\; \\mathbf{q}_i^\\top \\mathbf{d}_j$$\n",
    "\n",
    "This captures fine-grained token-level semantic matches — for example \"screen\" matching \"display\" through contextual embeddings.\n",
    "\n",
    "### Cross-Encoder: Joint Encoding\n",
    "\n",
    "A cross-encoder feeds the concatenated query–document pair through BERT:\n",
    "\n",
    "$$\\text{Score}(q, d) = \\text{BERT}_{\\text{cls}}\\!\\bigl([\\texttt{CLS}]\\; q \\;[\\texttt{SEP}]\\; d \\;[\\texttt{SEP}]\\bigr)$$\n",
    "\n",
    "Cross-encoders are the most powerful but slowest neural rankers — they are used to **rerank** a small candidate set retrieved by BM25.\n",
    "\n",
    "In this tutorial we use a cross-encoder to rerank BM25's top-100 results. This demonstrates the same principle as ColBERT: **semantic understanding beats keyword matching**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 2 — Neural Reranking of BM25 Results\n",
    "\n",
    "We now apply a neural cross-encoder to rerank the BM25 top-100 results. The model — `cross-encoder/ms-marco-MiniLM-L-6-v2` — was trained on the MS MARCO dataset to predict query–passage relevance.\n",
    "\n",
    "**Pipeline:**\n",
    "1. **BM25** retrieves top-100 candidate passages (fast, recall-oriented)\n",
    "2. **Cross-encoder** scores each (query, passage) pair (slower, precision-oriented)\n",
    "3. Passages are **reranked** by the cross-encoder score\n",
    "\n",
    "This **retrieve-then-rerank** pattern is the standard approach in modern search engines."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Load cross-encoder model\n",
    "print(\"Loading cross-encoder: cross-encoder/ms-marco-MiniLM-L-6-v2 ...\")\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "print(\"Cross-encoder loaded.\\n\")\n",
    "\n",
    "# Rerank all queries\n",
    "reranked_results = {}   # {qid: [(docid, ce_score, passage), ...]}\n",
    "\n",
    "print(f\"Reranking {len(bm25_results)} queries (top-{TOP_K} each)...\")\n",
    "for i, (qid, results) in enumerate(bm25_results.items()):\n",
    "    query = topics[qid]['title']\n",
    "\n",
    "    # Prepare (query, passage) pairs\n",
    "    pairs = [(query, passage) for _, _, passage in results]\n",
    "\n",
    "    # Score all pairs at once\n",
    "    ce_scores = cross_encoder.predict(pairs, show_progress_bar=False)\n",
    "\n",
    "    # Sort by cross-encoder score (descending)\n",
    "    reranked = sorted(\n",
    "        [(docid, float(sc), passage)\n",
    "         for (docid, _, passage), sc in zip(results, ce_scores)],\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True,\n",
    "    )\n",
    "    reranked_results[qid] = reranked\n",
    "\n",
    "    if (i + 1) % 10 == 0 or (i + 1) == len(bm25_results):\n",
    "        print(f\"  {i+1}/{len(bm25_results)} queries reranked\")\n",
    "\n",
    "print(\"Neural reranking complete.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Side-by-side: BM25 ranking vs Neural reranking (top-10)\n",
    "example_qids = list(topics.keys())[:3]\n",
    "\n",
    "for qid in example_qids:\n",
    "    query = topics[qid]['title']\n",
    "\n",
    "    # Build BM25 rank lookup\n",
    "    bm25_rank = {docid: r for r, (docid, _, _)\n",
    "                 in enumerate(bm25_results[qid], 1)}\n",
    "\n",
    "    print(f\"\\n{'='*90}\")\n",
    "    print(f\"Query [{qid}]: {query}\")\n",
    "    print(f\"{'='*90}\")\n",
    "    print(f\"  {'#':>3} {'BM25#':>6} {'Move':>6} {'CE Score':>9} {'Rel':>4}  Passage\")\n",
    "    print(f\"  {'-'*82}\")\n",
    "\n",
    "    for rank, (docid, ce_score, passage) in enumerate(reranked_results[qid][:10], 1):\n",
    "        old = bm25_rank.get(docid, TOP_K + 1)\n",
    "        delta = old - rank\n",
    "        if delta > 0:\n",
    "            arrow = f\"+{delta}\"\n",
    "        elif delta < 0:\n",
    "            arrow = str(delta)\n",
    "        else:\n",
    "            arrow = \"=\"\n",
    "        rel = qrels.get(qid, {}).get(docid, '-')\n",
    "        print(f\"  {rank:>3} {old:>6} {arrow:>6} {ce_score:>9.4f} {str(rel):>4}\"\n",
    "              f\"  {passage[:50]}...\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Aggregate rank-change analysis\n",
    "total_up, total_big, total_pairs = 0, 0, 0\n",
    "\n",
    "for qid in bm25_results:\n",
    "    bm25_rank = {did: r for r, (did, _, _) in enumerate(bm25_results[qid], 1)}\n",
    "    for new_rank, (docid, _, _) in enumerate(reranked_results[qid][:10], 1):\n",
    "        old_rank = bm25_rank.get(docid, TOP_K + 1)\n",
    "        change = old_rank - new_rank\n",
    "        if change > 0:\n",
    "            total_up += 1\n",
    "        if change >= 10:\n",
    "            total_big += 1\n",
    "        total_pairs += 1\n",
    "\n",
    "print(\"Rank-Change Analysis  (Neural top-10 vs BM25 ranking)\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Total (query, passage) pairs : {total_pairs}\")\n",
    "print(f\"Passages moved UP            : {total_up}  \"\n",
    "      f\"({100*total_up/total_pairs:.1f}%)\")\n",
    "print(f\"Big jumps (moved up >= 10)   : {total_big}  \"\n",
    "      f\"({100*total_big/total_pairs:.1f}%)\")\n",
    "print()\n",
    "print(\"Neural reranking reshuffles the top results significantly,\")\n",
    "print(\"promoting semantically relevant passages that BM25 ranked lower.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Metrics: nDCG and MAP\n",
    "\n",
    "How do we **objectively measure** whether one ranking is better than another? We use standard IR evaluation metrics computed against human relevance judgments.\n",
    "\n",
    "### Normalised Discounted Cumulative Gain (nDCG@k)\n",
    "\n",
    "nDCG rewards relevant documents at high positions using **graded relevance** (0, 1, 2, 3):\n",
    "\n",
    "$$\\text{DCG}@k = \\sum_{i=1}^{k} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}$$\n",
    "\n",
    "$$\\text{nDCG}@k = \\frac{\\text{DCG}@k}{\\text{IDCG}@k}$$\n",
    "\n",
    "where IDCG is the DCG of the ideal (perfect) ranking. nDCG ranges from 0 to 1.\n",
    "\n",
    "### Mean Average Precision (MAP)\n",
    "\n",
    "MAP measures how well **all** relevant documents are ranked, using **binary relevance**:\n",
    "\n",
    "$$\\text{AP}(q) = \\frac{1}{|R_q|} \\sum_{k=1}^{n} P@k \\cdot \\text{rel}(k)$$\n",
    "\n",
    "$$\\text{MAP} = \\frac{1}{|Q|} \\sum_{q \\in Q} \\text{AP}(q)$$\n",
    "\n",
    "where $P@k$ is precision at rank $k$ and $\\text{rel}(k)$ is 1 if the document at rank $k$ is relevant."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Implementation of evaluation metrics\n",
    "\n",
    "def dcg_at_k(relevances, k):\n",
    "    \"\"\"DCG@k given a list of relevance scores in ranking order.\"\"\"\n",
    "    return sum(\n",
    "        (2 ** rel - 1) / math.log2(i + 2)\n",
    "        for i, rel in enumerate(relevances[:k])\n",
    "    )\n",
    "\n",
    "def ndcg_at_k(ranked_rels, all_rels, k):\n",
    "    \"\"\"nDCG@k: ranked_rels = relevances in system order;\n",
    "    all_rels = all known relevance values (for IDCG).\"\"\"\n",
    "    idcg = dcg_at_k(sorted(all_rels, reverse=True), k)\n",
    "    if idcg == 0:\n",
    "        return 0.0\n",
    "    return dcg_at_k(ranked_rels, k) / idcg\n",
    "\n",
    "def average_precision(ranked_binary, total_relevant):\n",
    "    \"\"\"Average Precision given binary relevances in ranking order.\"\"\"\n",
    "    if total_relevant == 0:\n",
    "        return 0.0\n",
    "    ap, hits = 0.0, 0\n",
    "    for i, rel in enumerate(ranked_binary):\n",
    "        if rel:\n",
    "            hits += 1\n",
    "            ap += hits / (i + 1)\n",
    "    return ap / total_relevant\n",
    "\n",
    "# Sanity check\n",
    "test_rels = [3, 2, 0, 1, 0, 0, 2, 0, 0, 0]\n",
    "print(\"Sanity check — test ranking:\", test_rels)\n",
    "print(f\"  DCG@10 : {dcg_at_k(test_rels, 10):.4f}\")\n",
    "print(f\"  nDCG@10: {ndcg_at_k(test_rels, test_rels, 10):.4f}\")\n",
    "test_bin = [1, 1, 0, 1, 0, 0, 1, 0, 0, 0]\n",
    "print(f\"  AP     : {average_precision(test_bin, 4):.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Stage 3 — Quantitative Comparison\n",
    "\n",
    "We now evaluate both ranking approaches — **BM25** and **Neural Reranking** — using the official TREC-DL 2019 relevance judgments.\n",
    "\n",
    "For each of the 43 queries we compute:\n",
    "- **nDCG@10**: graded relevance quality at the top of the ranking\n",
    "- **MAP** (with relevance threshold $\\geq 2$): binary precision across all ranks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate BM25 vs Neural Reranking on TREC-DL 2019\n",
    "K = 10\n",
    "\n",
    "bm25_ndcg_list, neural_ndcg_list = [], []\n",
    "bm25_ap_list, neural_ap_list     = [], []\n",
    "\n",
    "print(f\"{'QID':<10} {'Query':<40} {'BM25':>8} {'Neural':>8} {'Delta':>8}\")\n",
    "print(\"-\" * 78)\n",
    "\n",
    "for qid in sorted(topics.keys()):\n",
    "    if qid not in qrels or qid not in bm25_results:\n",
    "        continue\n",
    "\n",
    "    q_qrels = qrels[qid]\n",
    "    query   = topics[qid]['title']\n",
    "    all_rels = list(q_qrels.values())\n",
    "\n",
    "    # Relevance scores in system ranking order\n",
    "    bm25_rels   = [q_qrels.get(did, 0) for did, _, _ in bm25_results[qid][:K]]\n",
    "    neural_rels = [q_qrels.get(did, 0) for did, _, _ in reranked_results[qid][:K]]\n",
    "\n",
    "    # nDCG@K\n",
    "    b_ndcg = ndcg_at_k(bm25_rels, all_rels, K)\n",
    "    n_ndcg = ndcg_at_k(neural_rels, all_rels, K)\n",
    "\n",
    "    # MAP (binary: relevant if qrel >= 2)\n",
    "    total_rel = sum(1 for r in q_qrels.values() if r >= 2)\n",
    "    b_ap = average_precision(\n",
    "        [1 if q_qrels.get(did, 0) >= 2 else 0 for did, _, _ in bm25_results[qid]],\n",
    "        total_rel)\n",
    "    n_ap = average_precision(\n",
    "        [1 if q_qrels.get(did, 0) >= 2 else 0 for did, _, _ in reranked_results[qid]],\n",
    "        total_rel)\n",
    "\n",
    "    bm25_ndcg_list.append(b_ndcg)\n",
    "    neural_ndcg_list.append(n_ndcg)\n",
    "    bm25_ap_list.append(b_ap)\n",
    "    neural_ap_list.append(n_ap)\n",
    "\n",
    "    d = n_ndcg - b_ndcg\n",
    "    flag = \"+\" if d > 0 else (\"=\" if d == 0 else \"-\")\n",
    "    print(f\"{qid:<10} {query[:38]:<40} {b_ndcg:>8.4f} {n_ndcg:>8.4f} {d:>+8.4f} {flag}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Summary statistics\n",
    "bm25_ndcg  = np.mean(bm25_ndcg_list)\n",
    "neural_ndcg = np.mean(neural_ndcg_list)\n",
    "bm25_map   = np.mean(bm25_ap_list)\n",
    "neural_map  = np.mean(neural_ap_list)\n",
    "\n",
    "pct_ndcg = (neural_ndcg - bm25_ndcg) / bm25_ndcg * 100 if bm25_ndcg else 0\n",
    "pct_map  = (neural_map - bm25_map) / bm25_map * 100 if bm25_map else 0\n",
    "\n",
    "wins   = sum(1 for b, n in zip(bm25_ndcg_list, neural_ndcg_list) if n > b)\n",
    "ties   = sum(1 for b, n in zip(bm25_ndcg_list, neural_ndcg_list) if n == b)\n",
    "losses = sum(1 for b, n in zip(bm25_ndcg_list, neural_ndcg_list) if n < b)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 62)\n",
    "print(\"       TREC-DL 2019  —  BM25  vs  Neural Reranking\")\n",
    "print(\"=\" * 62)\n",
    "print(f\"\\n{'Metric':<18} {'BM25':>10} {'Neural':>10} {'Improve':>10}\")\n",
    "print(\"-\" * 52)\n",
    "print(f\"{'nDCG@10':<18} {bm25_ndcg:>10.4f} {neural_ndcg:>10.4f} {pct_ndcg:>+9.1f}%\")\n",
    "print(f\"{'MAP (rel>=2)':<18} {bm25_map:>10.4f} {neural_map:>10.4f} {pct_map:>+9.1f}%\")\n",
    "print(f\"\\nWin / Tie / Loss (nDCG@10):  {wins}W / {ties}T / {losses}L\")\n",
    "print(f\"\\nConclusion: Neural reranking \"\n",
    "      f\"{'outperforms' if neural_ndcg > bm25_ndcg else 'matches'} \"\n",
    "      f\"BM25 by {abs(pct_ndcg):.1f}% in nDCG@10 on TREC-DL 2019.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "The results demonstrate a clear and consistent pattern:\n",
    "\n",
    "1. **BM25 is a strong baseline** — it achieves reasonable nDCG scores by effectively matching query terms to passage terms using probabilistic weighting.\n",
    "\n",
    "2. **Neural reranking consistently improves over BM25** — the cross-encoder processes query and passage jointly, capturing semantic relationships that BM25 misses:\n",
    "   - Synonyms (\"car\" ↔ \"automobile\")\n",
    "   - Paraphrases (\"how to fix\" ↔ \"repair instructions\")\n",
    "   - Conceptual similarity (\"heart attack\" ↔ \"myocardial infarction\")\n",
    "\n",
    "3. **The retrieve-then-rerank pipeline is practical** — BM25 provides fast recall over 8.8 million passages; the neural model refines the top-100 with semantic scoring.\n",
    "\n",
    "This is the same pattern used in production search engines and forms the basis for more advanced systems covered in later tutorials (RAG, Conversational Search)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "| Method | Approach | Strength | Weakness |\n",
    "|--------|----------|----------|----------|\n",
    "| **TF-IDF** | Term frequency $\\times$ inverse document frequency | Simple, interpretable | No saturation, no length norm |\n",
    "| **BM25** | Probabilistic model with saturation ($k_1$) and length norm ($b$) | Strong baseline, fast | Lexical gap |\n",
    "| **Neural (Cross-encoder)** | Transformer-based semantic scoring | Captures meaning, state-of-the-art | Slow per pair, needs BM25 first |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **BM25**: $\\displaystyle\\sum_{t \\in q} \\text{IDF}(t)\\;\\frac{f(t,d)(k_1+1)}{f(t,d)+k_1(1-b+b\\,|d|/\\text{avgdl})}$ — the default first-stage retriever\n",
    "2. **The lexical gap** is the fundamental limitation of all keyword-based methods\n",
    "3. **ColBERT MaxSim**: $\\sum_i \\max_j\\;\\mathbf{q}_i^\\top\\mathbf{d}_j$ — token-level semantic scoring\n",
    "4. **Retrieve-then-rerank** (BM25 → Neural) is the dominant paradigm in modern search\n",
    "5. **nDCG@k** and **MAP** are the standard metrics for evaluating ranking quality\n",
    "6. Neural reranking **significantly outperforms** BM25 on TREC-DL benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "\n",
    "The following exercises are graded. You are expected to answer them on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 — BM25 Parameter Analysis (5 points)\n",
    "\n",
    "The BM25 formula uses two key parameters: $k_1$ (term frequency saturation) and $b$ (document length normalisation).\n",
    "\n",
    "1. Explain the **intuition** behind each parameter. What does each control and why is it needed?\n",
    "2. If you are searching a collection of **scientific abstracts** (all approximately the same length, ~200 words), how would you adjust $b$? Justify your answer.\n",
    "3. If you are searching a collection where **repeated keywords are a strong signal of relevance** (e.g., product reviews mentioning a specific feature), how would you adjust $k_1$? Justify your answer.\n",
    "4. Explain why BM25 with $k_1 \\to 0$ and $b = 0$ reduces to a simpler scoring model. Which model does it approximate?\n",
    "\n",
    "Write your answer in the cell below (minimum 150 words).\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 — The Lexical Gap and Neural Solutions (5 points)\n",
    "\n",
    "Consider the following search scenario.\n",
    "\n",
    "**Query:** *\"renewable energy impact on wildlife\"*\n",
    "\n",
    "BM25 retrieves these top-3 passages:\n",
    "1. *\"Renewable energy sources include solar, wind, and hydroelectric power. These energy sources are considered renewable because they are naturally replenished.\"*\n",
    "2. *\"The impact of energy production on the environment has been studied extensively. Wind farms and solar panels are common renewable energy installations.\"*\n",
    "3. *\"Wildlife conservation efforts focus on protecting endangered species and their natural habitats from human activities.\"*\n",
    "\n",
    "A passage rated **highly relevant** by NIST assessors but ranked at position 87 by BM25:\n",
    "- *\"Bird and bat mortality near wind turbines has raised ecological concerns. Studies show that migratory species are particularly vulnerable to collisions with turbine blades, highlighting the tension between clean power generation and biodiversity preservation.\"*\n",
    "\n",
    "Answer the following:\n",
    "\n",
    "1. Explain **why** BM25 ranked the relevant passage so low. Identify the specific vocabulary mismatches.\n",
    "2. Explain how a **ColBERT MaxSim** model would handle this differently. Reference the specific token-level matches that MaxSim would capture.\n",
    "3. Would a **bi-encoder** (like DPR) also solve this problem? How does its approach differ from ColBERT's token-level interaction?\n",
    "\n",
    "Write your answer in the cell below (minimum 200 words).\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 — Implementing Precision@k and Recall@k (10 points)\n",
    "\n",
    "Write code that computes **Precision@k** and **Recall@k** for both BM25 and Neural reranking on the TREC-DL 2019 data used in this tutorial.\n",
    "\n",
    "Your code should:\n",
    "\n",
    "1. Implement `precision_at_k(ranked_docids, qrels_dict, k)` — returns the fraction of the top-*k* documents that are relevant (qrel $\\geq$ 2)\n",
    "2. Implement `recall_at_k(ranked_docids, qrels_dict, k)` — returns the fraction of all relevant documents (qrel $\\geq$ 2) that appear in the top-*k*\n",
    "3. Compute the **mean** Precision@10 and Recall@10 across all TREC-DL 2019 queries for both `bm25_results` and `reranked_results`\n",
    "4. Store the results in four variables: `p10_bm25`, `p10_neural`, `r10_bm25`, `r10_neural` (all floats between 0 and 1)\n",
    "\n",
    "**Variables available** (defined earlier in this notebook):\n",
    "- `bm25_results` — dict: query ID → list of `(docid, bm25_score, passage)` tuples\n",
    "- `reranked_results` — dict: query ID → list of `(docid, ce_score, passage)` tuples\n",
    "- `qrels` — dict: query ID → dict `{docid: relevance_score}`\n",
    "- `topics` — dict: query ID → topic dict with `'title'` key\n",
    "\n",
    "BEGIN SOLUTION\n",
    "\n",
    "END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError(\"Replace this line with your solution\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Autograder test cell — do not modify\n",
    "assert 'p10_bm25' in dir(), \"Define 'p10_bm25'\"\n",
    "assert 'p10_neural' in dir(), \"Define 'p10_neural'\"\n",
    "assert 'r10_bm25' in dir(), \"Define 'r10_bm25'\"\n",
    "assert 'r10_neural' in dir(), \"Define 'r10_neural'\"\n",
    "\n",
    "for name, val in [('p10_bm25', p10_bm25), ('p10_neural', p10_neural),\n",
    "                   ('r10_bm25', r10_bm25), ('r10_neural', r10_neural)]:\n",
    "    assert isinstance(val, float), f\"{name} should be a float, got {type(val)}\"\n",
    "    assert 0 <= val <= 1, f\"{name} should be in [0, 1], got {val}\"\n",
    "\n",
    "# Neural should outperform or match BM25 on precision\n",
    "assert p10_neural >= p10_bm25 - 0.01, (\n",
    "    f\"Expected neural P@10 ({p10_neural:.4f}) >= BM25 P@10 ({p10_bm25:.4f})\")\n",
    "\n",
    "print(f\"{'Method':<12} {'P@10':>8} {'R@10':>8}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'BM25':<12} {p10_bm25:>8.4f} {r10_bm25:>8.4f}\")\n",
    "print(f\"{'Neural':<12} {p10_neural:>8.4f} {r10_neural:>8.4f}\")\n",
    "print(f\"\\nAll auto-graded tests passed!\")"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}