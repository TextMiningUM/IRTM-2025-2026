{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7tNwXXJ_BBb"
      },
      "source": [
        "## Start by copying this into your Google Drive!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIBbWrF5_DXC"
      },
      "source": [
        "![Maastricht_University_logo.svg](data:image/svg+xml;base64,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)\n",
        "\n",
        "# Information Retrieval and Text Mining Course - Tutorial Document Representation\n",
        "Author: Gijs Wijngaard and Jan Scholtes\n",
        "Version: 2024-2025\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14xe-BeO_G34"
      },
      "source": [
        "Welcome to the tutorial about document representation. In this notebook you will go over a number of different methods to represent language. We start with simple representations of how to convert text into numbers. Afterwards, we focus on tf-idf and let you compute tf-idf yourself. Then, you will work with Word2Vec models, and we finish off with transformers and sentence transformers.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Ih_CLWBDAe"
      },
      "source": [
        "\n",
        "\n",
        "## Simple representations\n",
        "We start first with ways to get to numbers from data.\n",
        "Say we have the following sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOx7S5c5BCr-"
      },
      "outputs": [],
      "source": [
        "sentence = \"the quick brown fox jumps over the lazy dog\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcKShOniC-gK"
      },
      "source": [
        "We can start with several ways to represent this sentence. We count the occurrence of multiple words together. This is what we call a\n",
        "n-gram. With the counting of two words together, we call it a bigram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1D1tNh-CXaG"
      },
      "outputs": [],
      "source": [
        "splitted = sentence.split(\" \")\n",
        "[bigram for bigram in zip(splitted, splitted[1:])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H_ZcDytEF3o"
      },
      "source": [
        "With the grouping of 3 words together, we call it a trigram."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edis7nAnD_J7"
      },
      "outputs": [],
      "source": [
        "[trigram for trigram in zip(splitted, splitted[1:], splitted[2:])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXF2g4anFN0L"
      },
      "source": [
        "We can also just count each word in our sentence. The whole list of words and their occurrence is what we call a *bag-of-words*. The occurrence of each word is also called the *term frequency* (tf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0mi71kCE6Aq"
      },
      "outputs": [],
      "source": [
        "{word: sentence.count(word) for word in splitted}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iTvjbKCFYjX"
      },
      "source": [
        "Now as we can see, the word *the* scores higher than the rest in our word count. However, words such as *the* and *and* are not that important for algorithms: they do not say so much what the sentence is about. In contrast to words such as *fox* and *dog* for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_UDIfVi39c1"
      },
      "source": [
        "<a name=\"dataset\"></a>\n",
        "## Dataset\n",
        "We first start with collecting a dataset. In this tutorial, we use a [movie review dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/) from NLTK. This dataset contains 1000 positive movie reviews, and 1000 negative movie reviews. We can use this dataset for sentiment analysis: let the machine recognize words that are negative or positive to classify the movie review correctly as negative or positive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fz8sw8uN38x8"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words, movie_reviews as mr\n",
        "nltk_words = set(words.words())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlGANEGI8d_O"
      },
      "source": [
        "We first remove the punctuation from all the words, and afterwards we count the most common words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fazKk8kh5Peg"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "def remove_punct(word):\n",
        "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
        "    return word if word in nltk_words else ''\n",
        "all_words = Counter(filter(remove_punct, mr.words()))\n",
        "all_words.most_common(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvMOSCRS81nm"
      },
      "source": [
        "The same problem we have here. Words such as *the* and *a* are the most common amongst the movie reviews of our dataset. However, to do something with the movie review, such as classifying it, we should give a lower probability to these words, as they do not say much about the content itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty641CpU_1xk"
      },
      "outputs": [],
      "source": [
        "documents = [(list(filter(remove_punct, mr.words(f))), mr.categories(f)) for f in mr.fileids()]\n",
        "print(\"Total number of documents:\", len(documents))\n",
        "print(\"Total number of words in first document:\", len(documents[0][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6DBjrwDGoow"
      },
      "source": [
        "## tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXl-jPkoGsZW"
      },
      "source": [
        "With tf-idf we can give a more weighted value of relevance of a word (or term) in a text. The tf-idf score increases with the number of occurrences within a document and increases with the rarity of the term in the collection.\n",
        "\n",
        "Remember how we calculate the tf-idf score:\n",
        "\n",
        "$$w_{t,d} = \\log(1+\\text{tf}_{t,d}) \\times \\log_{10}(\\frac{N}{\\text{df}_{t}})$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27SyhyooCggX"
      },
      "source": [
        "Lets start with calculating the term frequency (tf). Now, we calculated the number of words for all documents. However, to calculate the tf-idf score we need to calculate the term-frequency for each term per document. Thus, we need to loop over the documents and count the occurrences of the terms per document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61lfkNqbkSU8"
      },
      "outputs": [],
      "source": [
        "tf = [Counter(words) for words, category in documents]\n",
        "tf[0].most_common(10) # Most common terms for the first document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV9FnDB2oJxH"
      },
      "source": [
        "Now lets also calculate the document frequency (df). This is a bit more involved, since we need to calculate for each word for how many documents that word occurs. We can do that with something like this.\n",
        "We make the documents into sets (a collection of unique words) to speed up the calculation. Instead of O(n) we get O(1). Then, we loop over all words, and retrun 1 for each document the word occurs in. We sum these 1's to get a count of all documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2WbKAjXhs5a"
      },
      "outputs": [],
      "source": [
        "setted_docs = [set(doc) for doc, category in documents]\n",
        "df = {word: sum([1 for doc in setted_docs if word in doc]) for word in all_words.keys()}\n",
        "list(df.items())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSkSlby1JVEV"
      },
      "source": [
        "### Exercise 1\n",
        "> Implement the tf-idf score for each word per document yourself.\n",
        "You may use `numpy` to calculate `log` and `log10`.\n",
        "\n",
        "> Hint: remember that you can access keys of a dictionary with `.keys()`, values with `.values()` and a tuple of both with `.items()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdIeXFByuDd6"
      },
      "outputs": [],
      "source": [
        "# ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsgtPmwkaN9f"
      },
      "source": [
        "### Exercise 2\n",
        "a. Using the list of tf-idf scores you computed above, get words with the highest valued tf-idf score for both the negative and the positive reviews.\n",
        "\n",
        "b. What do you notice? Write down in text what you see. Do you see a difference between both lists of 50 words? Are there also words that are the same? Could we train a classifier that given the tf-idf score of the words in a document could predict correctly whether the review was positive or negative?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER HERE"
      ],
      "metadata": {
        "id": "H4aqUOZ0pZZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO0X_3Ktkr2p"
      },
      "source": [
        "WRITE ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yams1sh5wljY"
      },
      "source": [
        "## Word2Vec\n",
        "In the previous section we have seen we can represent documents by its words by focussing on words that are least occurring in documents but occurring a lot in a specific document. In this section, we will focus on word representations. We will train a Word2Vec model from scratch, by using the same dataset as before. In this way, we try to compare the two datasets and see if we can find differences between words in a negative setting vs words in a positive setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br2FrWAyzZtG"
      },
      "source": [
        "Word2Vec learns its word embeddings by looking inside the documents and checking the nearby words. The core idea behind this is that similar words are nearby in a sentence.\n",
        "The most common implementation for Word2Vec in Python is the one by [gensim](https://radimrehurek.com/gensim/models/word2vec.html). We can compute the embeddings by passing our documents as sentences to the model. Then to get an embedding, we just index the models word vectors with our needed embedding:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWcCTa7Vx28V"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences=[doc for doc, cat in documents])\n",
        "word_vectors = model.wv\n",
        "word_vectors['the']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUWGDYUc0qFQ"
      },
      "source": [
        "We can find the most similar vector nearby a word using `most_similar`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbk1ZsaUzrGe"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar('king')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vu_9uBVx3Mfu"
      },
      "source": [
        "And we can even do arithmetic with it. The most famous example of this is the `king + man - woman = queen` analogy. By adding the vector of king and man to each other, and subtracting the vector of woman, we should get the queen vector. Lets try!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrD-Erlzzuvb"
      },
      "outputs": [],
      "source": [
        "word_vectors.most_similar(positive=['king','woman'],negative=['man'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfQD-O4P3199"
      },
      "source": [
        "We get queen as the second most similar vector. We only trained our word2vec model on our reviews dataset which is a small dataset for word2vec standards, so that makes sense.\n",
        "\n",
        "Lastly, lets plot the data. For this, we need to represent our vectors as a 2-d space. For this, we need a dimensionality reduction technique, such as PCA or t-SNE. We use t-SNE (invented by someone who did the same master as you are doing!). It might take a while to compute the vectors below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm5VC75O4l8U"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "tsne = TSNE(n_components=2, random_state=0)\n",
        "vectors = tsne.fit_transform(np.asarray(model.wv.vectors))\n",
        "x, y = zip(*vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMg0YUpX8_0R"
      },
      "outputs": [],
      "source": [
        "len(x), len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K__i6RQ7Jzf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 12))\n",
        "plt.scatter(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pretrained Word2Vec\n",
        "Word2Vec actually works best when using a pretrained word vectors. This means that we would not put in data in the model to train a good representation, but we rely on external researchers that have already trained such a system on so much data the word vectors have a good representation already.\n",
        "\n",
        "We now will use glove vectors. We can import such model like so. It might take a while to download them."
      ],
      "metadata": {
        "id": "QdtYf5ehuhM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader\n",
        "glove = gensim.downloader.load('glove-wiki-gigaword-50')"
      ],
      "metadata": {
        "id": "cO1sUsQOu2x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove[\"king\"]"
      ],
      "metadata": {
        "id": "PTBHPYXQvmTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0DHPtgXpXf_"
      },
      "source": [
        "### Exercise 3\n",
        "> Using all our `documents`, get the `glove` pretrained word vector for every word, take the average over all word vectors for each document and train a simple binary classifier from [scikit-learn](https://scikit-learn.org/) such as `LogisticRegression` or `SVC`(support vectors machine) on the averaged vectors per document with the classes (y value) being whether that review was positive or negative.\n",
        "\n",
        "> Remember, split the data in training and test sets first. For example in a 80/20 split. Both datasets should have about the same number of positive and negative reviews (use can use [this function](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)). In scikit-learn, use `.fit()` to fit the training data, then use `.predict()` to test on test data. You can use [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to test the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER HERE"
      ],
      "metadata": {
        "id": "Z-v99KUopn_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias in Word2Vec\n",
        "One of the problems with Word2Vec (and with machine learning in general) is that there is lots of biases assumed by the model. Examples of biases that can be harmful when using these algorithms include gender bias and ethnicity bias. Lets check for example what happens if we take the female equivalent of `doctor`:"
      ],
      "metadata": {
        "id": "7Rj4cz1zMCeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove.most_similar(positive=['doctor','woman'],negative=['man'])"
      ],
      "metadata": {
        "id": "mAjcNaqcMF_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "> Think of other examples of bias in word2vec (check the slides for ideas). Also explain why these types of biases are bad/harmful."
      ],
      "metadata": {
        "id": "jDdSAOEuT0b5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "SXAt62XxURRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# COMPUTE VECTORS HERE"
      ],
      "metadata": {
        "id": "TIVT4MwNUQ86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers"
      ],
      "metadata": {
        "id": "qGeOKjbPmc46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We arrive at the state-of-the-art, Transformers models! Although in another course we go deeper into Transformers itself, in this section we will go through representing our dataset as vectors. We do this again with the use of a pretrained model, for example BERT. We use the `transformers` library from HuggingFace to download the model, and use it on our data. Lets install the library first and import the model.\n",
        "\n",
        "In this section we will use Sentence Transformers library, which is a popular way of calculating embeddings for sentences using transformer models. See the documentation of the library [here](https://www.sbert.net/).\n",
        "In essence, this library basically also uses BERT-based models, but uses a mean pooling algorithm to average the embeddings out over its tokens. Its also more efficient, it would take some time using BERT to compute all the embeddings for every document in our reviews dataset, Sentence Transformers is optimized to do such task.\n",
        "\n",
        "We start with downloading the library using `pip` and importing a model."
      ],
      "metadata": {
        "id": "LmZj5iBvmhOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "XvSjvFzQm-g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can encode any sentence like this:"
      ],
      "metadata": {
        "id": "DRRDncajnI0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_embedding = sentence_model.encode(\"the quick brown fox jumps over the lazy dog\")\n",
        "sentence_embedding.shape"
      ],
      "metadata": {
        "id": "PGubLNl4nBwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now get a vector of 384 instead of a matrix of 11 by 768. This makes it much easier to deal with."
      ],
      "metadata": {
        "id": "NE9xT8UJnR5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5\n",
        "> We now again do sentiment classification, this time with Sentence Transformers. Convert the documents in your dataset by passing them all in your `encode()` function of a `sentence_model`. Then, using this matrix use the models you defined at Exercise 3 with this matrix as input. Again use the `accuracy_score` function on the test set, like you did in Exercise 3, to test how good these models perform."
      ],
      "metadata": {
        "id": "jT8Xg5I6eW_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ANSWER HERE"
      ],
      "metadata": {
        "id": "gu12iOsop8qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Do you see a difference between the accuracy had at Exercise 3 and the accuracy here? Why do you think this is the case? How can we even further improve the accuracy?"
      ],
      "metadata": {
        "id": "jkF-yx-Yp2op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER HERE"
      ],
      "metadata": {
        "id": "UYHX3RD7p-6E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Submission\n",
        "Please share your Colab notebook by clicking File on the top-left corner. Click under Download on Download .ipynb and upload that file to Canvas."
      ],
      "metadata": {
        "id": "wGBVYHLFS_2l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6EvAPS1lTATI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}